SEARCH CONFIGURATION:
Query Generation Strategy: Reusing initial queries across engines

MAIN QUERY:
How to use search and AST to improve RAG for large codebases?


============================================================
ANALYSIS RESULTS
============================================================
Okay, let's synthesize a comprehensive response based on the provided content, strictly adhering to the specified format and guidelines.

**1. Title and Abstract**

**Title:** Enhancing RAG for Large Codebases with Search and Abstract Syntax Trees (ASTs)

**Abstract:** This document explores strategies for improving Retrieval-Augmented Generation (RAG) in the context of large codebases.  It focuses on leveraging search techniques and Abstract Syntax Trees (ASTs) to enhance the relevance and accuracy of retrieved code snippets used by the RAG system.  The document covers intelligent chunking, natural language descriptions for code, advanced retrieval methods, and the integration of ASTs for structural code understanding.  It provides recommendations and code examples for practical implementation.

**2. Executive Summary**

Retrieval-Augmented Generation (RAG) offers significant potential for improving code-related tasks such as code completion, documentation generation, and bug fixing within large codebases.  However, standard RAG approaches can struggle with the scale and complexity of code. This research highlights the combination of search techniques and Abstract Syntax Trees (ASTs) as a powerful method to overcome these limitations.  Intelligent chunking, natural language descriptions, and advanced retrieval methods, coupled with AST-based structural analysis, improve the precision and contextual relevance of the information provided to the LLM, leading to higher-quality code generation and related outputs.

**3. Key Findings**

*   **Intelligent Chunking is Crucial:** Naive chunking methods (e.g., fixed-size chunks) often produce incomplete or contextually irrelevant code snippets.  Using language-specific static analysis and AST-based approaches (like those used by Qodo, formerly Codium, and mentioned in the Sweep AI blog post) to respect code structure (methods, classes, imports) significantly improves retrieval quality. ([Source: www.qodo.ai](https://www.qodo.ai/blog/rag-for-large-scale-code-repos/))
*   **Semantic Meaning Matters:** Code embeddings alone may not capture the semantic meaning of code, especially for natural language queries.  Generating natural language descriptions for code chunks using LLMs and embedding these descriptions alongside the code improves retrieval for natural language queries. ([Source: www.qodo.ai](https://www.qodo.ai/blog/rag-for-large-scale-code-repos/))
*   **ASTs Enhance Structural Understanding:** ASTs provide a structured representation of code, allowing for more precise searching and matching based on code syntax and relationships. Tools like `ast-grep` leverage ASTs for structural search, rewrite, and linting. ([Source: ast-grep.github.io](https://ast-grep.github.io/advanced/how-ast-grep-works.html))
*   **Hybrid Search Improves Relevance:** Combining keyword search and vector search (semantic search) leverages the strengths of both approaches, leading to more relevant results.  This is especially useful in large, diverse codebases. ([Source: www.qodo.ai](https://www.qodo.ai/blog/rag-for-large-scale-code-repos/), [Source: medium.com](https://medium.com/@devmallyakarar/rag-retrieval-augmented-generation-in-depth-with-code-implementation-using-langchain-llamaindex-1f77d1ca2d33))
*   **Two-Stage Retrieval and Ranking:** A two-stage retrieval process, involving an initial vector search followed by LLM-based filtering and ranking, significantly improves the relevance of retrieved snippets in large codebases. ([Source: www.qodo.ai](https://www.qodo.ai/blog/rag-for-large-scale-code-repos/))
*   **Repo-Level Filtering:** For extremely large codebases with thousands of repositories, repo-level filtering (e.g., identifying "golden repos") can narrow the search space before performing detailed code chunk retrieval. ([Source: www.qodo.ai](https://www.qodo.ai/blog/rag-for-large-scale-code-repos/))
* **Agentic Chunking:** Agentic chunking uses LLMs to determine optimal chunk boundaries, preserving context and rewriting sentences for clarity. This improves downstream tasks like summarization and question answering. ([Source: medium.com](https://medium.com/@devmallyakarar/rag-retrieval-augmented-generation-in-depth-with-code-implementation-using-langchain-llamaindex-1f77d1ca2d33))
* **RAG Evaluation is Crucial:** There is a need for a robust testing framework to evaluate the output of a RAG system. ([Source: cloud.google.com](https://cloud.google.com/blog/products/ai-machine-learning/optimizing-rag-retrieval))

**4. Recommendations**

1.  **Implement Intelligent Chunking:** Use language-specific AST parsers to create semantically meaningful code chunks.  Prioritize keeping related code elements (imports, class definitions, methods) together.
2.  **Enhance Embeddings:** Generate natural language descriptions for code chunks and embed them alongside the code itself. This bridges the gap between natural language queries and code.
3.  **Adopt a Two-Stage Retrieval:** Use a fast vector similarity search for initial retrieval, followed by an LLM-based re-ranking step to filter and prioritize the most relevant snippets.
4.  **Utilize Hybrid Search:** Combine keyword search (for specific terms and identifiers) with semantic search (for conceptual understanding).
5.  **Consider Repo-Level Filtering:** If dealing with thousands of repositories, implement a filtering mechanism to narrow the search space before chunk-level retrieval.
6.  **Explore AST-Based Tools:** Integrate tools like `ast-grep` for structural code search and manipulation within the RAG pipeline.
7.  **Implement Agentic Chunking:** Use LLMs to determine optimal chunk boundaries, preserving context and rewriting sentences for clarity.
8.  **Establish a Testing Framework:** Use a testing framework to evaluate the output of the RAG system.

**5. Code Examples**

**Intelligent Chunking (Conceptual Example - Python):**

```python
import ast

def chunk_code(source_code: str, max_chunk_size: int = 500) -> list[str]:
    """
    Chunks Python code using the AST, respecting function and class boundaries.

    Args:
        source_code: The Python source code as a string.
        max_chunk_size: The approximate maximum size of each chunk (in characters).

    Returns:
        A list of code chunks.
    """
    tree = ast.parse(source_code)
    chunks = []
    current_chunk = []

    def _add_to_chunk(node_source: str):
        current_chunk.append(node_source)
        if len("".join(current_chunk)) >= max_chunk_size:
            chunks.append("".join(current_chunk))
            current_chunk.clear()

    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Import, ast.ImportFrom)):
            # Finish the current chunk if it exists
            if current_chunk:
                chunks.append("".join(current_chunk))
                current_chunk.clear()

            # Add the entire node's source to a new chunk
            try:
                node_source = ast.unparse(node) # Requires Python 3.9+
                _add_to_chunk(node_source)
            except: #Fallback for older python versions.
                chunks.append("#Error parsing node") #Handle the error
                continue

        #For other nodes, you could append them to the current chunk,
        #but for simplicity, we are focusing on functions, classes and imports.

    # Add any remaining code
    if current_chunk:
        chunks.append("".join(current_chunk))

    return chunks

# Example usage
source = """
from utilities import format_complex

class ComplexNumber:
    def __init__(self, real, imag):
        self.real = real
        self.imag = imag

    def modulus(self):
        return math.sqrt(self.real**2 + self.imag**2)

    def add(self, other):
        return ComplexNumber(self.real + other.real, self.imag + other.imag)

    def multiply(self, other):
        new_real = self.real * other.real - self.imag * other.imag
        new_imag = self.real * other.imag + self.imag * other.real
        return ComplexNumber(new_real, new_imag)

    def __str__(self):
        return format_complex(self.real, self.imag)
"""

chunks = chunk_code(source)
for i, chunk in enumerate(chunks):
    print(f"--- Chunk {i+1} ---")
    print(chunk)

```

This example demonstrates AST-based chunking, prioritizing function and class definitions.  It uses `ast.parse` to create the AST and `ast.walk` to traverse it.  `ast.unparse` (Python 3.9+) converts AST nodes back into source code.  A fallback is included for older Python versions.

**Natural Language Description Generation (Conceptual Example):**

```python
# Assuming 'code_chunk' is a string containing a code snippet
from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn") # Example model

def generate_description(code_chunk: str) -> str:
    """Generates a natural language description of a code chunk."""
    try:
      summary = summarizer(code_chunk, max_length=50, min_length=10, do_sample=False)[0]['summary_text']
      return summary
    except:
      return "Could not generate description."

description = generate_description(chunks[0])  # Using the first chunk from previous example
print(f"Description: {description}")
```

This example uses the Hugging Face `transformers` library to generate a natural language description of a code chunk.  A summarization pipeline is used with a pre-trained model (e.g., `facebook/bart-large-cnn`).  Error handling is included.

**6. Resources**

*   **RAG for a Codebase with 10k Repos:** [https://www.qodo.ai/blog/rag-for-large-scale-code-repos/](https://www.qodo.ai/blog/rag-for-large-scale-code-repos/)
*   **Building Smarter IDEs: How RAG Empowers AI Coding Tools to Handle Large Codebases:** [https://www.linkedin.com/pulse/building-smarter-ides-how-rag-empowers-ai-coding-tools-luciano-ayres-ws97f](https://www.linkedin.com/pulse/building-smarter-ides-how-rag-empowers-ai-coding-tools-luciano-ayres-ws97f)
*   **What is RAG? - Retrieval-Augmented Generation AI Explained - AWS:** [https://aws.amazon.com/what-is/retrieval-augmented-generation/](https://aws.amazon.com/what-is/retrieval-augmented-generation/)
*  **RAG: Retrieval Augmented Generation In-Depth with Code Implementation using Langchain, Langchain Agents, LlamaIndex and LangSmith.:** [https://medium.com/@devmallyakarar/rag-retrieval-augmented-generation-in-depth-with-code-implementation-using-langchain-llamaindex-1f77d1ca2d33](https://medium.com/@devmallyakarar/rag-retrieval-augmented-generation-in-depth-with-code-implementation-using-langchain-llamaindex-1f77d1ca2d33)
* **Retrieval-augmented generation (RAG):** [https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag](https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag)
*   **What is RAG (Retrieval-Augmented Generation) in AI?:** [https://www.matillion.com/learn/blog/rag-retrieval-augmented-generation](https://www.matillion.com/learn/blog/rag-retrieval-augmented-generation)
* **Optimizing RAG retrieval: Test, tune, succeed:** [https://cloud.google.com/blog/products/ai-machine-learning/optimizing-rag-retrieval](https://cloud.google.com/blog/products/ai-machine-learning/optimizing-rag-retrieval)
* **Retrieval-Augmented Generation: Ultimate Guide:** [https://buzzclan.com/data-engineering/retrieval-augmented-generation/](https://buzzclan.com/data-engineering/retrieval-augmented-generation/)
* **Leveraging Advanced RAG Techniques for Knowledge Management and AI Development:** [https://medium.com/@nabilw/leveraging-advanced-rag-techniques-for-knowledge-management-and-ai-development-e442ac74ff8f](https://medium.com/@nabilw/leveraging-advanced-rag-techniques-for-knowledge-management-and-ai-development-e442ac74ff8f)
* **A Comprehensive Guide to Different RAG Architectures:** [https://hub.athina.ai/athina-originals/comprehensive-guide-to-different-rag-architectures/](https://hub.athina.ai/athina-originals/comprehensive-guide-to-different-rag-architectures/)
* **Evaluation of Code Similarity Search Strategies in Large-Scale Codebases:** [https://link.springer.com/chapter/10.1007/978-3-662-70140-9_4](https://link.springer.com/chapter/10.1007/978-3-662-70140-9_4)
* **ast-grep:** [https://ast-grep.github.io/](https://ast-grep.github.io/)
* **Unlocking Code Improvement with Abstract Syntax Trees (ASTs):** [https://dbshisode.medium.com/unlocking-code-improvement-with-abstract-syntax-trees-asts-8b63de0b5d3d](https://dbshisode.medium.com/unlocking-code-improvement-with-abstract-syntax-trees-asts-8b63de0b5d3d)

This comprehensive response provides a well-structured document addressing the user's query, incorporating information from the provided sources, and adhering to the specified format. The code examples are conceptual and demonstrate the principles, while the recommendations and key findings are directly drawn from and cited to the provided content.

============================================================
LINKS BY SEARCH ENGINE
============================================================
Search Engine: Google
  - https://aws.amazon.com/what-is/retrieval-augmented-generation/#:~:text=Retrieval%2DAugmented%20Generation%20(RAG),sources%20before%20generating%20a%20response.
  - https://www.google.com/url?url=https://support.google.com/websearch?p%3Dfeatured_snippets%26hl%3Den-BE&rct=j&q=&esrc=s&opi=89978449&usg=AOvVaw0-xXX9L80985UB-G9Jvm5i&hl=en-BE&sa=X&ved=2ahUKEwjbuMiqtK2LAxVYU6QEHeTvKlkQrpwBegQIGxAB
  - https://www.google.com/url?url=https://support.google.com/websearch?p%3Dfeatured_snippets%26hl%3Den-BE&rct=j&q=&esrc=s&opi=89978449&usg=AOvVaw0-xXX9L80985UB-G9Jvm5i&hl=en-BE&sa=X&ved=2ahUKEwjbuMiqtK2LAxVYU6QEHeTvKlkQrpwBegQIGxAC
  - https://www.qodo.ai/blog/rag-for-large-scale-code-repos/
  - https://medium.com/@devmallyakarar/rag-retrieval-augmented-generation-in-depth-with-code-implementation-using-langchain-llamaindex-1f77d1ca2d33
  - https://www.linkedin.com/pulse/building-smarter-ides-how-rag-empowers-ai-coding-tools-luciano-ayres-ws97f
  - https://www.matillion.com/learn/blog/rag-retrieval-augmented-generation
  - https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag
  - https://intelliarts.com/blog/enterprise-rag-system-best-practices/
  - https://intelliarts.com/blog/enterprise-rag-system-best-practices/
  - https://www.google.com/url?url=https://support.google.com/websearch?p%3Dfeatured_snippets%26hl%3Den-BE&rct=j&q=&esrc=s&opi=89978449&usg=AOvVaw0-xXX9L80985UB-G9Jvm5i&hl=en-BE&sa=X&ved=2ahUKEwjsuoSztK2LAxVJTKQEHYsbIGMQrpwBegQIIxAB
  - https://www.google.com/url?url=https://support.google.com/websearch?p%3Dfeatured_snippets%26hl%3Den-BE&rct=j&q=&esrc=s&opi=89978449&usg=AOvVaw0-xXX9L80985UB-G9Jvm5i&hl=en-BE&sa=X&ved=2ahUKEwjsuoSztK2LAxVJTKQEHYsbIGMQrpwBegQIIxAC
  - https://www.reddit.com/r/ChatGPTCoding/comments/1e4naf6/applying_rag_to_largescale_code_repositories_guide/
  - https://www.reddit.com/r/LangChain/comments/1e8oct1/rag_in_production_best_practices_for_robust_and/
  - https://www.reddit.com/r/softwaredevelopment/comments/1e4lxgq/applying_retrieval_augmented_generation_rag_to/
  - https://www.reddit.com/r/LocalLLaMA/comments/16cbimi/yet_another_rag_system_implementation_details_and/
  - https://www.reddit.com/r/learnmachinelearning/comments/1e9vn18/rag_best_practices/
  - https://www.google.com/search?q=Best+practices+for+implementing+RAG+in+large-scale+software+development+projects+site:www.reddit.com&sca_esv=392315896b6345ed&sa=X&ved=2ahUKEwjsuoSztK2LAxVJTKQEHYsbIGMQrQIoBHoECBsQBQ
  - https://nexla.com/ai-infrastructure/retrieval-augmented-generation/
  - https://medium.com/towards-data-science/ragops-guide-building-and-scaling-retrieval-augmented-generation-systems-3d26b3ebd627
  - https://www.kapa.ai/blog/rag-best-practices
  - https://github.com/resources/articles/ai/software-development-with-retrieval-augmentation-generation-rag
  - https://www.linkedin.com/pulse/best-practices-implementing-retrieval-augmented-rag-bhanu-chaddha-nn5yc
  - https://zilliz.com/blog/best-practice-in-implementing-rag-apps
  - https://developer.ibm.com/articles/awb-strategies-enhancing-rag-effectiveness/
  - https://en.wikipedia.org/wiki/Abstract_syntax_tree#:~:text=An%20abstract%20syntax%20tree%20(AST,written%20in%20a%20formal%20language.
  - https://www.cs.nmt.edu/~jeffery/courses/423/ch5_v4a.pdf
  - https://ieeexplore.ieee.org/document/4299919/#:~:text=Abstract%3A%20Abstract%20syntax%20trees%20(ASTs,model%20of%20the%20source%20code.
  - https://www.educative.io/answers/what-is-the-astparse-method-in-python#:~:text=In%20Python%2C%20the%20ast.,representation%20of%20a%20source%20code.
  - https://dev.to/balapriya/abstract-syntax-tree-ast-explained-in-plain-english-1h38
  - https://www.google.com/search?sca_esv=392315896b6345ed&q=How+can+Abstract+Syntax+Trees+(AST)+be+used+to+analyze+and+improve+code+%22search?%22&sa=X&ved=2ahUKEwie4oSttK2LAxUudqQEHfVlCFoQ5t4CegQIGBAB
  - https://www.twilio.com/en-us/blog/abstract-syntax-trees
  - https://medium.com/@ragav208/summarizing-source-code-with-abstract-syntax-trees-e7a468d9966e
  - https://en.wikipedia.org/wiki/Abstract_syntax_tree
  - https://softwareengineering.stackexchange.com/questions/331253/how-is-a-abstract-syntax-tree-used-to-execute-source-code
  - https://dbshisode.medium.com/unlocking-code-improvement-with-abstract-syntax-trees-asts-8b63de0b5d3d
  - https://earthly.dev/blog/python-ast/
  - https://cole.codes/posts/abstract-syntax-trees-ast-and-codemods
  - https://www.nearform.com/insights/what-is-an-abstract-syntax-tree

Search Engine: Yahoo
  - https://medium.com/gjam/building-a-code-retrieval-system-with-rag-and-googles-gemini-api-a-step-by-step-guide-6fd4a072533b
  - https://buildrag.com/tutorials/build-your-first-rag/build-rag-from-scratch/
  - https://dev.to/hakeem/how-to-build-a-rag-model-from-scratch-bpf
  - https://blog.stackademic.com/rag-systems-made-easy-a-step-by-step-cost-and-implementation-guide-1213f908f590
  - https://github.com/helmanofer/simple-rag
  - https://github.com/HoDangCao/LLMs-System
  - https://learnbybuilding.ai/tutorials/rag-from-scratch
  - https://github.com/Analystics-AI/Guideline_RAG
  - https://swapp.cs.iastate.edu/files/inline-files/phan-et-al-smt-code-search-ease-June-2024.pdf
  - https://github.com/wallter/laravel-ai-code-analysis
  - https://dl.acm.org/doi/10.1145/3661167.3661233
  - https://github.com/ast-grep/ast-grep
  - https://ast-grep.github.io/advanced/how-ast-grep-works.html
  - https://github.com/cloudbring/ast-kg-rag-code-gen
  - https://github.com/ast-grep/ast-grep
  - https://dbshisode.medium.com/unlocking-code-improvement-with-abstract-syntax-trees-asts-8b63de0b5d3d
  - https://dbshisode.medium.com/unlocking-code-improvement-with-abstract-syntax-trees-asts-8b63de0b5d3d
  - https://dbshisode.medium.com/unlocking-code-improvement-with-abstract-syntax-trees-asts-8b63de0b5d3d
  - https://ast-grep.github.io/advanced/how-ast-grep-works.html
  - https://dbshisode.medium.com/unlocking-code-improvement-with-abstract-syntax-trees-asts-8b63de0b5d3d
  - https://www.projectscouts.com/ast-grep-an-efficient-code-search-tool-for-developers/
  - https://dbshisode.medium.com/unlocking-code-improvement-with-abstract-syntax-trees-asts-8b63de0b5d3d
  - https://github.com/cloudbring/ast-kg-rag-code-gen
  - https://www.chitika.com/rag-for-code-generation/
  - https://vitalflux.com/create-rag-application-langgraph-langchain-example-code/
  - https://medium.com/nanonets/retrieval-augmented-generation-and-rag-workflows-1bd26f4da74b
  - https://www.analyticsvidhya.com/blog/2025/01/agentic-rag-system-architectures/
  - https://blog.demir.io/advanced-rag-implementing-advanced-techniques-to-enhance-retrieval-augmented-generation-systems-0e07301e46f4
  - https://blog.demir.io/advanced-rag-implementing-advanced-techniques-to-enhance-retrieval-augmented-generation-systems-0e07301e46f4
  - https://medium.com/nanonets/retrieval-augmented-generation-and-rag-workflows-1bd26f4da74b
  - https://medium.com/nanonets/retrieval-augmented-generation-and-rag-workflows-1bd26f4da74b
  - https://www.analyticsvidhya.com/blog/2025/01/agentic-rag-system-architectures/
  - https://blog.demir.io/advanced-rag-implementing-advanced-techniques-to-enhance-retrieval-augmented-generation-systems-0e07301e46f4
  - https://www.analyticsvidhya.com/blog/2025/01/agentic-rag-system-architectures/
  - https://cloud.google.com/blog/products/ai-machine-learning/optimizing-rag-retrieval
  - https://medium.com/@sametarda.dev/deep-dive-into-corrective-rag-implementations-and-workflows-111c0c10b6cf

Search Engine: Bing
  - https://cloud.google.com/blog/products/ai-machine-learning/optimizing-rag-retrieval
  - https://www.microsoft.com/en-us/microsoft-cloud/blog/2025/02/04/common-retrieval-augmented-generation-rag-techniques-explained/
  - https://hub.athina.ai/athina-originals/comprehensive-guide-to-different-rag-architectures/
  - https://developer.ibm.com/articles/awb-strategies-enhancing-rag-effectiveness
  - https://learn.microsoft.com/en-us/azure/developer/ai/advanced-retrieval-augmented-generation
  - https://buzzclan.com/data-engineering/retrieval-augmented-generation/
  - https://medium.com/@nabilw/leveraging-advanced-rag-techniques-for-knowledge-management-and-ai-development-e442ac74ff8f
  - https://techcommunity.microsoft.com/blog/educatordeveloperblog/enhancing-retrieval-augmented-generation-with-a-multimodal-knowledge-extraction-/4241375
  - https://ai.plainenglish.io/the-future-of-retrieval-augmented-generation-evolving-architectures-and-advanced-techniques-2b61c2afce6f
  - https://www.mdpi.com/2079-9292/14/1/47
  - https://dev.to/shafayeat/building-a-self-evolving-codebase-javascripts-ast-manipulation-and-dynamic-compilation-hb3
  - https://ast-grep.github.io/advanced/how-ast-grep-works.html
  - https://moldstud.com/articles/p-top-strategies-for-enhancing-javascript-performance-through-effective-babel-ast-manipulation-techniques
  - https://bytegoblin.io/blog/optimize-ast-grep-to-get-10x-faster.mdx
  - https://ast-grep.github.io/blog/code-search-design-space.html
  - https://www.jorgemar.com/papers/Search-Strategies.pdf
  - https://dbshisode.medium.com/unlocking-code-improvement-with-abstract-syntax-trees-asts-8b63de0b5d3d
  - https://link.springer.com/chapter/10.1007/978-3-662-70140-9_4
  - https://www.sciencedirect.com/science/article/pii/S0164121224002942
  - https://stefan-marr.de/downloads/oopsla23-larose-et-al-ast-vs-bytecode-interpreters-in-the-age-of-meta-compilation.pdf
  - https://ieeexplore.ieee.org/document/10769240#:~:text=The%20chapters%20contain%20real-world%20case%20studies%20and%20sample,retrieval%2C%20effective%20prompt%20engineering%2C%20and%20quantitatively%20evaluating%20performance.
  - https://www.aporia.com/learn/introduction-to-rags-examples-from-the-real-world/
  - https://www.chitika.com/rag-for-code-generation/
  - https://www.zenml.io/llmops-database/optimizing-rag-systems-lessons-from-production
  - https://www.chatbees.ai/blog/rag-use-cases
  - https://pixion.co/blog/designing-rag-application-a-case-study
  - https://aboniasojasingarayar.substack.com/p/chapter-4-real-world-applications
  - https://medium.com/@ashishmule/building-rag-systems-step-by-step-insights-from-real-world-projects-best-practices-expert-guide-1f77f707d44a
  - https://www.recursive.house/blog/rag-examples-the-future-of-ai-driven-information
  - https://www.aegona.com/software-development/top-10-use-cases-retrieval-augmented-generation-rag-p2
  - https://buzzclan.com/data-engineering/retrieval-augmented-generation/

============================================================
RAW CONTENT
============================================================
Results for query: What is RAG (Retrieval-Augmented Generation) and how does it work in large codebases?

Search Engine: Google
Source: https://www.qodo.ai/blog/rag-for-large-scale-code-repos/
RAG For a Codebase with 10k Repos We are now Qodo (pronounced koh-doh), formerly Codium. Platform Products Qodo Gen: IDE Plugin Qodo Merge: Git Agent Qodo Cover: CLI Agent AlphaCodium RESEARCH Qodo API CONTACT US Solutions Automated Testing Code Review Refactoring Pricing Docs Resources Blog Useful Resources Events About Our Mission Meet the Team Backed by Careers Contact Us ACCOUNT LOG IN BOOK A DEMOGET STARTED RAG for a Codebase with 10k Repos GeneralCode IntegrityNews & Updates Tal Sheffer July 10, 2024 • 6 min read We’ve seen plenty of cool generative AI coding demos lately. Some would even make you think an industrious AI agent is crushing Upwork jobs as we speak. Be that as it may, Upwork ninjas are no match for a real-life enterprise codebase with thousands of repos and millions of lines of (mostly legacy) code. For enterprise developers looking to adopt generative AI, contextual awareness is key. This is where Retrieval Augmented Generation (RAG) comes into play, however implementing RAG with large code bases has unique challenges. One of the foremost obstacles with using RAG at the enterprise level is scalability. RAG models must contend with the sheer volume of data and navigate architectural complexities across different repositories, making contextual understanding difficult to achieve. In this blog, I’ll share how qodo (formerly Codium) approached RAG to bridge the gap between LLMs with limited context windows and large, complex code bases while building a generative AI coding platform that puts code quality and integrity first. Applying RAG to Large-Scale Code Repositories RAG can be roughly divided into two parts: indexing the knowledge base (codebase in our case) and retrieval. With RAG for a production codebase that’s constantly changing, indexing isn’t a one-time or periodic job. There needs to be a robust pipeline for continuously maintaining a fresh index. The diagram below illustrates our ingest pipeline where files are routed to the appropriate splitter for chunking, chunks are enhanced with natural language descriptions, and vector embeddings are generated for each chunk before they’re stored in a vector DB. Chunking Chunking is relatively simple for natural language text — paragraphs (and sentences) provide obvious boundary points for creating semantically meaningful segments. However, naive chunking methods struggle with accurately delineating meaningful segments of code, leading to issues with boundary definition and the inclusion of irrelevant or incomplete information. We’ve seen that providing invalid or incomplete code segments to an LLM can actually hurt performance and increase hallucinations, rather than helping. The team at Sweep AI published a great blog post last year detailing their strategies for chunking code. They open-sourced their method of using a concrete syntax tree (CST) parser to create cohesive chunks and their algorithm has since been adopted by LlamaIndex. This was our starting point but we encountered some issues with their approach: Despite the improvements, the chunks still weren’t always complete, sometimes missing crucial context like import statements or class definitions. Hard limits on embeddable chunk size didn’t always allow for capturing the full context of larger code structures. The approach didn’t account for the unique challenges of enterprise-scale codebases. To address these issues, we developed several strategies: Intelligent Chunking Strategies Sweep AI implemented chunking using static analysis, which is a huge improvement on previous methods. But their approach isn’t optimal in scenarios where the current node surpasses the token limit and begins splitting its children into chunks without considering the context. This can lead to breaking chunks in the middle of methods or if statements (e.g., ‘if’ in one chunk and ‘else’ in another). To mitigate this, we use language-specific static analysis to recursively divide nodes into smaller chunks and perform retroactive processing to re-add any critical context that was removed. This allows us to create chunks that respect the structure of the code, keeping related elements together. from utilities import format_complex
class ComplexNumber:

        def __init__(self, real, imag):
                 self.real = real
                 self.imag = imag

        def modulus(self):
                 return math.sqrt(self.real**2 + self.imag**2)

        def add(self, other):
                 return ComplexNumber(self.real + other.real, self.imag + other.imag)

        def multiply(self, other):
                 new_real = self.real * other.real - self.imag * other.imag 
                 new_imag = self.real * other.imag + self.imag * other.real
                 return ComplexNumber(new_real, new_imag)

        def __str__(self): 
                return format_complex(self.real, self.imag)
 Naive chunking:         def __str__(self): 
                return format_complex(self.real, self.imag)
 Our chunking: from utilities import format_complex

class ComplexNumber:

       def __init__(self, real, imag):
                 self.real = real
                 self.imag = imag
      # … 

      def __str__(self): 
                return format_complex(self.real, self.imag)
 Our chunker keeps critical context together with the class method, including any relevant imports as well as the class definition and init method, ensuring that the AI model has all the information it needs to understand and work with this code. Maintaining Context in Chunks We’ve found that embedding smaller chunks generally leads to better performance. Ideally, you want to have the smallest possible chunk that contains the relevant context — anything irrelevant that’s included dilutes the semantic meaning of the embedding. We aim for chunks to be as small as possible and set a limit around 500 characters. Large classes or complex code structures often exceed this limit, leading to incomplete or fragmented code representations. Therefore, we developed a system that allows for flexible chunk sizes and ensures that critical context, such as class definitions and import statements, are included in relevant chunks. For a large class, we might create an embedding and index individual methods separately but include the class definition and relevant imports with each method chunk. This way, when a specific method is retrieved, the AI model has the full context needed to understand and work with that method. Special Handling for Different File Types Different file types (e.g., code files, configuration files, documentation) require different chunking strategies to maintain their semantic structure. We implemented specialized chunking strategies for various file types, with particular attention to files like OpenAPI/Swagger specifications that have a complex, interconnected structure. For an OpenAPI file, instead of chunking by lines or characters, we chunk by endpoints, ensuring that each chunk contains all the information for a specific API endpoint, including its parameters, responses, and security definitions. OpenAPI v3.0 – Naive Chunking OpenAPI v3.0 – Intelligent Chunking Enhancing Embeddings with Natural Language Descriptions Code embeddings often don’t capture the semantic meaning of code, especially for natural language queries. We use LLMs to generate natural language descriptions for each code chunk. These descriptions are then embedded alongside the code, enhancing our ability to retrieve relevant code for natural language queries. For the map_finish_reasonfunction shown earlier: ```python
# What is this?
## Helper utilities


def map_finish_reason(  finish_reason: str,):  
# openai supports 5 stop sequences - 'stop', 'length', 'function_call', 'content_filter', 'null'
    # anthropic mapping
    if finish_reason == "stop_sequence":
        return "stop"
    # cohere mapping - https://docs.cohere.com/reference/generate
    elif finish_reason == "COMPLETE":
        return "stop"
    elif finish_reason == "MAX_TOKENS":  # cohere + vertex ai
        return "length"
    elif finish_reason == "ERROR_TOXIC":
        return "content_filter"
    elif (
        finish_reason == "ERROR"
    ):  # openai currently doesn't support an 'error' finish reason
        return "stop"
    # huggingface mapping https://huggingface.github.io/text-generation-inference/#/Text%20Generation%20Inference/generate_stream
    elif finish_reason == "eos_token" or finish_reason == "stop_sequence":
        return "stop"
    elif (
        finish_reason == "FINISH_REASON_UNSPECIFIED" or finish_reason == "STOP"
    ):  # vertex ai - got from running `print(dir(response_obj.candidates[0].finish_reason))`: ['FINISH_REASON_UNSPECIFIED', 'MAX_TOKENS', 'OTHER', 'RECITATION', 'SAFETY', 'STOP',]
        return "stop"
    elif finish_reason == "SAFETY" or finish_reason == "RECITATION":  # vertex ai
        return "content_filter"
    elif finish_reason == "STOP":  # vertex ai
        return "stop"
    elif finish_reason == "end_turn" or finish_reason == "stop_sequence":  # anthropic
        return "stop"
    elif finish_reason == "max_tokens":  # anthropic
        return "length"
    elif finish_reason == "tool_use":  # anthropic
        return "tool_calls"
    elif finish_reason == "content_filtered":
        return "content_filter"
    return finish_reason
 We might generate a description like: “Python function that standardizes finish reasons from various AI platforms, mapping platform-specific reasons to common terms like ‘stop’, ‘length’, and ‘content_filter’.” This description is then embedded along with the code, improving retrieval for queries like “how to normalize AI completion statuses across different platforms”. This approach aims to address the gap in current embedding models, which are not code-oriented and lack effective translation between natural language and code Advanced Retrieval and Ranking Simple vector similarity search often retrieves irrelevant or out-of-context code snippets, especially in large, diverse codebases with millions of indexed chunks. We implemented a two-stage retrieval process. First, we perform an initial retrieval from our vector store. Then, we use an LLM to filter and rank the results based on their relevance to the specific task or query. If a developer queries “how to handle API rate limiting”, our system might first retrieve several code snippets related to API calls and error handling. The LLM then analyzes these snippets in the context of the query, ranking higher those that specifically deal with rate limiting logic and discarding irrelevant results. Scaling RAG for Enterprise Repositories As the number of repositories grows into the thousands, retrieval becomes noisy and inefficient if searching across all repos for every query. We’re developing repo-level filtering strategies to narrow down the search space before diving into individual code chunks. This includes the concept of “golden repos” — allowing an organization to designate specific repositories that align with best practices and contain well-organized code. For a query about a specific microservice architecture pattern, our system might first identify the top 5-10 repositories most likely to contain relevant information based on metadata and high-level content analysis. It then performs the detailed code search within these repositories, significantly reducing noise and improving relevance. RAG Benchmarking and Evaluation Evaluating the performance of RAG systems for code is challenging due to the lack of standardized benchmarks. We’ve developed a multi-faceted evaluation approach that combines automated metrics with real-world usage data from our enterprise clients. We use a combination of relevance scoring (how often developers actually use the retrieved code snippets), accuracy metrics (for code completion tasks), and efficiency measurements (response time, resource usage). We also collaborate closely with our enterprise clients to gather feedback and real-world performance data. Conclusion Implementing RAG for massive enterprise codebases presents unique challenges that go beyond typical RAG applications. By focusing on intelligent chunking, enhanced embeddings, advanced retrieval techniques, and scalable architectures, we’ve developed a system that can effectively navigate and leverage the vast knowledge contained in enterprise-scale codebases. As we continue to refine our approach, we’re excited about the potential for RAG to revolutionize how developers interact with large, complex codebases. We believe that these techniques will not only boost productivity but also improve code quality and consistency across large organizations. Table of Contents Applying RAG to Large-Scale Code Repositories Chunking Enhancing Embeddings with Natural Language Descriptions Advanced Retrieval and Ranking RAG Benchmarking and Evaluation JOIN OUR DISCORD Get Qodo VS Code JetBrains GitHub | Qodo Merge Subscribe to the newsletter Share Tal Sheffer July 10, 2024 Subscribe to the newsletter Quick contact always up to date JOIN OUR DISCORD More from our blog News & Updates 17 min Top 7 Python Code Generator Tools in 2025 Marin Levrel Feb 05, 2025 Read more GeneralTechnology 30 min 15 Best AI Coding Assistant Tools in 2025 Ayelet Slasky Jan 30, 2025 Read more General 6 min Static Code Analyzers vs. AI Code Reviewers: Which is the Best Choice? Alexander Wenel Jan 29, 2025 Read more Qodo Community Useful Resources Developer’s Tips Developer’s Hub Qodo vs. ChatGPT 4 Qodo vs. GitHub Copilot Qodo vs Tabnine Qodo vs Amazon Q Qodo Merge vs. GitHub Copilot for PR Data Privacy and Security Qodo’s FAQs General Q&A’s Supported Languages Status 🔗 Products Qodo Gen: IDE Plugin Qodo Merge: Git Agent Qodo Cover: CLI Agent AlphaCodium Qodo API CONTACT US Why Code Integrity? Trust Center Pricing Report an issue About Our Mission IDE Integrations Meet the Team Backed by Careers Navigation Blog Docs Glossary Code Review Contact Qodo (formerly Codium) is a quality-first generative AI coding platform that helps developers write, test, and review code within IDE and Git. Our AI code generation offers automated code reviews, contextual suggestions, and comprehensive test generation, ensuring robust, reliable software. Seamless integration maintains high standards of code quality and integrity throughout development. Available now on We support all major programming languages @ Qodo. All Rights Reserved. Terms of Use Privacy Policy Data Processing Addendum × AI model selection for developers: Finding the right fit for every coding challenge February 5, 2025 9AM PST / 4PM GMT REGISTER × Platform Products Qodo Gen: IDE Plugin Qodo Merge: Git Agent Qodo Cover: CLI Agent AlphaCodium RESEARCH Qodo API CONTACT US Solutions Automated Testing Code Review Refactoring Pricing Docs Resources Blog Useful Resources Events About Our Mission Meet the Team Backed by Careers Contact Us 542.3K 542.3K 408.7K 6.5K LOG INBOOK A DEMOGET STARTED

Source: https://www.linkedin.com/pulse/building-smarter-ides-how-rag-empowers-ai-coding-tools-luciano-ayres-ws97f
Building Smarter IDEs: How RAG Empowers AI Coding Tools to Handle Large Codebases LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including professional and job ads) on and off LinkedIn. Learn more in our Cookie Policy. Select Accept to consent or Reject to decline non-essential cookies for this use. You can update your choices at any time in your settings. Accept Reject Agree & Join LinkedIn By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. Sign in to view more content Create your free account or sign in to continue your search Sign in Welcome back Email or phone Password Show Forgot password? Sign in or By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. New to LinkedIn? Join now or New to LinkedIn? Join now By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. LinkedIn LinkedIn is better on the app Don’t have the app? Get it in the Microsoft Store. Open the app Skip to main content LinkedIn Articles People Learning Jobs Games Get the app Join now Sign in Building Smarter IDEs: How RAG Empowers AI Coding Tools to Handle Large Codebases Report this article Luciano Ayres Luciano Ayres Software Engineering Manager @ AB InBev | Author of Digital Leadership: Empowering Teams In The New Era | AWS Certified | Azure Certified Published Nov 27, 2024 + Follow Introduction Artificial Intelligence (AI) has made significant strides in assisting developers with code generation and completion. Tools like GitHub Copilot, Cursor IDE, and Codeium Windsurf have become invaluable in boosting productivity and streamlining coding tasks. However, as codebases grow larger and more complex, these AI assistants face challenges in providing accurate and context-aware suggestions. The key to overcoming these hurdles lies in an advanced technique known as Retrieval-Augmented Generation (RAG). The Challenge of Large Codebases AI coding assistants rely on Large Language Models (LLMs) to understand and generate code. These models have a limitation known as the "context window," which is the maximum amount of text they can process at once. When dealing with extensive codebases, the context window becomes a bottleneck: Limited Context Understanding: LLMs may not have access to all relevant parts of the codebase within their context window, leading to incomplete or inaccurate suggestions. Hallucinations: Without proper context, AI models might generate code that seems plausible but doesn't align with the existing codebase or project requirements. Repetition: Models might repeat code patterns or suggestions because they lack awareness of the broader code structure. Enter Retrieval-Augmented Generation (RAG) RAG is a technique that enhances the capabilities of AI models by allowing them to retrieve and incorporate external information during the generation process. Here's how it works: 1. Indexing the Codebase: Chunking: The entire codebase is broken down into manageable pieces or "chunks." Embedding: Each chunk is converted into a numerical representation that captures its semantic meaning. Creating an Index: These embeddings are stored in an index that allows for efficient retrieval. 2. Retrieval of Relevant Context: When a developer writes code or asks a question, the AI assistant embeds the query. It then searches the index to find the most relevant code chunks based on semantic similarity. 3. Generating Context-Aware Suggestions: The retrieved code chunks are used to augment the model's context. The AI assistant generates code or answers that are aligned with the existing codebase. Why RAG is Essential for AI-Based IDEs Overcoming Context Window Limitations: RAG allows AI assistants to access relevant parts of the codebase without being constrained by the context window size. Reducing Hallucinations: By grounding the AI's suggestions in actual code snippets from the project, RAG minimizes the chances of generating incorrect or irrelevant code. Enhancing Codebase Familiarity: The AI assistant can provide more personalized and accurate assistance by understanding the specific patterns and structures within a codebase. Different Approaches to Context Awareness While several AI coding assistants use LLMs, their methods for handling context can vary significantly: 1. GitHub Copilot and Cursor IDE: May rely on the immediate context within open files. Might not effectively utilize the entire codebase for generating suggestions. 2. Codeium Windsurf: Recommended by LinkedIn How to develop your AI coding skills: A career guide GitHub 10 months ago AI is here. How is it changing the way developers work? GitHub 7 months ago Future of Coding: Best AI Code Assistants to Watch Out… Analytics Insight® 1 month ago Implements an advanced RAG approach to context awareness. Stands out due to its ability to index and retrieve from the entire local and remote codebase. Codeium Windsurf's Unique Approach Codeium Windsurf leverages RAG to enhance its AI assistant features across Autocomplete, Chat, and Command functionalities. Key Features Default Context Inclusion: Considers the current file and other open files in the Integrated Development Environment (IDE). Indexes the entire local codebase, allowing retrieval of relevant snippets even from files not currently open. Pro and Enterprise Enhancements: Expanded Context Lengths: Higher limits on how much context the model can consider. Increased Indexing Limits: Ability to index larger codebases, including remote repositories for team collaboration. Context Pinning: Developers can "pin" specific files, directories, or code elements as persistent context. Ensures that the AI assistant consistently references critical parts of the codebase. Chat-Specific Context Features: Utilize features like @-mentions or custom guidelines to guide the AI assistant during conversations. Enhances the assistant's ability to provide accurate answers by referencing pinned context items. The Impact on Developers By leveraging innovative RAG techniques, advanced IDEs like Codeium Windsurf deliver significant benefits to developers managing large codebases: Increased Productivity: Faster access to relevant code snippets and functions without manual searching. Improved Accuracy: More precise code suggestions that align with the project's existing structures and conventions. Seamless Collaboration: Teams can ensure consistency across different parts of the codebase, even when working remotely. Conclusion As codebases continue to expand, the limitations of traditional AI coding assistants become more pronounced. Retrieval-Augmented Generation offers a powerful solution by enabling AI models to access and utilize relevant information from vast codebases effectively. Codeium Windsurf exemplifies how embracing RAG can lead to more intelligent, context-aware coding assistance, ultimately enhancing developer experience and productivity. References Codeium Documentation: Context Awareness GitHub Copilot Documentation Introduction to Retrieval-Augmented Generation (RAG) OpenAI API Tokenizer How Context Length Impacts Language Models Like Like Celebrate Support Love Insightful Funny Comment Copy LinkedIn Facebook Twitter Share 10 To view or add a comment, sign in More articles by Luciano Ayres Mental Model-Driven Development (MMDD): A Human-Centric Method for Efficient AI-Assisted Coding Dec 7, 2024 Mental Model-Driven Development (MMDD): A Human-Centric Method for Efficient AI-Assisted Coding Abstract AI-assisted coding has made it easy to rely on Large Language Models (LLMs) for everything, from architecture… 8 AI Morphic Framework (AIMF): A Vision for Self-Evolving Software Dec 4, 2024 AI Morphic Framework (AIMF): A Vision for Self-Evolving Software Abstract The AI Morphic Framework represents a transformative approach to software development, where applications are… 7 2 Comments Create Projects with Generative AI for Free: Using Google Gemini API in Colab Nov 9, 2024 Create Projects with Generative AI for Free: Using Google Gemini API in Colab I firmly believe that generative AI should be free for everyone. Making these powerful tools accessible empowers… 11 Why Go is a Game Changer for Engineering Managers and Their Teams Nov 3, 2024 Why Go is a Game Changer for Engineering Managers and Their Teams Imagine empowering your engineering team to achieve unparalleled productivity, build lightning-fast applications, and… 20 2 Comments YAML vs. JSON: Why YAML Wins for Large Language Model Outputs Oct 16, 2024 YAML vs. JSON: Why YAML Wins for Large Language Model Outputs As Large Language Models (LLMs) such as GPT-4 continue to be deployed in various applications, the format in which they… 12 4 Comments Fine-Tune Your AI with Ollama Model Files: A Step-by-Step Tutorial Oct 14, 2024 Fine-Tune Your AI with Ollama Model Files: A Step-by-Step Tutorial If you’re diving into the world of AI models, you’ve probably heard of Ollama—an innovative platform that allows you to… 23 AI Model Training Guide: Understanding Training, Validation, and Test Data Oct 9, 2024 AI Model Training Guide: Understanding Training, Validation, and Test Data Data is often called the new oil in artificial intelligence (AI) and machine learning (ML), but having vast amounts of… 4 Fine-Tuning the Engine: Techniques for Optimizing AI Model Performance Oct 2, 2024 Fine-Tuning the Engine: Techniques for Optimizing AI Model Performance Just as a skilled car mechanic fine-tunes a car engine to achieve optimal performance, AI model developers must… 6 Generative AI and Humans: An Adversarial Relationship Parallel to GANs Sep 25, 2024 Generative AI and Humans: An Adversarial Relationship Parallel to GANs The rapid advancement of generative AI technologies has introduced new dynamics in the relationship between humans and… 7 2 Comments 6 Ways to Apply Machine Learning Principles for Career Growth Sep 18, 2024 6 Ways to Apply Machine Learning Principles for Career Growth Success in today's professional world hinges on the ability to learn quickly and adapt to change. As technology… 10 Show more See all articles Insights from the community Data Science What are the best practices for collaborative scientific computing projects in Python? Algorithms Here's how you can master the key problem solving techniques for Algorithm professionals. Computer Science A junior developer is struggling with complex algorithms. How can you guide them to success? Artificial Intelligence How can you manage version control in AI programming languages? Programming How can you use structured programming to improve chatbot accuracy and efficiency? Artificial Intelligence How can you build a chatbot from scratch using programming languages? Artificial Intelligence How can you make sure your AI code is scalable? Algorithms Here's how you can efficiently lead a team of algorithm developers. Algorithms Here's how you can handle feedback on your algorithmic code maintainability. Journalism Here's how you can master the most in-demand coding languages for journalists in today's digital age. Show more Show less Others also viewed AI Changed My Coding Style Jason Arbon 7mo Learning AI with GitHub Copilot Blockchain Council 10mo Creating Your Own Intelligent GPT Agent: The Coding Wingman (FastAPI + GitHub Search API) Reuven Cohen 1y Is Machine-Written Code Safe? A Deep Dive into the Future of Programming Miracle Software Systems, Inc 1mo AI - Monday, December 30, 2024: Commentary with Notable and Interesting News, Articles, and Papers Robert Sutor 1mo Best AI-Powered Coding Assistant Tools in 2024 Hanu Koshti 3mo Using AI to encourage best practices in the code review process Abi Noda 7mo Introducing the New Coding Template: An app template for all your coding use cases. Clarifai 8mo Open Source Large Language Models (LLMs) in Software Development: Transforming the Coding Landscape Kartheek Thangella 2mo AI coding assistant? Say hello to Duet AI Deqode 1y Show more Show less Explore topics Sales Marketing IT Services Business Administration HR Management Engineering Soft Skills See All LinkedIn © 2025 About Accessibility User Agreement Privacy Policy Cookie Policy Copyright Policy Brand Policy Guest Controls Community Guidelines العربية (Arabic) বাংলা (Bangla) Čeština (Czech) Dansk (Danish) Deutsch (German) Ελληνικά (Greek) English (English) Español (Spanish) فارسی (Persian) Suomi (Finnish) Français (French) हिंदी (Hindi) Magyar (Hungarian) Bahasa Indonesia (Indonesian) Italiano (Italian) עברית (Hebrew) 日本語 (Japanese) 한국어 (Korean) मराठी (Marathi) Bahasa Malaysia (Malay) Nederlands (Dutch) Norsk (Norwegian) ਪੰਜਾਬੀ (Punjabi) Polski (Polish) Português (Portuguese) Română (Romanian) Русский (Russian) Svenska (Swedish) తెలుగు (Telugu) ภาษาไทย (Thai) Tagalog (Tagalog) Türkçe (Turkish) Українська (Ukrainian) Tiếng Việt (Vietnamese) 简体中文 (Chinese (Simplified)) 正體中文 (Chinese (Traditional)) Language

Source: https://www.google.com/url?url=https://support.google.com/websearch?p%3Dfeatured_snippets%26hl%3Den-BE&rct=j&q=&esrc=s&opi=89978449&usg=AOvVaw0-xXX9L80985UB-G9Jvm5i&hl=en-BE&sa=X&ved=2ahUKEwjbuMiqtK2LAxVYU6QEHeTvKlkQrpwBegQIGxAB


Source: https://www.google.com/url?url=https://support.google.com/websearch?p%3Dfeatured_snippets%26hl%3Den-BE&rct=j&q=&esrc=s&opi=89978449&usg=AOvVaw0-xXX9L80985UB-G9Jvm5i&hl=en-BE&sa=X&ved=2ahUKEwjbuMiqtK2LAxVYU6QEHeTvKlkQrpwBegQIGxAC


Source: https://aws.amazon.com/what-is/retrieval-augmented-generation/#:~:text=Retrieval%2DAugmented%20Generation%20(RAG),sources%20before%20generating%20a%20response.
What is RAG? - Retrieval-Augmented Generation AI Explained - AWS Skip to main content Click here to return to Amazon Web Services homepage About AWS Contact Us Support English My Account Sign In Create an AWS Account Close Profile Your profile helps improve your interactions with select AWS experiences. Login Close Profile Your profile helps improve your interactions with select AWS experiences. View profile Log out Amazon Q Products Solutions Pricing Documentation Learn Partner Network AWS Marketplace Customer Enablement Events Explore More Close عربي Bahasa Indonesia Deutsch English Español Français Italiano Português Tiếng Việt Türkçe Ρусский ไทย 日本語 한국어 中文 (简体) 中文 (繁體) Close My Profile Sign out of AWS Builder ID AWS Management Console Account Settings Billing & Cost Management Security Credentials AWS Personal Health Dashboard Close Support Center Expert Help Knowledge Center AWS Support Overview AWS re:Post Click here to return to Amazon Web Services homepage Close Profile Your profile helps improve your interactions with select AWS experiences. Login Close Profile Your profile helps improve your interactions with select AWS experiences. View profile Log out Close Profile Your profile helps improve your interactions with select AWS experiences. View profile Log out Get Started for Free Contact Us Products Solutions Pricing Introduction to AWS Getting Started Documentation Training and Certification Developer Center Customer Success Partner Network AWS Marketplace Support AWS re:Post Log into Console Download the Mobile App What is Cloud Computing? Cloud Computing Concepts Hub Artificial Intelligence Generative AI What is RAG (Retrieval-Augmented Generation)? Create an AWS Account Explore Free AI Offers Build, deploy, and run artificial intelligence applications in the cloud for free Check out Artificial Intelligence Services Innovate faster with the most comprehensive set of AI services Browse AI Trainings Build in-demand AI skills with course, tutorial, and resources Read AI & Machine Learning Blogs Read about the latest AWS AI & ML product news and best practices What is Retrieval-Augmented Generation? Why is Retrieval-Augmented Generation important? What are the benefits of Retrieval-Augmented Generation? How does Retrieval-Augmented Generation work? What is the difference between Retrieval-Augmented Generation and semantic search? How can AWS support your Retrieval-Augmented Generation requirements? What is Retrieval-Augmented Generation? Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts. Why is Retrieval-Augmented Generation important? LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has. Known challenges of LLMs include: Presenting false information when it does not have the answer. Presenting out-of-date or generic information when the user expects a specific, current response. Creating a response from non-authoritative sources. Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things. You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate! RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response. What are the benefits of Retrieval-Augmented Generation? RAG technology brings several benefits to an organization's generative AI efforts. Cost-effective implementation Chatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable. Current information Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users. Enhanced user trust RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution. More developer control With RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM's information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications. How does Retrieval-Augmented Generation work? Without RAG, the LLM takes the user input and creates a response based on information it was trained on—or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process. Create external data The new data outside of the LLM's original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand. Retrieve relevant information The next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, "How much annual leave do I have?" the system will retrieve annual leave policy documents alongside the individual employee's past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations. Augment the LLM prompt Next, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries. Update external data The next question may be—what if the external data becomes stale? To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents. You can do this through automated real-time processes or periodic batch processing. This is a common challenge in data analytics—different data-science approaches to change management can be used. The following diagram shows the conceptual flow of using RAG with LLMs. What is the difference between Retrieval-Augmented Generation and semantic search? Semantic search enhances RAG results for organizations wanting to add vast external knowledge sources to their LLM applications. Modern enterprises store vast amounts of information like manuals, FAQs, research reports, customer service guides, and human resource document repositories across various systems. Context retrieval is challenging at scale and consequently lowers generative output quality. Semantic search technologies can scan large databases of disparate information and retrieve data more accurately. For example, they can answer questions such as, "How much was spent on machinery repairs last year?” by mapping the question to the relevant documents and returning specific text instead of search results. Developers can then use that answer to provide more context to the LLM. Conventional or keyword search solutions in RAG produce limited results for knowledge-intensive tasks. Developers must also deal with word embeddings, document chunking, and other complexities as they manually prepare their data. In contrast, semantic search technologies do all the work of knowledge base preparation so developers don't have to. They also generate semantically relevant passages and token words ordered by relevance to maximize the quality of the RAG payload. How can AWS support your Retrieval-Augmented Generation requirements? Amazon Bedrock is a fully-managed service that offers a choice of high-performing foundation models—along with a broad set of capabilities—to build generative AI applications while simplifying development and maintaining privacy and security. With knowledge bases for Amazon Bedrock, you can connect FMs to your data sources for RAG in just a few clicks. Vector conversions, retrievals, and improved output generation are all handled automatically. For organizations managing their own RAG, Amazon Kendra is a highly-accurate enterprise search service powered by machine learning. It provides an optimized Kendra Retrieve API that you can use with Amazon Kendra’s high-accuracy semantic ranker as an enterprise retriever for your RAG workflows. For example, with the Retrieve API, you can: Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance. Use pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites. Support a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files. Filter responses based on those documents that the end-user permissions allow. Amazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples. Get started with Retrieval-Augmented Generation on AWS by creating a free account today Next Steps on AWS Check out additional product-related resources Innovate faster with the most comprehensive set of Generative AI services Sign up for a free account Instant get access to the AWS Free Tier. Sign up Start building in the console Get started building in the AWS management console. Sign in Sign In to the Console Learn About AWS What Is AWS? What Is Cloud Computing? AWS Accessibility What Is DevOps? What Is a Container? What Is a Data Lake? What is Artificial Intelligence (AI)? What is Generative AI? What is Machine Learning (ML)? AWS Cloud Security What's New Blogs Press Releases Resources for AWS Getting Started Training and Certification AWS Solutions Library Architecture Center Product and Technical FAQs Analyst Reports AWS Partners Developers on AWS Developer Center SDKs & Tools .NET on AWS Python on AWS Java on AWS PHP on AWS JavaScript on AWS Help Contact Us Get Expert Help File a Support Ticket AWS re:Post Knowledge Center AWS Support Overview Legal AWS Careers Create an AWS Account Amazon is an Equal Opportunity Employer: Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age. Language عربي Bahasa Indonesia Deutsch English Español Français Italiano Português Tiếng Việt Türkçe Ρусский ไทย 日本語 한국어 中文 (简体) 中文 (繁體) Privacy | Accessibility | Site Terms | Cookie Preferences | © 2024, Amazon Web Services, Inc. or its affiliates. All rights reserved. Ending Support for Internet Explorer Got it AWS support for Internet Explorer ends on 07/31/2022. Supported browsers are Chrome, Firefox, Edge, and Safari. Learn more » Got it

Source: https://medium.com/@devmallyakarar/rag-retrieval-augmented-generation-in-depth-with-code-implementation-using-langchain-llamaindex-1f77d1ca2d33
RAG: Retrieval Augmented Generation In-Depth with Code Implementation using Langchain, Langchain Agents, LlamaIndex and LangSmith. | by Devmallya Karar | Medium Open in app Sign up Sign in Write Sign up Sign in RAG: Retrieval Augmented Generation In-Depth with Code Implementation using Langchain, Langchain Agents, LlamaIndex and LangSmith. Devmallya Karar · Follow 54 min read · Sep 20, 2024 -- Listen Share Retrieval Augmented Generation (RAG) represents an innovative framework that combines the strengths of information retrieval and generative modelling to address complex query tasks in natural language processing. It was notably developed to enhance the capabilities of sequence-to-sequence models by dynamically retrieving external documents and incorporating their information during the generation process. At its core, RAG operates by integrating a pre-trained language model with a neural retrieval mechanism. This integration allows the model to access a vast external knowledge base (typically a corpus of documents) to fetch relevant information that aids in generating more informed and accurate responses. The RAG model effectively turns the challenge of generating language from a purely predictive task into an information-grounded task. The architecture of RAG can be primarily divided into two components: the retriever and the generator. The retriever is tasked with fetching relevant documents from the corpus based on the input query, while the generator, a sequence-to-sequence model, uses both the original query and the retrieved documents to produce the final output. Retrieval Mechanism The retrieval component of RAG is typically built on a dense vector space model where both the documents in the corpus and the input query are embedded into a high-dimensional space. The similarity between the query and document vectors (often calculated using dot product or cosine similarity) determines the relevance of the documents to the query. This process can be mathematically represented as: sim(q,d) = qTd where q and d are vector representations of the query and a document, respectively. Retrieval-Augmented Generation (RAG) system integrated with a Large Language Model (LLM). It outlines the workflow of a Retrieval-Augmented Generation (RAG) system integrated with a Large Language Model (LLM). This diagram illustrates how RAG systems enhance the capabilities of LLMs by incorporating external knowledge sources during the response generation process. Here’s a breakdown of each step in the flow: Prompt + Query: The process begins with an initial prompt and a query. This input can be any text-based question or request for information that the user wants to address using the system. Query: The system isolates the query from the initial input and uses it as the basis for retrieving information. The isolated query serves as a focused input for the retrieval system. Search Relevant Information: The query is used to search through external knowledge sources. These could be databases, documents, or any organized corpus that the system has access to. The search process aims to find data that is relevant to the query. Relevant Information for Enhanced Context: The information retrieved from the knowledge sources is then combined with the original input to create an enhanced context. This enriched context now includes both the original prompt (and/or query) and the additional relevant information fetched from external sources. This step is critical as it allows the LLM to have access to a broader and more specific context than what was originally provided. Generated Text Response: With an enhanced context that integrates the retrieved information, the Large Language Model endpoint then generates a response. This response is expected to be more informed, accurate, and relevant, as it is based on both the original query and the additional context provided by the retrieved information. Prompt + Query + Enhanced Context → Large Language Model Endpoint: This step in the flow represents the process of sending the enhanced input (original prompt/query plus retrieved information) to the LLM for processing. The LLM endpoint, likely a sophisticated neural network model, processes this information to generate a coherent and contextually enriched response. This architecture demonstrates the power of combining traditional LLMs with retrieval mechanisms to significantly enhance the model’s ability to generate detailed and accurate outputs by grounding the responses in retrieved external data. This approach is particularly useful in scenarios where the LLM needs to provide responses based on factual information or when handling complex queries that require detailed understanding and background knowledge. Mathematical Derivation of the Retrieval Mechanism The retrieval mechanism typically employs a dense vector space model where documents and queries are represented as high-dimensional vectors. The goal is to find document vectors that are closest to the query vector in this space, implying relevance to the query. Vector Representation Embedding: Each document d in the corpus and the query q are transformed into vectors: d = fdoc(d;θdoc) q = fquery(q;θquery). Here, fdoc and fquery are embedding functions (typically neural networks) that map the text to a high-dimensional vector space, and θdoc and θquery are the parameters of these functions. 2. Normalization: Often, these vectors are normalized to have unit length to simplify the calculation of similarity: Similarity Calculation Similarity Score: The similarity between the query and each document is calculated using the dot product of their normalized vectors: sim(q,d)=q^Td^ This can also be computed using cosine similarity since the vectors are normalized. Generation Mechanism Once the relevant documents are retrieved, the generator component, which leverages a transformer-based model like BERT or GPT, takes over. The input to this model is a combination of the input query and the contents of the retrieved documents. The generation process is governed by the conditional probability of generating a sequence of tokens y (the output) given the input sequence x and the retrieved documents D. This can be mathematically expressed as: Here, y<i represents all tokens before position iii, ensuring that the generation of each token conditions on the preceding tokens, the input query, and the content of the retrieved documents. Training Training a RAG model involves optimizing both the retriever and the generator components. The goal is to improve the relevance of the retrieved documents as well as the accuracy of the generated text. The training objective typically combines the loss functions from both retrieval and generation tasks: Retrieval Loss: This loss measures how well the retriever is selecting relevant documents. A common approach is to use a ranking loss like triplet loss, which encourages the retriever to rank the correct document above incorrect ones. Generation Loss: This is usually a cross-entropy loss that measures how well the generator predicts the target sequence given the input and retrieved documents. The combined objective function is often formulated as a weighted sum of both losses: where α is a hyperparameter that balances the importance of the retrieval and generation tasks. Issues happens with Search & Retrieval The search and retrieval method significantly boosts the efficiency and accuracy of Large Language Models (LLMs), but it also introduces several potential challenges that could negatively affect the user experience if not addressed early. A common issue occurs when the system receives a query that does not closely align with any of the entries in the vector store — akin to searching for a needle in a haystack and not finding the needle. This situation often results from unique or highly specific queries. As a result, the system may only be able to pull the “most similar” chunks of data, which may not be wholly relevant. Consequently, this mismatch can lead to inadequate responses from the LLM. Since the quality of the LLM’s outputs relies heavily on the relevance of the retrieved data chunks, an inappropriate match might lead to responses that are only tangentially related to, or even completely disconnected from, the user’s original query. Image by Ariza Irrelevant or subpar responses from the Large Language Model (LLM) can lead to user frustration, reducing their satisfaction and potentially eroding trust in both the system and the product as a whole. To prevent these issues, it is critical to monitor three key areas: Query Density (Drift): This refers to the coverage of user queries by the vector store. A significant drift in query density can indicate that the vector store is not fully capturing the range of user queries, leading to a lack of closely related data chunks. Regular monitoring of query density can help identify these gaps or deficiencies. With these insights, we can enhance the vector store by adding more relevant data chunks or refining existing ones, thereby improving the system’s capability to retrieve data in response to user queries. Ranking Metrics: These metrics assess the effectiveness of the search and retrieval system in identifying the most relevant chunks. A decline in these metrics suggests that the system’s ability to differentiate between relevant and irrelevant chunks may require fine-tuning. User Feedback: Encouraging user feedback on the quality and relevance of the LLM’s responses is crucial for measuring user satisfaction and pinpointing areas for improvement. Analyzing this feedback regularly can reveal patterns and trends, which can then inform necessary adjustments to the application. By focusing on these areas, we can enhance the responsiveness and reliability of the LLM, ensuring that it meets user expectations and maintains their trust in the system. Optimization of Search & Retrieval Optimizing the search and retrieval processes is a continuous effort that spans the entire lifecycle of your LLM-powered application, starting from the building phase through to post-production. In the building phase, it is crucial to focus on developing a robust testing and evaluation strategy. This proactive approach helps in identifying potential issues early, allowing for optimization of strategies and laying a strong foundation for the system. Important areas to concentrate on during this phase include: Chunking Strategy: Careful evaluation of how information is segmented and processed at this stage can illuminate areas where performance improvements are necessary. Retrieval Performance: Regular assessments of the system’s ability to fetch information will reveal whether there is a need to implement different tools or strategies, such as context ranking or utilizing HYDE. As the application moves into the post-production phase after release, the optimization efforts must persist. With a well-defined evaluation strategy in place, it becomes feasible to proactively detect emerging issues and continually enhance the model’s performance. Strategies to consider in this phase include: Expanding our Knowledge Base: By adding more documentation, the system’s capacity to provide accurate and customized responses is significantly improved. A larger dataset enables the LLM to offer more precise responses. Refining Chunking Strategy: Further adjustments to how information is segmented and processed can result in substantial performance improvements. Enhancing Context Understanding: Implementing an additional ‘context evaluation’ step allows the system to integrate the most pertinent context into the LLM’s responses, enhancing accuracy. Types of Search Vector databases, while exceptional at semantic search, are not a universal solution for all search-related challenges — they excel in certain aspects but fall short in others. Traditional keyword search, in many instances, can yield more pertinent results and enhance user satisfaction. This outcome is often attributed to the way results are ranked in vector databases; results with higher cosine similarity scores may overshadow partial matches that contain specific keywords the user is searching for, thereby reducing the perceived relevance of the results. On the other hand, traditional keyword search comes with its own set of limitations. For example, if a user inputs a term that is semantically close to but not exactly the same as the terms stored in the database, keyword search may fail to return useful and relevant results. This limitation highlights the inherent trade-off in search methodologies, where no single approach is perfect for every scenario. In response to these trade-offs, practical applications in search and retrieval often necessitate a combination of both keyword and vector search strategies. Vector databases play a crucial role here as they store embeddings that enable semantic similarity searches and are scalable to handle very large datasets. To summarize the distinct features and applications of each search type: Keyword Search: This method is ideal when users have a clear understanding of what they are looking for and expect to find results that match specific phrases or exact terms from their queries. It operates independently of vector databases. Vector Search: This approach is beneficial when users are unsure of the exact terms to use or when the query is more conceptual. It relies on a vector database to find semantically relevant results. Hybrid Search (Keyword + Vector): This method typically merges results from both keyword searches and vector searches. It then employs cross-encoder models to re-rank these results, ensuring that the most relevant outcomes from both methods are considered. This approach requires both a document database for keyword searches and a vector database for semantic searches. This combination, or hybrid approach, leverages the strengths of each method to deliver a more robust and accurate search experience, catering to a broader range of user needs and search scenarios. Semantic Search Semantic search aims to enhance the accuracy of search results by understanding the underlying intent and contextual meaning of a search query, rather than relying solely on lexical matches like traditional search engines. This method allows the system to recognize and retrieve synonyms and conceptually similar terms, improving the relevance of search results. Background The foundational concept of semantic search involves embedding all entries in a corpus — ranging from sentences and paragraphs to entire documents — into a vector space. When a search query is received, it is also transformed into a corresponding vector within the same space. By comparing these vectors, the system identifies entries in the corpus that are semantically close to the query, ensuring a high degree of semantic overlap. This approach allows the search engine to understand content at a deeper level than simple keyword matching. Symmetric vs. Asymmetric Semantic Search It is important to distinguish between symmetric and asymmetric semantic search, as they cater to different types of queries and content: Symmetric Semantic Search: This type involves queries and corpus entries that are similar in length and content depth. An example of this would be searching for questions similar in structure and intent, such as “How to learn Python online?” finding a match in “How to learn Python on the web?”. In symmetric search scenarios, it’s conceivable that the query and the corpus entries could be interchanged without losing relevance. Asymmetric Semantic Search: In this setup, queries are typically shorter — often just a question or a few keywords — while the desired results are more detailed or longer paragraphs that provide an answer. For example, the query “What is Python?” might return a detailed paragraph explaining that “Python is an interpreted, high-level and general-purpose programming language. Python’s design philosophy …”. Unlike symmetric search, in asymmetric scenarios, reversing the roles of the query and the corpus entries generally doesn’t yield useful results. Understanding these distinctions is crucial for optimizing the design and functionality of semantic search systems, as it directly influences how queries are processed and how results are generated. This differentiation ensures that the search system can effectively handle a wide range of query types and deliver the most relevant and contextually appropriate results. Retrieval Algorithms Similarity Search (Vanilla Search) & Maximum Marginal Relevance(MMR) When retrieving documents from a large corpus, most methods use similarity metrics like cosine similarity, Euclidean distance, or dot product to identify documents that are closest to the query or question posed. These metrics effectively pull the most similar documents based on the query, aligning well with the search intent. However, there arises a challenge when we desire not only similarity but also diversity among the retrieved documents. This need is particularly critical when we want to avoid redundancy and provide a broad perspective on a topic. This is where the concept of Maximum Marginal Relevance (MMR) becomes invaluable. Understanding Maximum Marginal Relevance (MMR) MMR is designed to balance between the relevance of the documents to the query and the diversity among the documents selected for retrieval. This approach helps ensure that the returned set of documents is both relevant and varied, providing a comprehensive view of the subject matter. In practical applications like unsupervised learning for key phrase extraction, MMR can significantly enhance the utility and information quality. For example, consider a scenario where key phrases extracted from product reviews include terms like “Good Product,” “Great Product,” “Nice Product,” “Excellent Product,” “Easy Install,” “Nice UI,” “Lightweight.” Without MMR, the system might overly focus on similar phrases that frequently appear like “Good Product,” “Great Product,” and “Nice Product,” which all describe the same feature of the product and rank highly due to their frequency. Implementing MMR in Key Phrase Selection To address this, if we only have space to display five key phrases, it’s crucial to select diverse phrases to cover a broader range of attributes, ensuring they provide varied insights into the product. MMR helps in: Removing Redundant Phrases: By using cosine similarity, MMR identifies and filters out phrases that are too similar to each other. Re-ranking the Key Phrases: MMR re-ranks the remaining phrases to prioritize those that add new information, thereby maximizing the relevance and diversity of the phrases shown. Example of MMR in Action Suppose after the initial ranking, the phrases “Good Product,” “Great Product,” and “Nice Product” are all high on the list. MMR would evaluate the similarity among these phrases and likely retain only one or two while elevating others like “Easy Install” and “Lightweight,” which diversify the content presented. Broader Application and Future Discussion MMR is just one of the many advanced retrieval strategies. Others include Multi Query Retrieval, Long-Context Reorder, Multi-Vector Retriever, Parent Document Retriever, Self-Querying, and Time-weighted Vector Store Retrieval. These methodologies, which offer various enhancements and optimizations for specific scenarios, will be explored in further detail in an upcoming blog post. By integrating MMR into document retrieval and key phrase extraction processes, we ensure that the information presented is not only relevant but also diverse, thereby enriching the user’s experience and providing a fuller understanding of the topic. Retrieve & Re-Rank Semantic search has greatly benefited from advancements in natural language processing, particularly through the use of models like Sentence Transformers. These models are designed to compute embeddings for texts ranging from short queries to lengthy paragraphs, making them ideal for semantic search applications where the understanding of context and meaning is crucial. Using Sentence Transformers in Semantic Search Sentence Transformers generate embeddings that capture the semantic nuances of texts, enabling them to match queries with content that is contextually relevant, even if the exact keywords are not present. Here’s how it generally works: Embedding Generation: Each piece of text — whether a query, sentence, or paragraph — is transformed into a vector (embedding) using the Sentence Transformer model. This process involves encoding the textual information into a high-dimensional space where semantically similar phrases are placed closer together. Embedding Storage: These embeddings are then stored in a searchable format. Modern vector databases or approximate nearest neighbor (ANN) search libraries (like FAISS or Annoy) are used to manage these embeddings efficiently, allowing for quick retrieval even in large-scale datasets. Semantic Matching: When a query is received, it is also converted into an embedding. The system then searches the vector database to find the closest embeddings (i.e., the most semantically related texts) to the query embedding. This method enhances the capability of search systems to return more relevant and meaningful results by understanding the content’s context rather than relying solely on keyword matches. Enhancing Semantic Search with Retrieve & Re-Rank For more complex search tasks, such as those found in question-answering systems, a two-step process called Retrieve & Re-Rank can significantly improve the effectiveness of the search: Retrieval Phase: In the first phase, the system uses the query’s embedding to fetch a preliminary set of relevant documents from the corpus. This step is focused on speed and recall, aiming to gather a broad set of potentially relevant answers without yet fine-tuning for the best match. Re-Ranking Phase: In the second phase, the system employs a more computationally intensive model to analyze the context and relevance of each retrieved document more deeply. This might involve additional features like the exactness of match, the depth of answer, and relevance to the query’s intent. Advanced models, possibly including cross-encoders or even larger transformers, evaluate the documents in relation to the query to determine the best fit. This Retrieve & Re-Rank method allows for an efficient handling of large datasets in the initial retrieval while ensuring high accuracy in the final results through a detailed re-ranking. It balances the need for quick response times with the desire for high precision, making it particularly suitable for complex query answering tasks where both relevance and context sensitivity are critical. Implementing these sophisticated search mechanisms not only improves user satisfaction through more accurate and relevant responses but also enhances the robustness of search systems in handling diverse and complex queries. Retrieve & Re-Rank Pipeline A pipeline for information retrieval / question-answering retrieval that works well is the following. All components are provided and explained in this article: This method is particularly effective for complex information retrieval tasks where precision is critical. Here’s a detailed look at each step involved in this search methodology: Step 1: Initial Retrieval The first stage involves the use of a retrieval system to gather a large set of potential results, typically around 100, that may be relevant to the given search query. This retrieval can be conducted using one of two primary methods: Lexical Search: Utilizing traditional search engines like ElasticSearch, this method relies on keyword matching. It scans documents to find occurrences of the words included in the search query. While fast and efficient, lexical search may retrieve documents that match the query terms but miss contextual relevance or semantic similarity. Dense Retrieval: Employing a bi-encoder architecture, dense retrieval processes both the query and the documents in the corpus into dense vector representations. These embeddings are then compared (usually via cosine similarity) to identify documents whose contexts are semantically close to that of the query. This method tends to pull more contextually relevant documents compared to lexical search but may still include some less relevant results. Step 2: Re-Ranking with a Cross-Encoder Once the initial list of potential hits is retrieved, the second stage of the process begins: Re-Ranking: A cross-encoder is employed to re-evaluate the relevance of each candidate document to the search query. Unlike the bi-encoder used for dense retrieval, a cross-encoder takes the pair of the query and each individual document as input and outputs a relevance score. This model is more computationally intensive because it performs a deeper analysis of the interaction between the query and document content, considering the nuances of their relationship. Scoring and Ranking: The cross-encoder scores each document for its relevance to the query. These scores are then used to sort the documents, resulting in a ranked list where the most relevant documents appear first. This refined list significantly improves the quality of the results that will be presented to the user. Output The final output of this two-step process is a carefully curated list of documents, ranked by their relevance to the user’s query. This method effectively combines the speed and scalability of initial retrieval methods (either lexical or dense retrieval) with the precision of deep learning models (cross-encoder), ensuring that users receive the most accurate and contextually appropriate responses to their searches. This dual-stage retrieval and re-ranking system is especially beneficial in environments where the accuracy of search results is paramount, such as in academic research, legal document retrieval, or any professional setting where precision and context are crucial. Pre-trained Bi-Encoders (Retrieval) The bi-encoder produces embeddings independently for your paragraphs and for your search queries. You can use it like this: from sentence_transformers import SentenceTransformer model = SentenceTransformer("multi-qa-mpnet-base-dot-v1") docs = [     "My first paragraph. That contains information",     "Python is a programming language.", ] document_embeddings = model.encode(docs) query = "What is Python?" query_embedding = model.encode(query) Now let’s focus on chunking in-depth !! Introduction to Chunking Chunking in Retrieval-Augmented Generation (RAG) refers to the process of splitting a large document or dataset into smaller, more manageable “chunks” before retrieving and generating relevant information. This technique is essential in RAG systems because it enables more efficient retrieval by ensuring that only pertinent parts of a document are processed, rather than the entire text. When a user query is made, chunking allows the retriever to focus on smaller, contextually relevant sections, improving the quality of the generated response. The system retrieves chunks of text from a knowledge base and passes them to the generator model (like GPT), which synthesizes an answer. The size of the chunk is critical: too large, and irrelevant information may get included; too small, and important context could be lost. Often, chunk sizes are determined based on token limits or the nature of the data (e.g., paragraphs, sentences). Effective chunking improves retrieval precision, minimizes unnecessary computation, and enhances overall response relevance, especially for applications like question-answering, summarization, and decision support in large corpora. Image In simple way chunking is breaking down large pieces of text into smaller, more manageable chunks. This process has two main phases: Data Preparation: Reliable data sources are segmented into chunked documents and stored in a database. The database can be a vector store if you generate embeddings within the chunks. Retrieval: When a user asks a question, the system searches through the document chunks using vector search, full-text search, or a combination of both. This process identifies and retrieves the chunks most relevant to the user’s query. Why Chunking is Crucial in RAG Architectures ? Chunking is indeed a crucial element in Retrieval-Augmented Generation (RAG) architectures, and your points capture the nuances effectively. Here’s an expanded look at why chunking matters in RAG systems: Increased Accuracy with Smaller Chunks: When text is broken down into smaller chunks, each piece becomes more manageable for the system to search through. This leads to more accurate retrieval of relevant information. Smaller chunks allow the RAG system to avoid overloading the generative model with unnecessary context and focus on pinpointing the most relevant pieces of information based on the input query. Enhanced Contextual Generation: Larger chunks can provide better context for generative models, making it easier for the system to generate responses that are more coherent and contextually appropriate. By balancing the size of chunks, RAG systems can make use of the additional context available from bigger chunks, leading to more nuanced answers while still maintaining relevance Scalability and Performance: Chunking significantly reduces computational overhead by breaking large datasets into manageable pieces. These smaller chunks can be processed in parallel, allowing the RAG architecture to scale effectively while ensuring fast, efficient retrieval and generation processes. This parallelization is especially important when working with vast amounts of data or when speed is critical. Balance between Chunk Size and Retrieval Needs: One challenge is ensuring that chunks are neither too small, which can lose context, nor too large, which can reduce retrieval precision. Therefore, an optimal chunk size is essential, balancing the trade-off between retrieval accuracy and contextual understanding. Techniques to Improve Chunking Several advanced methods can be employed to refine chunking: Fixed Character Sizes: This is a simple technique where text is divided into fixed-size segments. While easy to implement, it may not always capture the semantic boundaries between ideas, which can affect retrieval and generation quality. Here’s an example of how to implement fixed character size chunking using the code provided previously: # Sample text to chunk text = "This is the text I would like to chunk up. It is the example text for this exercise." # Set the chunk size chunk_size = 35 # Initialize a list to hold the chunks chunks = [] # Iterate over the text to create chunks for i in range(0, len(text), chunk_size):     chunk = text[i:i + chunk_size]     chunks.append(chunk) # Display the chunks print(chunks) # Output: ['This is the text I would like to ch', 'unk up. It is the example text for ', 'this exercise'] Using LangChain’s CharacterTextSplitter to achieve the same result: from langchain.text_splitter import CharacterTextSplitter # Initialize the text splitter with specified chunk size text_splitter = CharacterTextSplitter(chunk_size=35, chunk_overlap=0, separator='', strip_whitespace=False) # Create documents using the text splitter documents = text_splitter.create_documents([text]) # Display the created documents for doc in documents:     print(doc.page_content) # Output: # This is the text I would like to ch # unk up. It is the example text for # this exercise Recursive Character Text Splitting: Instead of arbitrary fixed sizes, this method uses separators like spaces, punctuation, or newlines to ensure chunks contain coherent units of information. It’s an improvement over fixed-length chunking as it considers natural language structures. Here’s an example of how to implement recursive character text splitting in Langchain: %pip install -qU langchain-text-splitters Firstly install the long-chain-text-splitters library if you haven’t done this yet. from langchain_text_splitters import RecursiveCharacterTextSplitter # Sample text to chunk text = """ The Olympic Games, originally held in ancient Greece, were revived in 1896 and have since become the world’s foremost sports competition, bringing together athletes from around the globe. """ # Initialize the recursive character text splitter with specified chunk size text_splitter = RecursiveCharacterTextSplitter(     # Set a really small chunk size, just to show.     chunk_size=30,     chunk_overlap=20,     length_function=len,     is_separator_regex=False, ) # Create documents using the text splitter documents = text_splitter.create_documents([text]) # Display the created documents for doc in documents:     print(doc.page_content) # Output: # “The Olympic Games, originally” # “held in ancient Greece, were” # “revived in 1896 and have” # “have since become the world’s” # “world’s foremost sports” # “competition, bringing together” # “together athletes from around” # “around the globe.” In this method, the text is first split by larger structures like paragraphs, and if the chunks are still too large, it further splits them using smaller structures like sentences. Each chunk maintains meaningful context and avoids cutting off vital information. Recursive character text splitting strikes a balance between simplicity and sophistication, providing a robust method for chunking that respects the text’s inherent structure. Document-Specific Splitting: Some documents (e.g., PDFs or Markdown files) require unique chunking strategies to account for format-specific characteristics such as headers, tables, or lists. Tailoring chunking methods to document structure can help capture more meaningful chunks. Here’s an example of how to implement document-specific splitting for Markdown and Python files: Markdown Splitting from langchain.text_splitter import MarkdownTextSplitter # Sample Markdown text markdown_text = """ # Fun in California ## Driving Try driving on the 1 down to San Diego ### Food Make sure to eat a burrito while you're there ## Hiking Go to Yosemite """ # Initialize the Markdown text splitter splitter = MarkdownTextSplitter(chunk_size=40, chunk_overlap=0) # Create documents using the text splitter documents = splitter.create_documents([markdown_text]) # Display the created documents for doc in documents:     print(doc.page_content) # Output: # # Fun in California\n\n## Driving # Try driving on the 1 down to San Diego # ### Food # Make sure to eat a burrito while you're # there # ## Hiking\n\nGo to Yosemite Python Code Splitting from langchain.text_splitter import PythonCodeTextSplitter # Sample Python code python_text = """ class Person:     def __init__(self, name, age):         self.name = name         self.age = age p1 = Person("John", 36) for i in range(10):     print(i) """ # Initialize the Python code text splitter python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0) # Create documents using the text splitter documents = python_splitter.create_documents([python_text]) # Display the created documents for doc in documents:     print(doc.page_content) # Output: # class Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age # p1 = Person("John", 36)\n\nfor i in range(10):\n    print(i) Document-specific splitting preserves the document’s logical structure, making the chunks more meaningful and contextually accurate. For example, headers and sections are separated in Markdown files, while classes and functions are used in Python code. This method enhances the system’s ability to retrieve and generate relevant responses by maintaining the integrity of different document types, thereby improving the overall performance and accuracy of the RAG system. Semantic Splitting: Using embeddings and semantic analysis allows systems to chunk based on meaning rather than structure. This advanced method ensures that each chunk contains conceptually related information, leading to more accurate retrieval. Here’s an example of how to implement semantic splitting using embeddings. from sklearn.metrics.pairwise import cosine_similarity from langchain.embeddings import OpenAIEmbeddings import re # Sample text text = """ One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear. Teachers and coaches implicitly told us the returns were linear. "You get out," I heard a thousand times, "what you put in." They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business. It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. """ # Splitting the text into sentences sentences = re.split(r'(?<=[.?!])\s+', text) sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(sentences)] # Combine sentences for context def combine_sentences(sentences, buffer_size=1):     for i in range(len(sentences)):         combined_sentence = ''         for j in range(i - buffer_size, i):             if j >= 0:                 combined_sentence += sentences[j]['sentence'] + ' '         combined_sentence += sentences[i]['sentence']         for j in range(i + 1, i + 1 + buffer_size):             if j < len(sentences):                 combined_sentence += ' ' + sentences[j]['sentence']         sentences[i]['combined_sentence'] = combined_sentence     return sentences sentences = combine_sentences(sentences) # Generate embeddings oai_embeds = OpenAIEmbeddings() embeddings = oai_embeds.embed_documents([x['combined_sentence'] for x in sentences]) # Add embeddings to sentences for i, sentence in enumerate(sentences):     sentence['combined_sentence_embedding'] = embeddings[i] # Calculate cosine distances def calculate_cosine_distances(sentences):     distances = []     for i in range(len(sentences) - 1):         embedding_current = sentences[i]['combined_sentence_embedding']         embedding_next = sentences[i + 1]['combined_sentence_embedding']         similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]         distance = 1 - similarity         distances.append(distance)         sentences[i]['distance_to_next'] = distance     return distances, sentences distances, sentences = calculate_cosine_distances(sentences) # Determine breakpoints and create chunks import numpy as np breakpoint_distance_threshold = np.percentile(distances, 95) indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold] # Combine sentences into chunks chunks = [] start_index = 0 for index in indices_above_thresh:     end_index = index     group = sentences[start_index:end_index + 1]     combined_text = ' '.join([d['sentence'] for d in group])     chunks.append(combined_text)     start_index = index + 1 if start_index < len(sentences):     combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])     chunks.append(combined_text) # Display the created chunks for i, chunk in enumerate(chunks):     print(f"Chunk #{i+1}:\n{chunk}\n") Semantic splitting uses embeddings to create semantically similar chunks, improving retrieval accuracy and contextual generation in RAG systems. Focusing on the meaning of the text ensures that each chunk contains coherent and relevant information, enhancing the performance and reliability of the RAG application. Agentic Splitting: Leveraging large language models (LLMs) to determine the most appropriate chunking boundaries based on both content and context. This intelligent splitting can adapt dynamically to the text, ensuring that each chunk is optimal for retrieval and generation purposes. How Does Agentic Chunking Work? The method of agentic chunking plays a vital role in improving the processing of long documents by large language models (LLMs). It addresses the limitations of both recursive character splitting and semantic splitting, which often fail to capture the coherent structure of a document. By breaking text into manageable pieces (chunks) based on complete sentences, agentic chunking maintains more logical integrity across the content. Key Features of Agentic Chunking: Evaluates Sentences Independently: Unlike methods based on token length or semantic shifts, agentic chunking evaluates each sentence as an independent unit. This helps avoid breaking sentences mid-thought and maintains logical coherence across splits. Preserves Contextual Clarity: When dealing with documents that may have pronouns or other references (such as “he” in your example), agentic chunking ensures that sentences are rewritten to maintain clarity. For instance, “He was leading NASA’s Apollo 11 mission” is clarified to “Neil Armstrong was leading NASA’s Apollo 11 mission.” Propositioning: This process rewrites sentences so that they become independent propositions, complete with explicit subjects, reducing ambiguity. Each sentence, when passed to the LLM, stands alone without relying on previous sentences for understanding. By using propositioning, sentences like: “On July 20, 1969, astronaut Neil Armstrong walked on the moon.” “Neil Armstrong was leading NASA’s Apollo 11 mission.” are now clearer and can be chunked independently, enabling the LLM to better manage relationships between sentences, even if they are far apart in the original document. This process significantly improves how LLMs handle texts by reducing ambiguity and enhancing the ability to chunk logically grouped sentences, ultimately improving downstream tasks such as summarization, content extraction, and question answering. Now, the LLM can individually check every sentence and allocate it to a chunk or create one if it is irrelevant. This is possible because every sentence has a subject. Implementing Agentic chunking Now, we have a rough idea of how agentic chunking works. We also know that the sentences need to be propositioned for it to work. However, there are many different ways to implement this; no single package does it for us. Let’s start with propositioning. Propositioning the text As we now understand propositioning, we can create our own prompt to let an LLM do this for us. Fortunately, an excellent prompt is hosted in the Langchain hub. Let’s pull the prompt template, create an LLM chain, and test it. obj = hub.pull("wfh/proposal-indexing") # You can explore the prompt template behind this by running the following: # obj.get_prompts()[0].messages[0].prompt.template llm = ChatOpenAI(model="gpt-4o") # A Pydantic model to extract sentences from the passage class Sentences(BaseModel):     sentences: List[str] extraction_llm = llm.with_structured_output(Sentences) # Create the sentence extraction chain extraction_chain = obj | extraction_llm # Test it out sentences = extraction_chain.invoke(     """     On July 20, 1969, astronaut Neil Armstrong walked on the moon .     He was leading the NASA's Apollo 11 mission.     Armstrong famously said, "That's one small step for man, one giant leap for mankind" as he stepped onto the lunar surface.     """ ) >>['On July 20, 1969, astronaut Neil Armstrong walked on the moon.',  "Neil Armstrong was leading NASA's Apollo 11 mission.",  'Neil Armstrong famously said, "That\'s one small step for man, one giant leap for mankind" as he stepped onto the lunar surface.'] To handle large texts effectively, splitting the text into paragraphs and applying propositioning within each paragraph is a better approach. This ensures that pronouns like “he” remain contextually tied to the relevant subject within the paragraph. By doing this, we maintain clarity within smaller sections of the text, ensuring accurate chunking and extraction. Using a Pydantic model to extract structured sentences after splitting by paragraphs allows for better handling of references and improves the model’s ability to infer meaning accurately. paragraphs = text.split("\n\n") propositions = [] for i, p in enumerate(paragraphs):     propositions = extraction_chain.invoke(p         propositions.extend(propositions) The above code snippet will create a list of propositions within each paragraph’s context. To create chunks using an LLM agent, especially after applying propositioning, here’s how the process works: Step-by-Step Process: Initialize an empty dictionary called chunks to store the grouped propositions. {     "12345": {         "chunk_id": "12345",         "propositions": [             "The month is October.",             "The year is 2023."         ],         "title": "Date & Time",         "summary": "This chunk contains information about dates and times, including the current month and year.",     },     "67890": {         "chunk_id": "67890",         "propositions": [             "One of the most important things that I didn't understand about the world as a child was the degree to which the returns for performance are superlinear.",             "Teachers and coaches implicitly told us that the returns were linear.",             "I heard a thousand times that 'You get out what you put in.'"         ],         "title": "Performance Returns",         "summary": "This chunk contains information about performance returns and how they are perceived differently from reality.",     } } As it encounters a new proposition, the agent either adds it to an existing chunk or creates a new chunk if no suitable one is found. The decision on whether an existing chunk matches is based on the incoming proposition and the chunk’s current summary. Additionally, if new propositions are added to a chunk, the agent can update the chunk’s summary and title to reflect the new information. This ensures that the metadata stays relevant as the chunk evolves. Let’s code them step by step. To create chunks for the first time, we need a function that initializes a chunk when a new proposition appears. Since there are no chunks initially, this function will create the first chunk to store the initial proposition. This function will also be used anytime the agent determines that a new chunk is necessary for a different proposition. The chunks are stored outside of the function to allow continuous updates by this and other functions. When creating a chunk, we use the LLM to generate both a title and a summary based on the first proposition added to the chunk. from langchain_core.prompts import ChatPromptTemplate from langchain_openai import ChatOpenAI llm = ChatOpenAI(temperature=0) chunks = {} def create_new_chunk(chunk_id, proposition):     summary_llm = llm.with_structured_output(ChunkMeta)     summary_prompt_template = ChatPromptTemplate.from_messages(         [             (                 "system",                 "Generate a new summary and a title based on the propositions.",             ),             (                 "user",                 "propositions:{propositions}",             ),         ]     )     summary_chain = summary_prompt_template | summary_llm     chunk_meta = summary_chain.invoke(         {             "propositions": [proposition],         }     )     chunks[chunk_id] = {         "summary": chunk_meta.summary,         "title": chunk_meta.title,         "propositions": [proposition],     } In Step II, we need a function to add new propositions to existing chunks as we continue scanning the document. When adding a new proposition to a chunk, the existing title and summary may no longer accurately represent the chunk’s content. Therefore, the function should re-evaluate these elements and update them if necessary. To simplify this process, we use an LLM to decide whether changes are needed for the title and summary. By configuring the LLM with a Pydantic model, the output is structured, ensuring that the updated title and summary are organized properly, rather than just returning random text. This structured approach ensures that any changes to the chunk’s title and summary remain consistent and aligned with the content. from langchain_core.pydantic_v1 import BaseModel, Field class ChunkMeta(BaseModel):     title: str = Field(description="The title of the chunk.")     summary: str = Field(description="The summary of the chunk.") def add_proposition(chunk_id, proposition):     summary_llm = llm.with_structured_output(ChunkMeta)     summary_prompt_template = ChatPromptTemplate.from_messages(         [             (                 "system",                 "If the current_summary and title is still valid for the propositions return them."                 "If not generate a new summary and a title based on the propositions.",             ),             (                 "user",                 "current_summary:{current_summary}\n\ncurrent_title:{current_title}\n\npropositions:{propositions}",             ),         ]     )     summary_chain = summary_prompt_template | summary_llm     chunk = chunks[chunk_id]     current_summary = chunk["summary"]     current_title = chunk["title"]     current_propositions = chunk["propositions"]     all_propositions = current_propositions + [proposition]     chunk_meta = summary_chain.invoke(         {             "current_summary": current_summary,             "current_title": current_title,             "propositions": all_propositions,         }     )     chunk["summary"] = chunk_meta.summary     chunk["title"] = chunk_meta.title     chunk["propositions"] = all_propositions In Step III, we need an agent that determines whether a proposition belongs to an existing chunk or if a new chunk should be created. The agent will: Check Existing Chunks: When a new proposition arrives, the agent assesses if any of the current chunks are suitable for storing it. If a relevant chunk exists, the agent calls the add_proposition function, passing the chunk ID and proposition. Create New Chunk: If no existing chunk is suitable, the agent calls the create_new_chunk function to start a new chunk for this proposition. LLM Decision: The agent leverages an LLM to decide whether to push the proposition to an existing chunk or create a new one, ensuring the decision is contextually informed. Based on the LLM’s response, it triggers the appropriate function to either add or create a chunk. This way, the agent intelligently organizes propositions into the most relevant chunks. def find_chunk_and_push_proposition(proposition):     class ChunkID(BaseModel):         chunk_id: int = Field(description="The chunk id.")     allocation_llm = llm.with_structured_output(ChunkID)     allocation_prompt = ChatPromptTemplate.from_messages(         [             (                 "system",                 "You have the chunk ids and the summaries"                 "Find the chunk that best matches the proposition."                 "If no chunk matches, return a new chunk id."                 "Return only the chunk id.",             ),             (                 "user",                 "proposition:{proposition}" "chunks_summaries:{chunks_summaries}",             ),         ]     )     allocation_chain = allocation_prompt | allocation_llm     chunks_summaries = {         chunk_id: chunk["summary"] for chunk_id, chunk in chunks.items()     }     best_chunk_id = allocation_chain.invoke(         {"proposition": proposition, "chunks_summaries": chunks_summaries}     ).chunk_id     if best_chunk_id not in chunks:         best_chunk_id = create_new_chunk(best_chunk_id, proposition)         return     add_proposition(best_chunk_id, proposition) The Problem: Hallucinations and Errors in RAG Systems An important issue raised which surrounds the limitations of Retrieval-Augmented Generation (RAG) systems and their susceptibility to hallucinations and logical errors, particularly in the response generation process. To address the problem more systematically, let’s break it down further: 1. The Fundamental Problem: Large Language Models (LLMs) can generate hallucinations — incorrect responses that are not grounded in the provided context or the underlying knowledge base. Even in a RAG setup where the retrieval mechanism is designed to enhance the accuracy of the model’s responses by fetching relevant data, errors can still occur during generation. 2. Why Do Hallucinations Occur in RAG Systems? The two main reasons for hallucinations in RAG systems, as you pointed out, are: LLM Limitations in Reasoning Across Context: Despite the presence of the correct information in the retrieved context, LLMs may still generate incorrect responses. This happens because the model must synthesize the answer from potentially fragmented data, often requiring reasoning that spans across multiple retrieved documents. If the model fails to do so effectively, hallucinations arise. Suboptimal Retrieval: If the retrieved context is incomplete or irrelevant (due to issues like poor document chunking or formatting, or the absence of crucial information in the knowledge base), the LLM might generate a response based on an inaccurate or inadequate context, leading to hallucinations. 3. Examples of High-Impact Failures: The case of Air Canada you mentioned highlights the real-world consequences of these hallucinations. When a chatbot generates an incorrect response, such as misrepresenting a refund policy, it can lead to reputational damage and legal repercussions, as well as a loss of trust in the system. 4. Detection and Mitigation of Hallucinations: The primary goal of any enterprise RAG system is to ensure that its responses are both accurate and reliable. Current approaches to detecting incorrect responses include: Post-Response Validation: Implementing mechanisms to verify the factual accuracy of the generated output against the retrieved context or external sources before presenting it to the user. Confidence Scoring: Leveraging confidence scores for both the retrieved context and the generated response can help determine the likelihood of an answer being incorrect. Feedback Loops: Continuous improvement of retrieval algorithms and refinement of the knowledge base to enhance retrieval accuracy and minimize the risk of hallucination. AI-Powered Fact-Checkers: Deploying additional AI models trained specifically for factual consistency checks can help reduce hallucinations in generated responses. 5. Current Research Directions: To mitigate the issue, research in the following areas is vital: Improved Contextual Reasoning: Enhancing LLMs’ ability to perform complex reasoning across multiple retrieved documents, ensuring that correct relationships between facts are understood and preserved in the response. Robust Retrieval Mechanisms: Ensuring the retrieval component can fetch not just relevant, but comprehensive and precise chunks of information to minimize the possibility of incorrect or incomplete context leading to hallucinations. Real-Time Error Detection: Development of detectors that can assess the generated response in real-time, flagging potential inaccuracies for further review before the response is delivered to users. By focusing on these detection mechanisms and improving the retrieval process, organizations can better safeguard their RAG systems from hallucinations and ensure greater trust and reliability in the responses provided by LLMs. The Solution: Hallucination Detection Methods The outlined hallucination detection methods provide a robust framework to mitigate the risks associated with LLM-generated hallucinations in high-stakes applications such as medicine, law, and finance. Here’s a deeper look at each method’s potential and implementation: 1. Self-Evaluation (Self-eval): This method capitalizes on the LLM’s own reasoning ability by prompting it to assess its own output based on a predefined confidence scale. By asking the model to explain its confidence via chain-of-thought (CoT) reasoning, it encourages deeper introspection before assigning a confidence score. Advantages: Simple and effective, especially when the LLM has access to the context. The reasoning provided by the LLM could give valuable insights into why a certain score was assigned, helping developers understand where hallucinations are likely. Scalable since it involves no external processes or systems. Potential Challenges: The LLM’s self-assessment might still be biased or flawed, especially if the LLM initially hallucinated due to misunderstanding the context. It may still miss subtle factual inconsistencies. 2. G-Eval (from DeepEval package): G-Eval improves upon simple self-evaluation by introducing a multi-step criterion-based evaluation. By using predefined criteria to measure the factual correctness of a response, it provides a more structured way for LLMs to reason about the accuracy of their output. Advantages: More rigorous than self-evaluation as it considers multi-step criteria, providing a more detailed and systematic evaluation. Proven correlation with human judgment on several datasets makes it a reliable method for applications where accuracy is critical. Potential Challenges: Complexity increases, as setting up and fine-tuning multi-step criteria for diverse use cases may require additional effort. May need fine-tuning based on the specific knowledge database and context format. 3. Hallucination Metric (from DeepEval package): This method uses one LLM to evaluate the degree to which another LLM’s response contradicts the retrieved context. The idea is to detect contradictions between the generated answer and the provided context, which could signal hallucination. Advantages: This metric focuses explicitly on contradiction detection, which is one of the main signs of hallucinations. Having a second LLM review the response introduces a new perspective, reducing the chance of unchecked hallucinations. Potential Challenges: Running multiple LLM evaluations can increase computation costs. Not all hallucinations are contradictions; some hallucinations may introduce new, unsupported information without direct contradiction, so it might miss such cases. 4. RAGAS: The RAG-specific evaluation suite, RAGAS, is a powerful tool that provides multiple scores, each tailored to evaluating different aspects of hallucination in RAG systems. The three key scores are: Faithfulness: Measures the fraction of claims in the response that are supported by the retrieved context. Advantages: Directly assesses how grounded the response is in the retrieved information, helping catch instances where the model “makes things up.” Challenges: May require precise context retrieval to be fully effective. Answer Relevancy: Computes the similarity between the original question and LLM-generated questions based on the response, using vector embeddings. Advantages: Ensures that the generated response is relevant to the question, filtering out irrelevant or off-topic hallucinations. Challenges: While it checks for relevancy, it might not catch factual inaccuracies if the response is relevant but wrong. Context Utilization: Assesses the degree to which the context was used in forming the response. Advantages: Identifies whether the LLM relied on the provided context, catching hallucinations due to insufficient reliance on the retrieved information. Challenges: This score alone won’t guarantee correctness; context utilization must be paired with other metrics to ensure faithfulness. 5. Strategic Use Cases for Detection Methods: High-Stakes Applications: For sectors like healthcare, law, and finance, using a combination of these methods could be crucial. For example, Self-eval could provide a quick, scalable approach to flag potential hallucinations, while G-Eval or the Hallucination Metric could serve as more intensive, second-layer checks for higher confidence levels. Cost-Optimization in RAG Systems: Methods like Self-eval or Answer Relevancy could be used early in the retrieval pipeline to determine when a response should be trusted or whether it’s worth invoking more resource-intensive retrieval or re-querying steps. If a high confidence is obtained early, this could avoid unnecessary computational overhead. 6. Combined Approach to Detection: Each of these methods has strengths and weaknesses, but they can be combined for robust detection. A practical implementation might involve: Self-eval as a quick, lightweight first-pass filter. G-Eval or the Hallucination Metric as a second-pass evaluator for cases where the initial self-assessment indicates low-to-moderate confidence. RAGAS scores as a final measure for faithfulness, relevancy, and context utilization, allowing developers to fine-tune responses based on specific application needs. By incorporating these detection methods into RAG systems, we can better ensure that LLM responses are trustworthy and avoid the detrimental effects of hallucinations in sensitive. Here is the specific prompt template used: Question: {question} Answer: {response} Evaluate how confident you are that the given Answer is a good and accurate response to the Question. Please assign a Score using the following 5-point scale: 1: You are not confident that the Answer addresses the Question at all, the Answer may be entirely off-topic or irrelevant to the Question. 2: You have low confidence that the Answer addresses the Question, there are doubts and uncertainties about the accuracy of the Answer. 3: You have moderate confidence that the Answer addresses the Question, the Answer seems reasonably accurate and on-topic, but with room for improvement. 4: You have high confidence that the Answer addresses the Question, the Answer provides accurate information that addresses most of the Question. 5: You are extremely confident that the Answer addresses the Question, the Answer is highly accurate, relevant, and effectively addresses the Question in its entirety. The output should strictly use the following template: Explanation: [provide a brief reasoning you used to derive the rating Score] and then write ‘Score: <rating>’ on the last line. Here is the prompt template used to prompt TLM: Answer the QUESTION using information only from CONTEXT: {context} QUESTION: {question} Now we will simplify 6 types of RAG’s !! Standard RAG: Standard RAG (Retrieval-Augmented Generation) is a framework designed to combine retrieval mechanisms with language generation models. Its main goal is to enhance the accuracy and relevance of generated text by retrieving related information from external sources and then using that retrieved data as input to a generative model, typically a large language model (LLM) like GPT or BERT. Here’s how Standard RAG works: 1. Document Retrieval: In the first step, RAG retrieves relevant documents from an external knowledge base (like a database, a set of documents, or even the web) based on a given query or user input. This retrieval process is usually done using embeddings-based similarity search. A common model used for this is a dense retriever, such as DPR (Dense Passage Retrieval), which converts both the query and documents into vectors and retrieves the most similar documents. 2. Contextual Document Integration: Once the relevant documents are retrieved, they are passed along with the original user input (query) to a generative model. These documents act as additional context for the generative model, allowing it to provide more accurate and contextually informed responses. 3. Answer Generation: The generative model (often based on Transformer architectures like GPT-3 or similar) combines the original user query with the retrieved documents and generates a coherent response that synthesizes the retrieved information. The final output is usually more precise and factually grounded compared to a purely generative model because the model has direct access to the relevant documents. Example Workflow of Standard RAG: Query: The user asks a question like “What is the capital of France?” Retrieval: The system retrieves documents that mention France, capital cities, and related topics. Augmentation: These retrieved documents are fed into the generative model. Generation: The model produces an answer like “The capital of France is Paris,” based on both its pre-training knowledge and the retrieved documents. Corrective RAG Corrective RAG (Retrieval-Augmented Generation) is an extension or variation of the RAG framework that integrates a retrieval mechanism with a generative model. In Corrective RAG, the focus is on using feedback or correction loops to improve the quality and accuracy of generated responses by refining the retrieved information or adjusting the generated outputs based on errors or inconsistencies. Here’s how Corrective RAG generally works: Retrieval-Augmented Generation (RAG) Overview: Retrieval: A retriever model is used to fetch relevant documents or data from a knowledge base based on the input query. Generation: A generative model (like GPT) takes both the query and the retrieved documents as input and generates a response using this augmented information. 2. Corrective RAG Enhancements: Initial Generation: The initial RAG process happens where a response is generated based on retrieved information and the input query. Error Detection: A secondary system (often using another model or a human feedback loop) identifies potential errors, inconsistencies, or areas of improvement in the generated output. This could involve fact-checking, correcting grammar, or refining the relevance of the generated answer. Correction Process: Based on the detected errors, corrective feedback is provided. This could involve: Re-retrieval: Triggering the retriever to fetch more accurate or additional supporting information based on the errors. Re-generation: The generative model reprocesses the query and retrieved documents, incorporating corrections to improve accuracy and coherence. Looping: This correction loop can iterate multiple times until a satisfactory or error-free output is generated. 3. Feedback Integration: Human-in-the-loop: In some cases, human feedback can be provided to improve the corrective process, either by validating the correction or suggesting modifications manually. Automatic Feedback: Alternatively, machine learning models (e.g., classifiers or ranking models) can be used to automatically assess the quality of outputs and trigger corrective actions. 4. Applications of Corrective RAG: Fact-checking: Ensuring generated responses are factually accurate. Error Handling in Conversational AI: Correcting misunderstandings or inaccurate answers in chatbots or virtual assistants. Improving QA Systems: Enhancing question-answering systems by continuously refining answers through feedback and correction loops. Speculative RAG Speculative Retrieval-Augmented Generation (Speculative RAG) is an advanced technique that combines retrieval-based and generative models to enhance the efficiency and effectiveness of generating factual and accurate text. The main idea is to speed up the generation process by using a retrieval system to fetch relevant information early on, allowing the generative model to focus on more specific tasks rather than generating the entire text from scratch. Here’s how Speculative RAG works: Key Components: Retrieval Model: This system searches and retrieves relevant documents or information from a large corpus, such as a database or the web. The retrieved documents are the basis for the generation process, providing factual grounding to the model. Generative Model: A large language model (LLM) generates text based on the input from the retrieval model. The model leverages the retrieved information to produce coherent and contextually accurate responses. Speculative Generation: The generative model first predicts or “speculates” on the output based on an initial set of retrievals and its own internal knowledge. If the retrieval system finds highly relevant data, the model revises its predictions, improving accuracy. If the retrieved information is already accurate, the process continues seamlessly. Parallel Processing: Speculative RAG accelerates the generation process by running the retrieval and generative processes in parallel. The generative model does not wait for the retrieval system to finish; instead, it begins generating based on partial or initial retrievals and adjusts its output as more relevant information becomes available. How It Works: Initial Query: A user poses a question or query. Retrieval Step: The system sends this query to the retrieval model, which searches for relevant documents or sources from a knowledge base. Speculative Generation: Simultaneously, the generative model begins constructing a response based on partial information or its own internal knowledge. Feedback Loop: As more relevant information is retrieved, the speculative generation is refined in real-time. The final output is a combination of generative reasoning and factual data from the retrieval system. Response: The model delivers a faster and more factually accurate response, as the retrieval system acts as a “corrector” or “guide” for the generative model’s output. Benefits: Faster Response Time: Since the retrieval and generation happen simultaneously, it reduces latency compared to traditional RAG, where the generation process waits for retrieval results. Improved Accuracy: The speculative nature helps the generative model correct potential errors by using more accurate information from the retrieved documents. Efficiency: It optimizes the retrieval-augmented generation process by reducing the need for multiple interactions between the retrieval and generative systems. Speculative RAG is useful in applications where speed and factual accuracy are crucial, such as in customer support, research assistance, and real-time data generation. Fusion RAG Fusion RAG (Fusion Retrieval-Augmented Generation) is a hybrid approach that combines retrieval-based methods with generation-based techniques for enhancing the performance of AI models, particularly in tasks that involve answering complex queries or generating text. The approach is particularly useful in open-domain question answering, where the system needs to access external information to provide accurate and contextually relevant answers. How Fusion RAG Works Retrieval (R): The first step involves retrieving relevant documents or pieces of information from a large corpus (such as knowledge bases, web pages, or databases). This is usually done using dense retrieval models like BERT or bi-encoder models, which can efficiently search for documents most relevant to the input query. The retrieval model finds a set of documents or passages that are likely to contain relevant information. 2. Fusion (F): The retrieved information is then fused together. This step involves combining relevant sections from multiple documents. The fusion process enhances the richness and diversity of the information, ensuring the final result draws on a broader and more comprehensive range of knowledge. This can be done through techniques such as attention mechanisms or weighted aggregation of the retrieved content. 3. Augmented Generation (AG): After retrieving and fusing the relevant information, the model uses a generation model (like a Transformer-based language model) to create a response or output based on the input query and the retrieved documents. The generation model uses the retrieved content as an additional context, improving the accuracy and relevance of the generated response. Key Features Hybrid Approach: Fusion RAG leverages both retrieval-based models (for precision) and generative models (for fluency and creativity). Efficient Knowledge Integration: Instead of generating responses purely from internal parameters, the model incorporates external knowledge for better factual accuracy. Scalability: It can handle large datasets or knowledge bases, making it suitable for applications like chatbots, search engines, or even complex decision support systems. Benefits Enhanced Accuracy: The combination of retrieval and generation ensures that the responses are both relevant and factually correct. Dynamic Knowledge Updating: The retrieval component allows the system to use up-to-date information, unlike static language models that rely on pre-trained knowledge. Versatility: This method can be applied to various domains, including customer support, research, and even content creation, where accurate and context-aware responses are essential. Agentic RAG Agentic Retrieval-Augmented Generation (RAG) is an advanced framework designed to enhance traditional Retrieval-Augmented Generation models by integrating agency, or decision-making abilities, into the workflow. Standard RAG models combine a language model (like GPT or BERT) with a retrieval system to fetch relevant documents from external databases, which are then used to generate responses. The agentic variant goes beyond this by incorporating additional decision-making components that give the system more autonomy and context-awareness in managing tasks. Key Components of Agentic RAG Retrieval Mechanism: Like standard RAG, Agentic RAG retrieves external knowledge from structured or unstructured databases based on a query. This could be a database, document repository, or any knowledge source. Generation Mechanism: After retrieval, a language model generates a response that is informed by the retrieved documents. This ensures that responses are grounded in factual information. Agentic Layer (Decision-Making): What sets Agentic RAG apart is that it introduces an additional layer that can make decisions based on the context, retrieved information, and the user’s needs. This decision-making process allows the model to: Plan actions over multiple steps to achieve a goal. Decide which data to prioritize or retrieve. Control the flow of information, determining whether to retrieve more, generate a response, or ask for clarification. How Agentic RAG Works: Query Understanding: The system first interprets the query or task provided by the user. This could be a complex request requiring multiple pieces of information from various sources. Document Retrieval: Using a retrieval engine (like Elasticsearch or a vector-based database), the model retrieves relevant documents from a knowledge source. In Agentic RAG, this retrieval step can be optimized based on the context and agentic decision-making processes (e.g., it may choose to retrieve additional documents for better accuracy or stop if the retrieved information is sufficient). Agentic Decision-Making: At this stage, the model decides: How to use the retrieved documents. Whether to retrieve more data. If a subtask (such as parsing or filtering) needs to be performed before generation. The agentic system assesses whether the current data meets the requirements for generating a reliable response. Response Generation: The model uses the retrieved information and the decisions made in the previous step to generate an accurate, coherent, and contextually relevant response. The generation step relies on large language models to synthesize information. Post-Processing and Control Flow: If necessary, the system can engage in multi-turn dialogues, seeking clarification or additional input from the user before finalizing its response. It might also store outcomes for future learning, improving the response quality over time. Example Use Case: Due Diligence in M&A: In mergers and acquisitions, due diligence involves analyzing various financial and legal documents. An Agentic RAG system could autonomously retrieve relevant financial reports, analyze trends, and decide whether more information is needed. It could also summarize findings and offer recommendations based on predefined decision criteria. Advantages of Agentic RAG: Autonomy: By incorporating decision-making, it can handle more complex queries that require dynamic retrieval and multi-step processing. Efficiency: The agentic component optimizes the retrieval process, reducing unnecessary information fetching and focusing only on what’s needed. Contextual Awareness: It’s capable of understanding broader contexts and making informed decisions on the fly. This approach makes Agentic RAG highly useful for complex tasks in areas like research, decision support, and automation of knowledge-heavy workflows. Self RAG Self-Retrieval-Augmented Generation (Self-RAG) is an extension of the Retrieval-Augmented Generation (RAG) paradigm, which combines the benefits of large language models (LLMs) with external information retrieval systems to generate more accurate and grounded responses. How Self-RAG Works: Initial Query Generation: The system starts with an initial query, either provided by the user or generated by the model itself based on a given input. Self-Retrieval Step: The model queries its own knowledge base or an external data source to retrieve relevant documents or information that can help generate a more accurate response. The retrieval system could be a traditional search engine, a dense retriever (like sentence transformers), or a vector database with embeddings. Self-Selection of Evidence: From the retrieved information, the model selects or scores the most relevant pieces of evidence. This self-selection is crucial as it filters out unnecessary or less relevant information. Generation with Augmented Knowledge: The LLM generates a response by incorporating the retrieved and selected information. This “augmented” generation is meant to ensure that the output is more factually grounded and contextually relevant, improving both the accuracy and the quality of the generated text. Feedback Loop (Optional): In some implementations, the model might perform multiple iterations, refining the query or the retrieved evidence in response to how well the previous generation matched the input prompt. Key Benefits: Factual Accuracy: By retrieving relevant documents, Self-RAG ensures the generated responses are grounded in reality, improving factual correctness. Reduced Hallucination: LLMs sometimes produce information that sounds plausible but is incorrect. By consulting external sources, Self-RAG reduces this risk. Adaptability: The retrieval system can be tailored to different knowledge domains, allowing Self-RAG to adapt to specific tasks or topics. Applications: Self-RAG is often used in tasks like: Document generation based on large knowledge bases. Question answering, where the model needs external evidence. Personalized responses, where user-specific data is retrieved. RAG Code Implementation : Tools Used: LangChain: A framework for building applications with LLMs. LlamaIndex (formerly GPT Index): A library to efficiently query a document using a large language model. LangSmith: A tool for observability, logging, and experimentation with large language models. RetrieverQA: A QA mechanism where the model first retrieves relevant information from a knowledge base, then uses that information to generate a response. Plan in Pseudocode: Set up LangChain: Use LangChain to handle the interaction between LLM and retrievers. Configure LlamaIndex: Build the index to enable efficient document retrieval. Set up Retriever: Use RetrieverQA to query the document with relevant data. Connect LangSmith: Monitor the interactions, responses, and retriever performance. Final Response Generation: Combine the retrieved documents with LLM to generate the final answer. Steps: Install Required Libraries: First, install the required libraries. !pip install langchain llama-index langsmith retriever-qa Code Implementation: Here’s how to implement the RAG framework with LangChain, LlamaIndex, and LangSmith using RetrieverQA. # File: rag_retriever_qa.py from langchain.llms import OpenAI from llama_index import SimpleDocument, GPTSimpleVectorIndex from retriever_qa import RetrieverQA from langchain.prompts import PromptTemplate from langsmith import Client # Initialize LangSmith Client for observability client = Client(api_key="YOUR_LANGSMITH_API_KEY") # Step 1: Load Documents into LlamaIndex documents = [     SimpleDocument("The capital of France is Paris."),     SimpleDocument("Python is a popular programming language."),     SimpleDocument("The human brain has 86 billion neurons.") ] # Build the index using LlamaIndex index = GPTSimpleVectorIndex.from_documents(documents) # Step 2: Setup RetrieverQA with the built index retriever_qa = RetrieverQA(retriever=index) # Step 3: Create a LangChain LLM and define a simple Prompt llm = OpenAI(model="gpt-4", temperature=0.5) # Set up a LangChain QA prompt template prompt_template = PromptTemplate(     template="You are a helpful assistant. Use the following retrieved documents to answer the question:\n{retrieved_docs}\n\nQuestion: {question}\nAnswer:",     input_variables=["retrieved_docs", "question"] ) # Step 4: Define a function for Retrieval-Augmented Generation (RAG) def rag_retriever_qa(query):     # 1. Retrieve relevant documents from the index     retrieved_docs = retriever_qa.retrieve(query)     # 2. Create the input for the model with the retrieved documents     formatted_docs = "\n".join([doc.text for doc in retrieved_docs])         # 3. Build the final prompt     prompt = prompt_template.format(retrieved_docs=formatted_docs, question=query)         # 4. Generate the response using LangChain LLM     answer = llm.generate(prompt)         # 5. Log the interaction with LangSmith     client.log_interaction(         input=prompt,         output=answer,         retriever=retrieved_docs,         question=query     )     return answer # Step 5: Example usage of RAG with RetrieverQA if __name__ == "__main__":     query = "What is the capital of France?"     response = rag_retriever_qa(query)     print(f"Answer: {response}") Explanation of the Code: LangChain Initialization: The OpenAI object from langchain.llms initializes the connection to an OpenAI LLM (e.g., GPT-4), which is used for generating the final response based on retrieved documents. temperature=0.5 is used for controlling the randomness in the output. A lower value makes the model more deterministic. 2. LlamaIndex: The SimpleDocument class creates a document object. In this case, we provide three example documents that will be indexed. GPTSimpleVectorIndex builds an index from these documents. This index will later be used to retrieve relevant information. 3. RetrieverQA: RetrieverQA is initialized with the retriever (i.e., the LlamaIndex) that it will use to retrieve relevant information based on the query. The .retrieve() method returns relevant documents that match the query from the index. 4. Prompt Template: PromptTemplate defines the structure of the prompt that the LLM will use. It takes the retrieved documents and the user's query as inputs. The prompt asks the LLM to use the retrieved documents to answer the question. 5. LangSmith for Observability: LangSmith’s Client object logs the input, output, and retrieval steps of the interaction. This helps track and debug interactions over time. client.log_interaction() captures the prompt, response, retriever details, and query to store them for analysis. 6. RAG Flow: The rag_retriever_qa function executes the full RAG process. First, it retrieves relevant documents from the LlamaIndex, then uses those documents to generate a response with the OpenAI LLM. The prompt is formatted to include both the retrieved documents and the original question. 7. Running the Code: Finally, an example query "What is the capital of France?" is passed to the function, and the generated answer is printed. To implement caching and use a more sophisticated vector-based search with embeddings for better retrieval performance, we’ll enhance the code in the following ways: Steps: Caching: Implement a simple caching mechanism so that frequently asked questions do not require repeated retrieval from the LLM and documents. Embeddings for Vector-based Search: Use a language model to create embeddings for documents and the query. These embeddings will be used for efficient vector-based search. We’ll use OpenAIEmbeddings (or another embedding provider) to generate the embeddings for documents and queries, and we’ll enhance the retriever to perform vector similarity search. Plan: Embeddings Generation: Use a pre-trained model (e.g., OpenAI, Hugging Face) to generate embeddings for both the documents and the queries. Store these embeddings in a vector database. 2. Caching: Use a dictionary to store the previous queries and their answers to avoid unnecessary repeated processing. 3. Efficient Vector Retrieval: Perform similarity search using embeddings to retrieve the most relevant documents. Let’s walk through the updated code implementation. # File: rag_retriever_qa_with_cache.py from langchain.llms import OpenAI from llama_index import SimpleDocument, GPTSimpleVectorIndex, VectorIndexRetriever from langchain.embeddings import OpenAIEmbeddings from langchain.prompts import PromptTemplate from langsmith import Client import numpy as np from sklearn.metrics.pairwise import cosine_similarity from collections import defaultdict # Initialize LangSmith Client for observability client = Client(api_key="YOUR_LANGSMITH_API_KEY") # Step 1: Load documents and generate embeddings documents = [     SimpleDocument("The capital of France is Paris."),     SimpleDocument("Python is a popular programming language."),     SimpleDocument("The human brain has 86 billion neurons.") ] # Generate embeddings for the documents using OpenAIEmbeddings embedding_model = OpenAIEmbeddings() doc_texts = [doc.text for doc in documents] doc_embeddings = embedding_model.embed_documents(doc_texts) # Step 2: Setup Caching cache = defaultdict(lambda: None) # Step 3: Helper function to calculate cosine similarity def get_top_k_similar(query_embedding, doc_embeddings, k=3):     similarities = cosine_similarity([query_embedding], doc_embeddings)     top_k_indices = np.argsort(similarities[0])[-k:][::-1]     return top_k_indices # Step 4: Setup RetrieverQA using Vector Embeddings def retrieve_documents(query):     # Check the cache first     if cache[query]:         return cache[query]     # Generate embedding for the query     query_embedding = embedding_model.embed_query(query)         # Perform vector-based similarity search     top_k_indices = get_top_k_similar(query_embedding, doc_embeddings)         # Retrieve the top documents     retrieved_docs = [documents[i] for i in top_k_indices]     # Store in cache     cache[query] = retrieved_docs         return retrieved_docs # Step 5: Set up LangChain LLM and define a simple Prompt llm = OpenAI(model="gpt-4", temperature=0.5) # Set up a LangChain QA prompt template prompt_template = PromptTemplate(     template="You are a helpful assistant. Use the following retrieved documents to answer the question:\n{retrieved_docs}\n\nQuestion: {question}\nAnswer:",     input_variables=["retrieved_docs", "question"] ) # Step 6: Define a function for Retrieval-Augmented Generation (RAG) def rag_retriever_qa(query):     # 1. Retrieve relevant documents using vector similarity     retrieved_docs = retrieve_documents(query)     # 2. Format the retrieved documents for input to the model     formatted_docs = "\n".join([doc.text for doc in retrieved_docs])         # 3. Build the final prompt     prompt = prompt_template.format(retrieved_docs=formatted_docs, question=query)         # 4. Generate the response using LangChain LLM     answer = llm.generate(prompt)         # 5. Log the interaction with LangSmith     client.log_interaction(         input=prompt,         output=answer,         retriever=retrieved_docs,         question=query     )     return answer # Step 7: Example usage of RAG with Caching and Embeddings if __name__ == "__main__":     query = "What is the capital of France?"     response = rag_retriever_qa(query)     print(f"Answer: {response}") Explanation of Changes: Embedding Generation: We use OpenAIEmbeddings to generate embeddings for both the documents and queries. The embed_documents() method generates embeddings for the list of document texts, and embed_query() is used to create embeddings for the query. 2. Cosine Similarity for Vector-based Retrieval: The get_top_k_similar function calculates the cosine similarity between the query embedding and the document embeddings using scikit-learn's cosine_similarity function. It returns the indices of the top k most similar documents. The top documents are selected based on the highest similarity scores and returned as the retrieved documents. 3. Caching: A defaultdict is used as a simple cache to store the results for each query. If the same query is made again, it returns the cached result instead of recomputing the embeddings and similarity. 4. Integration with LangChain: Once the relevant documents are retrieved, the formatted_docs is passed into the prompt template, which then generates a response using the OpenAI LLM. 5. LangSmith Logging: As before, we log each interaction (input, output, retrieved docs, and query) using LangSmith for observability. Caching and Retrieval Optimization: Efficiency: The cache avoids unnecessary calls to the embedding model and LLM, thus speeding up responses for repeated queries. Accuracy: By using embeddings and cosine similarity, we are performing more sophisticated retrieval than simple keyword matching. Using LangChain Agents : To implement Retrieval-Augmented Generation (RAG) using LangChain Agents, LlamaIndex, LangSmith, and RetrieverQA, we’ll build a system where LangChain agents will dynamically retrieve relevant documents using LlamaIndex and handle the question-answering process with RetrieverQA. The responses and retrieval steps will be logged using LangSmith for observability. Steps to Implement: LangChain Agents: We’ll use LangChain’s agent system to dynamically retrieve documents based on the user query. LlamaIndex: This will be used to create an efficient index from documents and handle vector-based search to retrieve relevant information. RetrieverQA: Used to handle question answering by retrieving documents based on the query and generating the final response. LangSmith: Used for logging interactions (prompts, responses, retrieval steps) and debugging. Dependencies: You need the following libraries installed: !pip install langchain llama-index langsmith retriever-qa High-Level Flow: Agent Setup: A LangChain agent will handle the question-answering flow. It will retrieve the relevant documents from LlamaIndex, use RetrieverQA to fetch answers, and then use the LLM to generate the final response. LlamaIndex for Document Retrieval: The agent will interact with LlamaIndex to retrieve the most relevant documents from the indexed knowledge base. RetrieverQA: The agent will use RetrieverQA to retrieve answers based on the documents retrieved from the index. Logging with LangSmith: Each step of the retrieval and generation process will be logged using LangSmith. Code: # File: rag_langchain_agent.py from langchain.agents import initialize_agent, Tool, AgentExecutor from langchain.llms import OpenAI from llama_index import SimpleDocument, GPTSimpleVectorIndex from retriever_qa import RetrieverQA from langchain.prompts import PromptTemplate from langsmith import Client # Initialize LangSmith Client for observability and logging client = Client(api_key="YOUR_LANGSMITH_API_KEY") # Step 1: Load documents and create an index using LlamaIndex documents = [     SimpleDocument("The capital of France is Paris."),     SimpleDocument("Python is a popular programming language."),     SimpleDocument("The human brain has 86 billion neurons.") ] # Create the index from the loaded documents index = GPTSimpleVectorIndex.from_documents(documents) # Step 2: Set up the RetrieverQA using the LlamaIndex as the retriever retriever_qa = RetrieverQA(retriever=index) # Step 3: Define LangChain LLM and set up a simple QA prompt llm = OpenAI(model="gpt-4", temperature=0.5) # Prompt template for the agent to answer the questions based on the retrieved documents prompt_template = PromptTemplate(     template="You are a helpful assistant. Use the following retrieved documents to answer the question:\n{retrieved_docs}\n\nQuestion: {question}\nAnswer:",     input_variables=["retrieved_docs", "question"] ) # Step 4: Define Tools for the Agent # We define the tools that the LangChain agent will use to interact with the environment. # These tools will help in retrieving documents and generating the answers. def retrieve_documents(query):     # Step 1: Retrieve relevant documents from LlamaIndex using RetrieverQA     retrieved_docs = retriever_qa.retrieve(query)         # Format the retrieved documents     formatted_docs = "\n".join([doc.text for doc in retrieved_docs])         # Log the retrieval step with LangSmith     client.log_interaction(         input=query,         output=formatted_docs,         retriever=retrieved_docs     )         return formatted_docs def generate_answer(query, retrieved_docs):     # Step 2: Build the final prompt for LangChain LLM     prompt = prompt_template.format(retrieved_docs=retrieved_docs, question=query)         # Generate the response using LangChain LLM     answer = llm.generate(prompt)         # Log the LLM interaction with LangSmith     client.log_interaction(         input=prompt,         output=answer,         question=query     )         return answer # Define the Tools that the LangChain agent will use tools = [     Tool(         name="Document Retriever",         func=retrieve_documents,         description="Retrieve relevant documents based on a query."     ),     Tool(         name="Answer Generator",         func=generate_answer,         description="Generate a final answer based on retrieved documents."     ) ] # Step 5: Initialize the LangChain Agent # We initialize an agent with the tools we've defined and the LLM to handle the question-answer process. agent = initialize_agent(     tools=tools,     agent_type="zero-shot-react-description",  # The agent will use these tools based on their descriptions     llm=llm ) # Step 6: Define the RAG flow with LangChain Agent def rag_with_agent(query):     # The agent will first use the "Document Retriever" tool to fetch relevant docs     # Then use the "Answer Generator" to generate the final response     result = agent.run(query)         return result # Step 7: Example usage if __name__ == "__main__":     query = "What is the capital of France?"     response = rag_with_agent(query)     print(f"Answer: {response}") Detailed Explanation: LangSmith Initialization: The LangSmith client is initialized for logging each interaction (input, output, retriever, and question). This enables observability and debugging of the RAG process. 2. Document Indexing with LlamaIndex: We load some example documents (like the capital of France, programming in Python, etc.) and create an index using GPTSimpleVectorIndex. This index will later be used to retrieve relevant information. 3. RetrieverQA Setup: RetrieverQA is initialized using the LlamaIndex as the retriever. It will handle the retrieval of relevant documents based on the user query. 4. LangChain LLM: An OpenAI LLM (GPT-4 in this case) is instantiated. This will be used to generate the final answers based on the documents retrieved by RetrieverQA. 5. LangChain Agent and Tools: We define two tools for the LangChain agent: Document Retriever: This tool retrieves relevant documents from the LlamaIndex. It formats the documents and logs the interaction using LangSmith. Answer Generator: This tool takes the retrieved documents and generates a final response using the LLM. These tools are passed into the LangChain agent using initialize_agent. The agent dynamically decides how to use these tools based on the descriptions. 6. RAG Flow: In the rag_with_agent function, we pass the query to the agent. The agent retrieves the relevant documents using the Document Retriever tool and then generates the answer using the Answer Generator tool. The result is returned as the final answer. 7. Logging: Each interaction (retrieval and answer generation) is logged using LangSmith. This is useful for observability and monitoring the performance of both the retriever and the generator. Example Usage: If you run the script with the query "What is the capital of France?", the system will: Retrieve the relevant document about Paris from the LlamaIndex. Pass this document into the LLM, which generates the final answer: "The capital of France is Paris." And here is the joke !! Why did the Retrieval-Augmented Generation (RAG) model go to therapy? Because it had too many unresolved queries! 😄 Large Language Models Generative Ai Use Cases Langchain Llamaindex Langsmith -- -- Follow Written by Devmallya Karar 58 Followers ·32 Following Lead AI Scientist at HCLTech and entangled with Generative AI, Quantum ML, Deep Learning and Machine Learning. These are the O2 of my life. Follow No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams

Source: https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag
Retrieval-augmented generation (RAG) | Technology Radar | Thoughtworks 
    <div id="js-disabled-banner-wrapper"> <div class="js-disabled-banner-text grid__cell" id="notification-banner-text"> Enable javascript in your browser for better experience. Need to know to enable it? <a class="cookie-banner-link no-underline pop-out" href="https://www.enable-javascript.com/" target="_blank" aria-label="This is an external link. Opens in new tab">Go here. Menu Close What we do Go to overview Services Artificial Intelligence Cloud Customer Experience and Products Data and Analytics Managed Services Modernization Platforms Software-defined Vehicles Software Engineering Who we work with Go to overview Industries Automotive Public Sector Cleantech, Energy and Utilities Media and Publishing Retail and E-commerce Financial Services and Insurance Not-for-profit Travel and Transport Healthcare and Life Sciences Insights Go to overview Loading Resource Hubs Technology Enterprise technology and engineering excellence Business Business and industry insights for digital leaders Culture Explore what it means to be a Thoughtworker Publications and Tools Technology Radar An opinionated guide to today's technology landscape Perspectives A no-nonsense publication for digital leaders Digital Fluency Model A model to help you build a resilient business Decoder The business execs' A-Z guide to technology Looking Glass Bringing the tech-led business changes into focus All Insights Articles In-depth insights to help your business grow Blogs Expert advice on strategy, design, engineering, and careers Books Explore our extensive library to keep learning Podcasts Conversations on the latest in business and tech Careers Go to overview Application Process What to expect as you interview with us Consultant Life Learn what life is like as a Thoughtworker Grads and Career Changers Start your tech career on the right foot Search Jobs Find open positions in your region Stay Connected Sign up for our monthly newsletter Learning and Development Explore how we support career growth Benefits See how we take care of our people About Go to overview Our Purpose Diversity, Equity and Inclusion Our History Our Leaders Social Change News Partnerships Sustainability Conferences and Events Our Brand Awards and Recognition Investors Contact Search Close Worldwide | English Australia English Brazil English | Português Canada English Chile English | Español China English | Chinese Ecuador English | Español Germany English | Deutsch India English Singapore English Spain English | Español Thailand English United Kingdom English United States English Worldwide English Technology Radar Download Retrieval-augmented generation (RAG) Search Search Techniques Platforms Tools Languages & Frameworks Techniques Back Techniques Back Search Search Techniques Platforms Tools Languages & Frameworks Close Last updated : Oct 23, 2024 Oct 2024 Adopt ? We feel strongly that the industry should be adopting these items. We use them when appropriate on our projects. Retrieval-augmented generation (RAG) is the preferred pattern for our teams to improve the quality of responses generated by a large language model (LLM). We’ve successfully used it in many projects, including the Jugalbandi AI platform. With RAG, information about relevant and trustworthy documents is stored in a database. For a given prompt, the database is queried, relevant documents are retrieved and the prompt augmented with the content of the documents, thus providing richer context to the LLM. This results in higher quality output and greatly reduced hallucinations. The context window — which determines the maximum size of the LLM input — has grown significantly with newer models, but selecting the most relevant documents is still a crucial step. Our experience indicates that a carefully constructed smaller context can yield better results than a broad and large context. Using a large context is also slower and more expensive. We used to rely solely on embeddings stored in a vector database to identify additional context. Now, we're seeing reranking and hybrid search: search tools such as Elasticsearch Relevance Engine as well as approaches like GraphRAG that utilize knowledge graphs created with the help of an LLM. A graph-based approach has worked particularly well in our work on understanding legacy codebases with GenAI. Apr 2024 Adopt ? We feel strongly that the industry should be adopting these items. We use them when appropriate on our projects. Retrieval-augmented generation (RAG) is the preferred pattern for our teams to improve the quality of responses generated by a large language model (LLM). We’ve successfully used it in several projects, including the popular Jugalbandi AI Platform. With RAG, information about relevant and trustworthy documents — in formats like HTML and PDF — are stored in databases that supports a vector data type or efficient document search, such as pgvector, Qdrant or Elasticsearch Relevance Engine. For a given prompt, the database is queried to retrieve relevant documents, which are then combined with the prompt to provide richer context to the LLM. This results in higher quality output and greatly reduced hallucinations. The context window — which determines the maximum size of the LLM input — is limited, which means that selecting the most relevant documents is crucial. We improve the relevancy of the content that is added to the prompt by reranking. Similarly, the documents are usually too large to calculate an embedding, which means they must be split into smaller chunks. This is often a difficult problem, and one approach is to have the chunks overlap to a certain extent. Sep 2023 Trial ? Worth pursuing. It is important to understand how to build up this capability. Enterprises should try this technology on a project that can handle the risk. Retrieval-Augmented Generation (RAG) is a technique to combine pretrained parametric and nonparametric memory for language generation. It enables you to augment the existing knowledge of pretrained LLMs with the private and contextual knowledge of your domain or industry. With RAG, you first retrieve a set of relevant documents from the nonparametric memory (usually via a similarity search from a vector data store) and then use the parametric memory of LLMs to generate output that is consistent with the retrieved documents. We find RAG to be an effective technique for a variety of knowledge intensive NLP tasks — including question answering, summarization and story generation. Published : Sep 27, 2023 Related blips Elasticsearch Relevance Engine Assess Platforms April 2024 Using GenAI to understand legacy codebases Trial Techniques October 2024 Download the PDF English | Español | Português | 中文 Sign up for the Technology Radar newsletter Subscribe now Visit our archive to read previous volumes Go to archive Company About us What we do Partnerships Who we work with News Diversity, Equity and Inclusion Careers Investors Contact us Insights Preference center Articles Blogs Books Podcasts Site info Privacy policy Accessibility statement Modern slavery statement Code of conduct Integrity helpline Sustainable procurement policy Connect with us × WeChat © 2025 Thoughtworks, Inc.

Source: https://www.matillion.com/learn/blog/rag-retrieval-augmented-generation
What is RAG (Retrieval-Augmented Generation) in AI? Book a demo Products Data Productivity Cloud The all-in-one platform to build and manage data pipelines, create no-code data transformations, and deliver data for AI and analytics. Learn more about Data Productivity Cloud Generative AI Tap into unstructured data, create AI pipelines, and accelerate workloads. Automation & Management Centralize visibility into all data pipelines and trigger automatic pipeline tasks. Data Connectivity Connect any data source to your preferred cloud data platform, with pre-built or custom connectors. Security Protect your data and meet compliance requirements with our comprehensive security program. Data Transformation Cleanse, aggregate, and transform data with ETL no-code and high-code options. All Features Explore Matillion's product features and capabilities. Connectors Explore our library of pre-built data connectors or create your own custom connector to move data between systems. View all connectors Amazon S3 Move, combine, and transform data stored in Amazon S3 buckets. PostgreSQL Connect data from your PostgreSQL database and enhance it with Matillion. SAP Connect to SAP to make your enterprise data more accessible across the organization. Salesforce Load and transform your Salesforce data to help convert leads and close deals. Google Analytics Integrate data from Google Analytics for fast, actionable web, product, and marketing insights. Custom Connectors Request a connector and we can build it for you or learn how you can build your own custom connector. Data Productivity Cloud Generative AI Data Connectivity Data Transformation Automation & Management Security All Features Connectors Amazon S3 PostgreSQL SAP Salesforce Google Analytics View all connectors Solutions Solutions Start your next data project with Matillion Cloud data warehousing Tap into unstructured data, create AI pipelines, and accelerate workloads. Legacy ETL Migration Centralize visibility into all data pipelines and trigger automatic pipelines tasks. Business Analytics Projects Connect Any data source to your preferred cloud data platform, with pre-built or custom connectors. Generative AI Protect your data and meet compliance requirements with our comprehensive security program. Data Movement + Reverse ETL Cleanse, aggregate, and transform data with ETL high-code and no-code options. Self-Service Business Intelligence Explore Matillion's product features and capabilities. By Industry See all industries Financial Services Healthcare Retail Communications and Media Technology and Software Manufacturing Success Stories Read stories Learn how Matillion has helped companies leverage their data to drive productivity, profits, and innovation. Solutions Cloud data warehousing Business Analytics Projects Data Movement + Reverse ETL Legacy ETL Migration Generative AI Self-Service Business Intelligence By Industry Financial Services Healthcare Life Sciences Retail Communications and Media Technology and Software See all industries Connectors Resources Resource Center Data success starts here. Explore Matillion's full resource center Learn PRODUCT Blog Industry & technology articles Documentation Find Data Productivity Cloud & Matillion ETL docs Product Tours View interactive demos & short videos eBooks Find how-to and best practice guides TRAINING Matillion Academy Discover training courses, tutorials, videos, and more How-to Videos Solve common orchestration problems Certifications Become an accredited Matillion expert How-to Articles Learn about advanced product capabilities Get Help SUPPORT & SERVICES Support Portal Search knowledgebase & submit cases Support Plans Find the option that fits your needs Professional Services Packages Get expert help with data projects Downloads (Matillion Exchange) Find connectors, drivers, transformation pipelines, and orchestration jobs Connect USER COMMUNITY Join the Community Learn & share best practices Roadmap & Ideas Portal Submit ideas & vote on enhancements Events Attend live & virtual events PARTNERS Become a Partner Learn about benefits & opportunities Find a Partner Consulting Partners Technology Partners Partner Portal Access partner resources Explore Matillion's full resource center PRODUCT Blog Documentation Product Tours eBooks TRAINING Matillion Academy How-to Videos Certifications How-to Articles SUPPORT AND SERVICES Support Portal Support Plans Professional Services Packages Downloads (Matillion Exchange) USER COMMUNITY Join the Community Roadmap & Ideas Portal Events PARTNERS Become a Partner Find a Consulting Partner Find a Technology Partner Partner Portal Pricing About Why Matillion? What makes Matillion different? Company History, leadership, & values Blog Industry and technology articles Customers Case studies & success stories Partners Technology & consulting partners Investors Our leading investors News Matillion & industry news Press Brand & Media Kit Careers Join team green! Forrester Study: The Total Economic Impact™ of Matillion Read report Why Matillion? Company Blog Customers Partners Investors News Press Careers Log In Start a free trial Book a demo Search Contact Sales Log In Back to all Blog 01.21.2025 What is RAG (Retrieval-Augmented Generation) in AI? Artificial intelligence has a truth problem. Despite all the sophistication, Large Language Models (LLMs) regularly generate responses that sound confidently sound but are factually wrong. That’s fine and dandy when you’re using it for trivial questions, but when your business is deploying AI solutions—this creates a real challenge: How do you harness the power of generative AI while guaranteeing its outputs are accurate, current, and trustworthy? That’s where RAG (Retrieval-Augmented Generation) can help. Instead of letting AI wing it based on its training data alone, RAG connects it with your actual business information, documentation, and knowledge bases. Instead of getting responses that sound plausible but might be wrong, you get answers grounded in your company's real data and documents. For businesses investing in AI, this isn't just a nice-to-have—it's becoming downright non-negotiable for building trust and delivering actual value. However, implementing RAG isn't just about bolting a search engine onto an LLM. It's about thoughtfully connecting AI models with your organization's knowledge in a way that improves accuracy (without sacrificing speed or scalability). And that’s easier said than done. Get it right, and you've got AI that's not just convincing, but correct. Get it wrong, and you're just adding complexity without solving the core problem. Below, we'll cut through the hype and walk through everything you need to know about RAG—from the basics to real-world implementation. We'll look at why it matters, how it works, and (most importantly) how to make it work for your business. What is RAG (Retrieval-Augmented Generation)? Retrieval-Augmented Generation (RAG) is an AI framework that improves large language models by connecting them with external data sources and knowledge bases to generate more accurate, current, and verifiable responses. Still, that technical definition only tells part of the story. RAG solves a fundamental limitation of traditional LLMs. While these models are amazing at understanding and generating human-like text, they're limited to the data they were trained on—data that becomes outdated the moment training ends. RAG changes this dynamic entirely. Instead of relying solely on built-in knowledge, RAG-enabled systems actively pull relevant information from your current data sources before generating a response. This means your AI can reference your latest product documentation, company policies, real-time customer data, or any other business-critical information you connect it to. It’s essentially merging the linguistic capabilities of LLMs with real-time access to your organization's data. When a user asks a question, the system doesn't just generate a response—it first searches through your connected data sources, finds relevant information, and uses that to inform its answer. The result is responses that aren't just linguistically fluent, but factually grounded in your actual business data. Rather than hoping an LLM's training data includes accurate information about your specific needs, RAG lets you explicitly control and update the knowledge your AI system draws on. How does Retrieval-Augmented Generation work? RAG ultimately follows a sequence of steps that transform a user's question into an accurate, sourced response. Here’s what it roughly looks like: Query processing: When a user asks a question, RAG analyzes and processes the query to understand what information it needs to retrieve. The system converts the natural language question into a format optimized for searching your knowledge base. Information retrieval: The system searches through your connected data sources: documentation, databases, or other knowledge bases. It uses advanced semantic search capabilities to find the most relevant information (not just exact keyword matches). Context assembly: RAG gathers the retrieved information and prepares it as context for the LLM. This step involves selecting the most relevant pieces of information and formatting them in a way that helps the LLM generate the most accurate response. Response generation: The LLM receives both the original query and the retrieved context. It uses this combination to generate a response that incorporates the specific information found in your knowledge base while maintaining natural language. Source attribution: Finally, RAG can track which sources were used to generate the response to help the system cite its sources and provide users with references to the original documents. You can fine-tune each of these steps based on your specific needs—whether you're prioritizing speed, accuracy, or comprehensiveness. The big differentiator is that RAG doesn't just search for answers: it intelligently combines retrieved information with the LLM's language abilities to create responses that are both informed and natural. Major benefits of implementing RAG Implementing RAG is about more than just improving accuracy (though, that’s typically the primary reason). It fundamentally transforms how your AI systems interact with your business data and serve your users. Here’s how: Better accuracy and reliability: RAG reduces AI hallucinations by grounding responses in actual business data. Instead of generating plausible-sounding but potentially incorrect responses, your AI provides answers backed by your verified sources. This matters for customer-facing applications where accuracy directly impacts trust and satisfaction. Real-time knowledge access: Traditional LLMs rely on static training data, but RAG-enabled systems can access and use your latest information. New product launches, policy updates, or market changes are immediately reflected in AI responses without any retraining. This keeps your AI current without the huge costs of model updates. Reduced operating costs: Using RAG helps you avoid the expensive and time-consuming process of retraining models for specific use cases. Instead, you simply just update your knowledge base, and RAG automatically incorporates the new information into responses. Improved compliance and auditability: Every response can be traced back to its source documents, and this creates a clear audit trail for compliance purposes. This transparency is necessarily in regulated industries where verifiability is mandatory. RAG makes it simple to show where AI systems get their information. More user trust: When AI systems can cite their sources and provide references, users are more likely to trust and rely on them. This increased confidence leads to higher adoption rates and better use of AI tools across your organization. Scalable knowledge management: Rather than relying on employees to remember and share information, your AI can access and use your entire knowledge base consistently and accurately. This makes specialized knowledge more accessible across your organization. Faster time to value: Instead of spending months fine-tuning models for your specific use case, RAG lets you leverage existing LLMs with your business data quickly. This means faster deployment and quicker realization of AI investments. The most common RAG implementation patterns Every organization's path to implementing RAG looks different. However, a few patterns have become more popular across certain industries. Here are the most successful approaches we’ve seen in production environments: Customer support automation RAG has changed customer service AI by helping support chatbots tap directly into support documentation, product manuals, and historical ticket resolutions. Customers get specific answers (instead of infuriating general responses) drawn from your actual support materials. This pattern works best when you have extensive documentation but struggle with response accuracy and consistency. Internal knowledge management Large organizations struggle with information silos and knowledge access. RAG-powered internal tools can search across departmental documentation, meeting notes, and internal wikis to give employees accurate, sourced answers about company policies, procedures, and best practices. Document analysis and insights RAG systems can process and analyze large document collections (from legal contracts to research papers) and answer specific questions about their contents. The advantage here is finding and synthesizing information across multiple documents without compromising accuracy or source attribution. This pattern is great for research, legal, and compliance teams. Technical documentation search Development teams and technical users use RAG systems to search across API documentation, codebase comments, and technical specifications. Instead of digging through multiple sources, developers can ask natural language questions and get accurate, contextualized responses with links to relevant documentation. Sales and product intelligence Sales teams can use RAG to access current product information, competitor analysis, and customer case studies during customer interactions. RAG gives your representatives accurate, up-to-date information when they need it most. This pattern works best when product information changes frequently but you can’t sacrifice accuracy. Compliance and policy guidance RAG systems can provide guidance on policies and procedures while citing specific regulations or internal policies. This maintains compliance requirements while providing clear audit trails for all recommendations. Each of these patterns can be customized and combined based on your specific needs. They provide proven starting points for RAG implementation. Your job is to find the pattern that aligns with your immediate business challenges while laying the groundwork for future expansion. 7 ways to measure RAG success Implementing RAG isn't cheap or simple. You need to know if all that effort is actually paying off. But measuring success goes beyond just checking if your AI gives better answers. Here are a few ways you can check to see if your RAG solution is working the way you want: Response accuracy score: Compare AI responses against known correct answers from your knowledge base. Don't just look for exact matches—evaluate whether the response captures the key information and context correctly. Track this over time to see if accuracy improves as you refine your system. Source relevance: Are the sources your RAG system pulls actually relevant to the question? A good RAG implementation should consistently retrieve information that matters, not just tangentially related content. Track the percentage of retrieved sources that directly address the user's query. User satisfaction metrics: The real test is whether users find the responses helpful. Track satisfaction scores, thumbs up or down ratings, or how often users need to rephrase their questions to get useful answers. Low scores here often point to gaps in your knowledge base or retrieval problems. Response time: Monitor how long it takes to generate responses, especially as your knowledge base grows. If response times creep up, you might need to optimize your retrieval process or index structure. Knowledge coverage: Track which parts of your knowledge base are being used and which aren't. This helps identify gaps in your content and areas where you might need better documentation. Low usage might mean either unnecessary content or retrieval problems. Cost per query: Calculate the total cost of running your RAG system divided by the number of queries processed. This should include computing resources, storage, and any API calls. Compare this against the value delivered to guarantee you're getting good ROI. Hallucination rate: Monitor how often your system generates responses that aren't supported by the retrieved sources. This is your canary in the coal mine for accuracy problems. A spike here means something's wrong with either retrieval or response generation. You're looking for trends and patterns, not just snapshot numbers. Plus, don't forget to get qualitative feedback from your users—sometimes the most valuable insights come from actual conversations about what's working and what isn't. How to get started with RAG Implementing RAG doesn't have to be overwhelming. Sure, you could spend months perfecting every detail, but it's better to start small and iterate. Here's a practical roadmap to get you started: Start with a clear use case: Don't try to boil the ocean. Pick a specific problem where accurate AI responses really matter: maybe it's customer support for your most common questions or helping sales teams access product specs. Gather your knowledge base: Round up the documents and data sources you'll need. This could be support tickets, product manuals, internal wikis, or whatever contains the truth you want your AI to reference. Don't worry about perfection. Start with what you have. Choose your tools: You'll need three main components: an LLM (like GPT or Claude), a vector database for storage and retrieval, and embedding models to connect them. Popular stacks include OpenAI + Pinecone or Anthropic + Weaviate, but there are plenty of options out there. Build a proof of concept: Start small. Build a basic RAG system that handles a subset of your use case. Test it with real queries, measure the results, and gather feedback. This helps you identify potential issues before going all-in. Scale gradually: Once your proof of concept works, slowly expand your knowledge base and use cases. Monitor performance and costs as you grow. This is where you'll learn what really matters for your specific situation. RAG is more of a journey than a destination. Your first implementation won't be perfect, and that's okay. Start simple, measure what matters, and improve based on real usage. It’s all about getting something useful up and running that you can build on. Scale your RAG implementation with confidence RAG isn't a magic bullet. However, when you get it right, it transforms how your AI delivers value. The challenge isn't understanding why you need RAG, it's implementing it in a way that actually moves the needle for your business. This is where having the right data infrastructure becomes non-negotiable. Your RAG implementation is only as good as the data foundation it's built on. You need reliable data pipelines, strong integration capabilities, and enterprise-grade security to make it work at scale. Matillion's data platform provides the foundation you need for successful RAG implementation: Unified data integration from any source so your RAG system has access to all relevant information Built-in data quality checks to maintain accuracy and reliability Enterprise-grade security and governance to protect sensitive information Scalable architecture that grows with your needs Expert support to guide your implementation Don't let data infrastructure challenges hold back your RAG program. Whether you're just starting out or looking to scale existing implementations, we'll help you build a foundation that delivers consistent, accurate results. Start your free trial to start building data pipelines that work (all without code). View all Blog Contents Don't miss a thing! Try Free Featured Resources Blog Intuitive UI for Debugging and Validation Discover how Matillion’s intuitive debugging and validation features and how they streamline error detection and resolution, ... Learn more Blog Box: More Than Just File Storage! Unlock the power of Box data with Matillion's new Box Flex Connector. Seamlessly extract, transform, and load Box content into ... Learn more Blog The Power of Generative AI Discover how GenAI integration gives MatiHelper its intelligence and simplifies AI adoption in data pipelines. Learn how Data ... Learn more View all resources Get started today Matillion's comprehensive data pipline platform offers more than point solutions. Start a free trial Book a demo Back to top Product Data Productivity Cloud AI Matillion ETL Connectors Security All Features Pricing Product Tour Request a demo Free Trial Resources All Blog Documentation Product tour Hub Academy Exchange Events Partners Community Roadmap Glossary Solutions Overview By Cloud Data Platform By Use Case By Industry Success Stories About Overview Why Matillion Leadership Careers Investors Brand and Media kit Contact Support & Services Support Services & Training Partner portal Support Portal © Copyright 2025 Matillion Ltd | Legal | Privacy Policy | Disclaimer | Modern Slavery Statement

Results for query: Combining AST and RAG for better knowledge retrieval in software engineering

Search Engine: Bing
Source: https://cloud.google.com/blog/products/ai-machine-learning/optimizing-rag-retrieval
RAG systems: Best practices to master evaluation for accurate and reliable AI. | Google Cloud Blog Jump to Content Cloud Blog Contact sales Get started for free Cloud Blog Solutions & technology AI & Machine Learning API Management Application Development Application Modernization Chrome Enterprise Compute Containers & Kubernetes Data Analytics Databases DevOps & SRE Maps & Geospatial Security Security & Identity Threat Intelligence Infrastructure Infrastructure Modernization Networking Productivity & Collaboration SAP on Google Cloud Storage & Data Transfer Sustainability Ecosystem IT Leaders Industries Financial Services Healthcare & Life Sciences Manufacturing Media & Entertainment Public Sector Retail Supply Chain Telecommunications Partners Startups & SMB Training & Certifications Inside Google Cloud Google Cloud Next & Events Google Cloud Consulting Google Maps Platform Google Workspace Developers & Practitioners Transform with Google Cloud Contact sales Get started for free AI & Machine Learning Optimizing RAG retrieval: Test, tune, succeed December 19, 2024 Hugo Selbie Customer Partner & Solutions Engineer Tom Pakeman Customer Partner & Solutions Engineer Join us at Google Cloud Next Early bird pricing available now through Feb 14th. Register Retrieval-augmented generation (RAG) supercharges large language models (LLMs) by connecting them to real-time, proprietary, and specialized data. This helps LLMs deliver more accurate, relevant, and contextually aware responses, minimizing hallucinations and building trust in AI applications. But RAG can be a double-edged sword: while the concept is straightforward – find relevant information and feed it to the LLM – its implementation is difficult to master. Done incorrectly, it can impact user trust in your AI's reliability. The culprit is often a lack of thorough evaluation. RAG systems that are not thoroughly evaluated lead to ‘silent failures’ which can undermine the reliability and trustworthiness of the system as a whole. In this blog post, we'll equip you with a series of best practices to identify issues within your RAG system and fix them with a transparent, automated evaluation framework. $300 in free credit to try Google Cloud AI and ML Build and test your proof of concept with $300 in free credit for new customers. Plus, all customers get free monthly usage of 20+ products, including AI APIs. Start building for free Step 1. Create a testing framework Testing a RAG system consists of running a set of queries against the tool and evaluating the output. A key prerequisite for rapid testing and iteration is to decide on a set of metrics as the definition of success, and calculate them in a rigorous, automated, and repeatable fashion. Below are some guidelines: Assemble a test dataset of high-quality questions Ensure that your test set covers a broad subset of the underlying data, and includes variations in phrasing and question complexity that match real-world use cases. Pro tip: It’s a good idea to consult with stakeholders and end users here to ensure the quality and relevance of this dataset. Assemble a ‘golden’ reference dataset of desired outputs to use in evaluation Although some metrics can be calculated without a reference dataset, having a set of known-good outputs allows us to produce a more comprehensive and nuanced range of evaluation metrics. Only change one variable at a time between test runs There are many features of a RAG pipeline that can make a difference – by changing them one at a time, we can be sure that a change in evaluation scores is attributable to a single feature alone. Similarly, we must ensure that between test runs we do not change the evaluation questions being used, the reference answers, or any system-wide parameters and settings. The basic process here is to change one aspect of the RAG system, run the battery of tests, adapt the feature, run the exact same battery of tests again and then see how the test results have changed. Once you are satisfied that a feature cannot be improved, freeze the configuration and move on to testing a separate part of the process. This testing framework can be visualized as three components: Reference questions and answers: The set of queries to be evaluated. Depending on which metrics are being calculated we may include corresponding reference answers. RAG processes The retrieval and summarization techniques being changing and evaluated Question outputs The evaluation outputs as scored by the testing framework Choosing appropriate metrics Establishing the best metrics to assess your system involves trial and error. Predefined testing frameworks exist that have been designed to speed up the process by providing prebuilt metrics that can also be adapted to your specific use case. This allows you to quickly generate baseline scores for the evaluation and refinement of your RAG system. From this baseline, you can then systematically modify retrieval and generation capabilities and measure any improvements. Common RAG evaluation frameworks include: Ragas Ragas is an open-source tool for evaluating RAG systems. It measures key aspects like factual accuracy, answer relevance, and how well retrieved content matches the question. Ragas also helps generate test data, making it easier for developers to improve RAG systems for accuracy and usefulness. Vertex AI gen AI evaluation service The Vertex AI gen AI evaluation service helps users test and compare generative models or applications based on custom metrics. It supports model selection, prompt engineering, and fine-tuning and allows users to define metrics, prepare data, run evaluations, and review results. The service works with Google's models, third-party models, and open models across various languages, using both model-based and computation-based assessment methods. Example metrics Model-based metrics utilize a proprietary Google model to assess the output of a candidate model. Functioning as an evaluator, this model scores responses based on predefined criteria. Pointwise metrics: The judge model assigns a numerical score (e.g., on a scale of 0-5) to the candidate model's output, indicating its alignment with the evaluation criteria. A higher score signifies a better fit. Pairwise metrics: The judge model compares the responses of two models and identifies the superior one. This approach is frequently employed to benchmark a candidate model against a baseline model. Computation-based metrics: These metrics utilize mathematical formulas to compare the model's output against a ground truth or reference. Popular examples include ROUGE and BLEU. Opinionated tiger team actions Collaborate with stakeholders to develop a set of "golden" question inputs. These questions should accurately reflect the main use cases the RAG system is intended to address. It's crucial to include a diverse range of query types, such as simple, complex, multi-part, and misspelled queries to ensure comprehensive testing. Make use of the Vertex AI generative AI evaluation framework. This framework allows developers to quickly implement multiple test metrics, and run multiple tests on a model’s performance with minimal setup. It offers a fast feedback loop, so improvements can be made rapidly. Conduct a pointwise evaluation of the RAG retrieval system. Generate model scores based on the following criteria: Response groundedness: The extent to which the generated text aligns with the factual information retrieved from the source documents. Verbosity: The length and detail of the response. While beneficial for providing comprehensive understanding, excessive verbosity may indicate difficulty in concisely and accurately answering the question. You may wish to tune this metric based on your use case. Instruction following: The system's ability to generate text that accurately and comprehensively adheres to given instructions, ensuring the output is relevant and aligned with user intent. Question answer quality as related to instructions: The ability of the RAG system to generate text that correctly answers a user's question with a high level of detail and coherence. Store results in a shared location such as Vertex AI Experiments, which allows for simple comparisons over time. Step 2. Root cause analysis and iterative testing The goal of setting up a repeatable testing framework is ideally understanding the root cause of issues. RAG is fundamentally based on two components: (1) the retrieval accuracy of your nearest neighbor matches and (2) the context that you provide to the LLM that generates your responses. Identifying and isolating these components individually allows you to determine the specific areas that may be causing problems and formulating testable hypotheses that can be performed as experiments and run in Vertex AI using the Gen AI evaluation framework. Typically when performing a root cause analysis exercise, the user will execute a testing run as a baseline, modify the implementation of one of the RAG components, and re-execute the testing run. The delta between the output scores of the testing metrics is the influence of the RAG component that was altered. The goal in this phase is to modify and document the components carefully, aiming to optimize towards a maximum score for each of the chosen metrics. Often the temptation is to make multiple modifications between testing runs which can mask the impact of a specific process and whether it was successful in creating a measurable change in your RAG system. Examples of RAG experiments to run Example RAG components to experiment with: What is the ideal number of neighbors for a document chunk that gets passed into an LLM to improve answer generation? How does embedding model choice affect retrieval accuracy? How do different chunking strategies affect quality? For example, adjusting variables like chunk size or overlap, or exploring strategies such as pre-processing chunks to summarize or paraphrase them with a language model. When it comes to generation, simply comparing Model A vs. Model B or Prompt A vs. Prompt B is particularly useful for fine-tuning prompt design or adjusting model configurations, helping developers to optimize models and prompts for specific use cases. What happens when you enrich documents with metadata like title, author, and tags for better retrieval signals? Opinionated tiger team actions Test model A vs model B for generation tasks (simple and can produce measurable results) Test chunking strategies for retrieval within a single embedding model (400 chars, 600 chars, 1200 chars, Full document text) Test pre-processing of long chunks to summarize them to smaller chunk sizes. Test what data is passed to the LLM as context. For example, do we pass the matched chunks themselves, or use these as a lookup to find the source document and pass the whole document text to the LLM, making use of long context windows. Step 3. Human evaluation Although quantitative metrics created by your testing framework provide valuable data, qualitative feedback from real users is also crucial. Automated testing tools are efficient for scalability and rapid iteration, but they cannot replicate human judgment in ensuring high-quality output. Human testers can evaluate subtle aspects like the tone of responses, the clarity of explanations, and potential ambiguity. Combining qualitative and quantitative testing provides a more holistic understanding of your RAG system’s performance. Human tests are typically run after you’ve achieved a solid level of baseline answer quality by optimizing evaluation metrics through the automated testing framework. You may wish to include human response evaluation as part of your broader user-testing motions for the system as a whole, such as performance, UX, etc. Similar to previous experiments, human testers can focus on specific system features following structured steps, or they can assess the overall application and provide comprehensive qualitative feedback. Because human testing is time consuming and repetitive, it is essential to identify users who are engaged and willing to provide meaningful feedback. Opinionated tiger team actions Identify key personas based on the RAG system’s target users Recruit a representative sample of participants that matches these personas to ensure realistic feedback. If possible, include both technical and non-technical user groups for testing Sit with the user (if possible) to ask follow-up questions and dig into the detail of their responses Conclusion To begin your own evaluation, explore Google Cloud’s generative AI evaluation service, where you can create both prebuilt and custom evaluation methodologies to enhance your RAG system. Posted in AI & Machine Learning Developers & Practitioners Related articles AI & Machine Learning How to build a strong brand logo with Imagen 3 and Gemini By Layolin Jesudhass • 4-minute read Sustainability Designing sustainable AI: A deep dive into TPU efficiency and lifecycle emissions By David Patterson • 4-minute read Partners Helping our partners co-market faster with AI By Aimee Catalano • 6-minute read Application Development Improving model performance with PyTorch/XLA 2.6 By Kyle Meggs • 5-minute read Footer Links Follow us Google Cloud Google Cloud Products Privacy Terms Cookies management controls Help Language‪English‬‪Deutsch‬‪Français‬‪한국어‬‪日本語‬

Source: https://buzzclan.com/data-engineering/retrieval-augmented-generation/
Retrieval-Augmented Generation: Ultimate Guide Skip to content Toggle Navigation Home About Services IT Infrastructure Business Intelligence Managed IT Services Data And Analytics Cyber Security Database As A Service Cloud Consulting Staffing Services Digital Transformation QA Services Data Engineering Workforce Management ServiceNow Mobile App Healthcare Data Services View All Services Services IT Infrastructure Services Cyber Security Services Database As A Service Cloud Consulting Staffing Services Digital Transformation All Services Company About Careers Privacy Policy Applicant Privacy Policy CCPA DIR-ITSAC Blogs Contact Us Retrieval-Augmented Generation: Revolutionizing AI with Dynamic Knowledge Integration Home Data Engineering Retrieval-Augmented Generation: Revolutionizing AI with Dynamic Knowledge Integration Ananya Arora Nov 25, 2024 Introduction In the rapidly evolving landscape of artificial intelligence and natural language processing, Retrieval-Augmented Generation (RAG) has emerged as a groundbreaking approach to enhancing the capabilities of large language models. As organizations and researchers seek to unlock the full potential of AI-driven language understanding and generation, RAG stands out as a powerful technique that bridges the gap between vast knowledge bases and dynamic, context-aware responses. This comprehensive exploration of Retrieval-Augmented Generation aims to provide readers with a deep understanding of RAG, its inner workings, and its transformative impact on natural language processing. From its fundamental principles to real-world applications, we will explore RAG’s intricacies and illuminate its role in shaping the future of AI-driven communication and problem-solving. Throughout this piece, we will navigate through several key areas: defining RAG and its historical context, unraveling the technical aspects of how it functions, examining its myriad benefits, exploring diverse applications across industries, providing implementation strategies, showcasing real-world case studies, and peering into future trends and ongoing research. By the end of this journey, readers will gain a comprehensive understanding of the Retrieval-Augmented Generation and its potential to revolutionize how we interact with and leverage artificial intelligence in our daily lives and professional endeavors. What is Retrieval-Augmented Generation (RAG)? Retrieval-augmented generation (RAG) is an innovative approach in natural language processing that combines the power of large language models with the ability to retrieve and incorporate external knowledge. At its core, RAG is a hybrid system that enhances the generation capabilities of AI models by allowing them to access and utilize relevant information from a vast corpus of data during the text generation process. The fundamental idea behind RAG is to augment the inherent knowledge of a pre-trained language model with dynamically retrieved information. This approach addresses one of the key limitations of traditional language models: their reliance solely on the knowledge embedded in their parameters during training. By incorporating a retrieval mechanism, RAG enables models to tap into up-to-date, context-specific information, leading to more accurate, relevant, and informative outputs. The concept of RAG can be traced back to the ongoing efforts in the AI community to create more robust and reliable language models. As researchers and developers grappled with the challenges of maintaining accuracy and relevance in rapidly changing information landscapes, the need for a system that could dynamically incorporate new knowledge became apparent. The evolution of RAG is closely tied to advancements in information retrieval systems, question-answering models, and large language models. Early work in this direction focused on improving question-answering systems by retrieving relevant passages from large document collections. As language models grew in size and capability, researchers began exploring ways to combine these models with retrieval mechanisms to enhance their performance on knowledge-intensive tasks. A significant milestone in the development of RAG came with the publication of the paper “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks” by Lewis et al. in 2020. This work introduced a unified framework for retrieval-augmented generation and demonstrated its effectiveness across various natural language processing tasks. Since then, RAG has gained considerable attention in the AI community, with numerous researchers and organizations building upon and refining the concept. The historical context of RAG is also intertwined with the broader trends in AI and NLP. As the limitations of purely generative models became more apparent, especially in tasks requiring up-to-date or specialized knowledge, the AI community began exploring hybrid approaches that could leverage both learned representations and external knowledge sources. RAG emerged as a promising solution to this challenge, offering a flexible and powerful framework for integrating retrieval and generation. In recent years, the rapid advancement of large language models like GPT-3, BERT, and their successors has further accelerated the development and adoption of RAG. These models’ impressive generative capabilities provided an ideal foundation for RAG systems. By combining the fluency and coherence of large language models with the ability to retrieve and incorporate relevant information, RAG opened up new possibilities for creating more intelligent and adaptive AI systems. As we delve deeper into RAG’s workings and applications, it’s important to recognize its place in the broader context of AI development. RAG represents a significant step towards creating more versatile, knowledgeable, and context-aware AI systems. It addresses some key challenges traditional language models face while paving the way for more advanced and capable AI assistants and tools. How Does Retrieval-Augmented Generation Work? Retrieval-augmented generation operates on a sophisticated mechanism that seamlessly integrates information retrieval with text generation. To understand how RAG works, it’s essential to break down its core components and processes and examine how they interact to produce enhanced outputs. At a high level, RAG consists of two primary components: a retriever and a generator. The retriever is responsible for identifying and extracting relevant information from a large corpus of data. At the same time, the generator uses this retrieved information along with its pre-trained knowledge to produce the final output. Let’s delve into each component and the overall process in more detail. The Retriever The retriever component is designed to efficiently search a vast database of documents or knowledge sources to find information relevant to the given input or query. This process typically involves several steps: Indexing: Before retrieval can occur, the knowledge base must be indexed. This involves creating a searchable representation of each document or information, often using techniques like dense vector embeddings or inverted indices. Query Processing: When a new input or query is received, the retriever processes it to create a search-friendly representation. This might involve generating embeddings for the query or extracting key terms. Similarity Matching: The processed query is compared against the indexed documents to find the most relevant matches. This can be done using various similarity metrics, such as cosine similarity for vector embeddings or BM25 for term-based retrieval. Ranking and Selection: The retrieved documents or passages are ranked based on relevance, and a subset is selected for generation. The Generator The generator component is typically a large language model pre-trained on vast text data. In the context of RAG, this model is adapted to work with the retrieved information. The generator’s role includes: Contextual Understanding: Processing the original input or query to understand the context and requirements of the task. Information Integration: Incorporating the retrieved information into its generation process. This often involves attending to both the input and the retrieved documents simultaneously. Text Generation: Producing coherent and relevant text that addresses the input while leveraging the retrieved information. The RAG Process The overall RAG process can be summarized in the following steps: Input Reception: The system receives an input, which could be a question, a prompt, or any text that requires a response. Retrieval: The input is passed to the retriever, which searches the knowledge base and returns relevant documents or passages. Augmentation: The retrieved information is combined with the original input to create an augmented context. Generation: The augmented context is fed into the generator, which produces the final output. Output Delivery: The generated text is returned as the response to the original input. RAG Architecture The architecture of a RAG system is designed to facilitate efficient interaction between the retriever and generator components. Key aspects of the architecture include: Encoder-Decoder Framework: Many RAG systems utilize an encoder-decoder architecture, where the encoder processes the input and retrieved documents, and the decoder generates the output. Attention Mechanisms: Sophisticated attention mechanisms allow the generator to focus on relevant input parts and retrieve information during generation. Parametric and Non-Parametric Knowledge: RAG combines the parametric knowledge embedded in the pre-trained generator with the non-parametric knowledge accessed through retrieval. End-to-end Training: Advanced RAG systems can be trained end-to-end, simultaneously optimizing the retriever and generator components. Caching and Efficiency Optimizations: To improve performance, RAG systems often incorporate caching mechanisms and other optimizations to reduce latency in retrieval and generation. The intricate interplay between retrieval and generation in RAG systems allows for a dynamic and adaptive approach to natural language processing tasks. By leveraging pre-trained knowledge and dynamically retrieved information, RAG can produce more accurate, relevant, and up-to-date responses than traditional language models. As we continue to explore RAG’s benefits and applications, it’s important to remember this underlying mechanism. RAG’s power and versatility come from its ability to retrieve and integrate relevant information on the fly, enabling it to tackle a wide range of knowledge-intensive tasks with improved accuracy and context awareness. Benefits of Retrieval-Augmented Generation Retrieval-augmented generation offers many advantages that address key limitations of traditional language models and open up new possibilities in natural language processing. Let’s explore the primary benefits of RAG in detail: Improved Accuracy One of the most significant benefits of RAG is its ability to dramatically improve generated content’s accuracy. This improvement stems from several factors: Up-to-date Information: RAG can respond based on the latest data by retrieving information from a continually updated knowledge base, reducing the risk of outdated or incorrect information. Fact Verification: The retrieval component allows the system to cross-reference generated content against source documents, enhancing factual accuracy. Reduced Hallucination: Traditional language models sometimes generate plausible-sounding but incorrect information, a phenomenon known as “hallucination.” RAG mitigates this by grounding responses in retrieved facts. Contextual Relevance: By incorporating context-specific information retrieved for each query, RAG can produce more accurate and relevant responses tailored to the user’s specific needs. Improved Specificity: RAG enables models to provide more detailed and specific information by drawing upon a vast knowledge base rather than relying solely on generalized knowledge. Enhanced Knowledge Integration RAG excels at integrating external knowledge into the generation process, offering several advantages: Expanded Knowledge Base: While traditional language models are limited to the knowledge embedded in their parameters during training, RAG can access a much larger and more diverse pool of information. Domain Adaptation: RAG systems can easily adapt to specialized domains by incorporating relevant domain-specific knowledge bases without requiring extensive retraining of the entire model. Multi-source Integration: RAG can seamlessly combine information from multiple sources, providing a more comprehensive and nuanced understanding of complex topics. Dynamic Learning: As the knowledge base is updated, RAG can immediately incorporate new information into its responses without retraining the entire model. Transparency and Explainability: By explicitly retrieving and using external information, RAG can provide clearer explanations for its outputs, enhancing trust and interpretability. Efficiency and Scalability RAG offers significant advantages in terms of efficiency and scalability: Reduced Model Size: By offloading some knowledge to external databases, RAG can achieve high performance with smaller, more efficient language models. Faster Updates: Updating the knowledge base is quicker and more straightforward than retraining large language models, allowing for more frequent and agile knowledge updates. Resource Optimization: RAG can optimize resource usage by retrieving only the necessary information for each query rather than storing all knowledge within the model parameters. Scalable Knowledge Expansion: The retrieval component allows for virtually unlimited expansion of the knowledge base without increasing the size of the core language model. Improved Inference Speed: In many cases, RAG can provide faster inference times compared to extremely large language models, especially for knowledge-intensive tasks. Versatility and Adaptability RAG systems demonstrate remarkable versatility across various applications: Task Flexibility: RAG can be applied to various NLP tasks, from question-answering and summarization to content generation and dialogue systems. Language and Domain Adaptability: By changing the retrieval corpus, RAG can easily adapt to different languages, cultures, and specialized domains. Personalization: RAG can incorporate user-specific or organization-specific knowledge bases, enabling highly personalized interactions. Hybrid Approaches: RAG can be combined with AI techniques, such as reinforcement learning or few-shot learning, to create even more powerful and adaptive systems. Enhanced Ethical and Legal Compliance RAG offers several benefits in terms of ethical and legal considerations: Source Attribution: By explicitly retrieving information, RAG can more easily provide source attributions for its outputs, addressing concerns about copyright and intellectual property. Bias Mitigation: The ability to retrieve information from diverse sources can help reduce biases present in pre-trained models. Content Moderation: RAG systems can be more easily constrained to retrieve from approved sources, helping to ensure the generation of appropriate and compliant content. Auditability: The retrieval process provides an additional layer of transparency, making auditing and verifying the system’s outputs easier. As we continue to explore RAG’s applications and implementation strategies, these benefits highlight why it has become such a promising approach in natural language processing. The combination of improved accuracy, enhanced knowledge integration, efficiency, versatility, and ethical considerations makes RAG a powerful tool for addressing many challenges in developing advanced AI systems for language understanding and generation. Applications of Retrieval-Augmented Generation Retrieval-augmented generation has found various applications across various domains, revolutionizing how we approach knowledge-intensive NLP tasks and enhancing the capabilities of large language models. Let’s explore some of the key applications of RAG: Knowledge-Intensive NLP Tasks RAG has proven particularly effective in tasks that require access to extensive and up-to-date knowledge: Question Answering: RAG excels in open-domain question answering, where it can retrieve relevant information from a large corpus to provide accurate and detailed answers. This is particularly useful in applications like customer support chatbots, educational tools, and research assistants. Fact-Checking and Verification: By retrieving and cross-referencing information from reliable sources, RAG can verify claims and detect misinformation, making it a valuable tool for journalists, researchers, and fact-checking organizations. Information Synthesis: RAG can gather and synthesize information from multiple sources to create comprehensive reports, literature reviews, or state-of-the-art summaries on specific topics. Named Entity Recognition and Linking: RAG can enhance named entity recognition by retrieving contextual information about entities, improving accuracy in tasks like entity linking and disambiguation. Text Summarization: RAG can produce more accurate and informative summaries of long documents or multiple related texts by retrieving key information and context. Large Language Models RAG has significantly enhanced the capabilities of large language models: Knowledge Augmentation: RAG allows large language models to augment their built-in knowledge with retrieved information, enabling them to provide more accurate and up-to-date responses. Specialized Domain Adaptation: By incorporating domain-specific knowledge bases, RAG enables large language models to quickly adapt to specialized fields like medicine, law, or engineering without extensive retraining. Multilingual and Cross-lingual Applications: RAG can help language models bridge language gaps by retrieving and translating relevant information, enabling more effective cross-lingual communication and understanding. Content Generation: In tasks like article writing, report generation, or creative writing, RAG can provide language models with relevant facts, statistics, and context to produce more informative and accurate content. Dialogue Systems: RAG enhances the ability of conversational AI to engage in more informed and context-aware dialogues, making them more useful in applications like virtual assistants, customer service, and educational chatbots. Corrective and Active Retrieval RAG has opened up new possibilities in information retrieval and correction: Dynamic Fact Correction: RAG systems can actively retrieve and correct outdated or incorrect information in real time, ensuring that generated content remains accurate and up-to-date. Contextual Information Retrieval: RAG enables more sophisticated information retrieval by understanding the context of queries and retrieving information that is not just keyword-matched but contextually relevant. Iterative Refinement: Some RAG implementations use active retrieval techniques to iteratively refine their search and generation process iteratively, leading to more precise and relevant outputs. Bias Detection and Mitigation: By actively retrieving from diverse sources, RAG can help identify and mitigate biases in generated content, promoting more balanced and fair outputs. Source Triangulation: RAG can retrieve information from multiple sources to cross-verify facts and provide a more comprehensive view of complex topics. Industry-Specific Applications RAG has found applications across various industries: Healthcare: In medical applications, RAG can assist in diagnosis by retrieving relevant case studies, research papers, and treatment guidelines. It can also help in drug discovery by synthesizing information from vast databases of chemical compounds and clinical trials. Legal: RAG enhances legal research and contract analysis by retrieving relevant case laws, statutes, and precedents. It can also assist in due diligence processes by quickly synthesizing information from large volumes of legal documents. Finance: In financial services, RAG can be used for market analysis, risk assessment, and fraud detection by retrieving and analyzing vast amounts of financial data and news. Education: RAG powers intelligent tutoring systems to retrieve and present relevant educational content based on a student’s queries or learning needs. It can also assist in curriculum development and research. E-commerce: RAG enhances product recommendation systems by retrieving and synthesizing product information, user reviews, and market trends to provide more accurate and personalized recommendations. Journalism and Media: RAG assists in fact-checking, source verification, and content creation by retrieving relevant information from diverse sources and synthesizing it into coherent narratives. Scientific Research: In research applications, RAG can help with literature reviews, hypothesis generation, and experimental design by retrieving and synthesizing information from vast scientific databases and publications. Manufacturing and Engineering: RAG can assist in technical documentation, troubleshooting, and design processes by retrieving relevant specifications, standards, and best practices from extensive technical libraries. Emerging Applications As RAG technology continues to evolve, new and innovative applications are emerging: Augmented Creativity: RAG enhances creative processes in advertising, content creation, and product design by providing relevant inspirations, trends, and contextual information. Personalized Learning: Advanced RAG systems are being developed to create highly personalized learning experiences by retrieving and adapting educational content based on individual learning styles, preferences, and progress. Virtual Assistants and Digital Twins: RAG is enhancing the capabilities of virtual assistants and digital twins, allowing them to provide more accurate, context-aware, and personalized responses by retrieving user-specific or organization-specific information. Automated Journalism: Some news organizations are experimenting with RAG-powered systems that can generate news articles by retrieving and synthesizing information from multiple sources, with human editors providing oversight and final editing. Policy Analysis and Decision Support: Governments and organizations are exploring using RAG in policy analysis and decision-making processes, leveraging its ability to retrieve and synthesize complex information from diverse sources. Implementing Retrieval-Augmented Generation Implementing Retrieval-Augmented Generation (RAG) requires careful planning and consideration of various factors. This section will provide strategies, introduce tools and libraries, and offer a step-by-step tutorial for implementing RAG in various systems. Implementation Strategies Define Your Use Case: Identify the NLP task or problem you want to address with RAG. This will guide your choices regarding model architecture, retrieval strategy, and knowledge base selection. Choose Your Knowledge Base: Select or create a knowledge base relevant to your use case. This could be a collection of documents, a structured database, or a combination of both. Consider factors like data quality, update frequency, and coverage. Select a Retrieval Method: Choose an appropriate retrieval method based on your use case and knowledge base. Options include: – Dense Retrival: Using dense vector embeddings for both queries and documents. – Sparse Retrieval: Traditional keyword-based methods like BM25 or TF-IDF. – Hybrid Approaches: Combining dense and sparse retrieval for better performance. Choose a Generation Model: Select a pre-trained language model that fits your size, performance, and specialization requirements. Popular choices include GPT models, T5, or BART. Design the RAG Architecture: Decide on the overall architecture of your RAG system. This includes how the retriever and generator components interact, how retrieved information will be incorporated into the generation process and any additional components like rerankers or fusion mechanisms. Implement Efficient Indexing: To ensure fast retrieval, implement efficient indexing techniques for large-scale applications. This may involve using specialized data structures or distributed indexing systems. Optimize for Latency and Resource Usage: Consider techniques like caching frequently retrieved information, batching queries, or quantization to reduce the model size and inference time. Implement Monitoring and Logging: Set up robust monitoring and logging systems to track the performance of your RAG implementation, including retrieval accuracy, generation quality, and system latency. Plan for Continuous Improvement: Develop a strategy for continuously updating your knowledge base and fine-tuning your models based on new data and user feedback. Tools and Libraries Several tools and libraries are available to facilitate the implementation of RAG systems: Hugging Face Transformers: Provides pre-trained models and tools for retrieval and generation tasks. It offers easy-to-use interfaces for implementing RAG with models like BART and T5. LangChain : It is a popular framework for developing applications with large language models, including RAG implementations. It provides abstractions for working with different retrieval and generation components. Haystack: It is an open-source framework specifically designed for building RAG systems. It offers modular components for document storage, retrieval, and generation. PyTorch and TensorFlow: These deep learning frameworks provide the foundation for implementing custom RAG architectures and training pipelines. FAISS: It (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors, useful for implementing dense retrieval systems. Elasticsearch: A popular search engine that can be used for implementing sparse retrieval in RAG systems. OpenAI API: Provides access to powerful language models that can be used as the generation component in RAG systems. Pinecone: A vector database service that efficiently stores and retrieves dense vector embeddings. Step-by-Step Tutorial Here’s a basic tutorial for implementing a simple RAG system using Python and some popular libraries: Set up the environment: ```python !pip install transformers pytorch faiss-cpu import torch from transformers import DPRQuestionEncoder, DPRContextEncoder, RAG, RagTokenizer, RagSequenceForGeneration ``` Prepare the knowledge base: ```python # For this example, we'll use a small in-memory knowledge base knowledge_base = [ "The capital of France is Paris.", "The Eiffel Tower is located in Paris.", "Paris is known as the City of Light." ] # Create FAISS index for efficient retrieval question_encoder = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base") ctx_encoder = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base") # Encode the knowledge base ctx_embeddings = ctx_encoder(knowledge_base, return_tensors="pt").pooler_output index = faiss.IndexFlatIP(ctx_embeddings.size(1)) index.add(ctx_embeddings.detach().numpy()) ``` Set up the RAG model: ```python model = RagSequenceForGeneration.from_pretrained("facebook/rag-sequence-nq") tokenizer = RagTokenizer.from_pretrained("facebook/rag-sequence-nq") # Replace the default retriever with our custom one model.set_retriever(index, question_encoder, ctx_encoder, knowledge_base) ``` Generate responses ```python def generate_response(question): input_ids = tokenizer(question, return_tensors="pt").input_ids output = model.generate(input_ids) return tokenizer.decode(output[0], skip_special_tokens=True) # Example usage question = "What is the capital of France?" response = generate_response(question) print(f"Question: {question}") print(f"Response: {response}") ``` This tutorial provides a basic implementation of RAG. In a real-world scenario, you must consider factors like scaling the knowledge base, handling updates, and fine-tuning the model for your specific use case. Remember that implementing RAG often requires iterative refinement and optimization. Start with a simple implementation and gradually enhance it based on performance metrics and user feedback. Case Studies and Real-World Examples Let’s explore some case studies and real-world examples from various industries and use cases to better understand the practical applications and impact of retrieval-augmented generation. Case Study 1: Enhancing Customer Support with RAG Company: (redacted) Inc. is a large technology company providing software solutions to businesses. Challenge: (redacted) Inc. struggled with the increasing volume and complexity of customer support queries. Based on a traditional language model, their existing chatbot system often provided generic or outdated responses, leading to customer frustration and increased workload for human support staff. Solution: The company implemented a RAG-based customer support system. They used their extensive knowledge base of product documentation, FAQs, and past support tickets as the retrieval corpus. The system was designed to understand customer queries, retrieve relevant information, and generate detailed, context-aware responses. Implementation: Knowledge Base Preparation: They indexed their entire support documentation, including product manuals, troubleshooting guides, and resolved ticket histories. Query Understanding: Implemented a fine-tuned BERT model to classify incoming queries into relevant categories. Retrieval System: A dense retrieval system based on sentence transformers was used to find the most relevant documents for each query. Generation Model: Employed a fine-tuned T5 model as the generator, which could incorporate retrieved information into its responses. Human-in-the-Loop: Implemented a confidence scoring system, routing complex queries to human agents when necessary. Results: 40% reduction in average response time for customer queries. 30% increase in first-contact resolution rate. 25% reduction in escalations to human agents. 90% positive feedback from customers on the accuracy and helpfulness of responses. Significant reduction in the workload of human support staff, allowing them to focus on more complex issues. Key Learnings: Regularly updating the knowledge base was crucial for maintaining accuracy. Implementing a feedback loop from human agents to improve the system’s performance over time was highly beneficial. Transparency in AI-generated responses, including source citations, increased customer trust. Case Study 2: RAG for Medical Research Assistance Organization: MedResearch Institute, a leading medical research facility. Challenge: MedResearch Institute researchers needed help keeping up with the rapidly growing medical literature. They needed a system that could assist in literature reviews, hypothesis generation, and identifying potential research directions. Solution: The institute developed a RAG-based research assistant system that could process natural language queries, retrieve relevant information from medical databases and journals, and generate comprehensive summaries and insights. Implementation: Data Integration: Integrated multiple medical databases, including PubMed, clinical trial registries, and internal research repositories. Specialized Embeddings: Developed domain-specific embeddings trained on medical literature to improve retrieval accuracy. Multi-step Retrieval: Implemented a multi-step retrieval process that identified relevant papers and extracted key information. Abstractive Summarization: A biomedical-specific T5 model fine-tuned on medical literature was used to generate summaries and insights. Citation and Fact-Checking: Implemented a system to provide citations for generated content and cross-verify facts across multiple sources. Results: 50% reduction in time spent on initial literature reviews for new research projects. 35% increase in relevant papers identified for each research query. 28% improvement in hypothesis generation, as measured by the number of novel research directions pursued. 95% accuracy in generated summaries, as verified by domain experts. Significant increase in cross-disciplinary insights due to the system’s ability to connect information from various medical subfields. Key Learnings: Domain-specific training and fine-tuning were crucial for high performance in specialized medical fields. Explaining and citing sources for generated content was essential for building trust among researchers. Continuous updates to the knowledge base and model were necessary to keep up with the rapidly evolving field of medical research. Examples and Best Practices Let’s understand the key best practices for RAG systems via examples. Legal Research and Contract Analysis Law firms use RAG systems to assist in legal research and contract analysis. These systems can quickly retrieve relevant case laws, statutes, and precedents, significantly reducing the time lawyers spend researching. Best practices include: Regularly updating the legal database to include the latest rulings and legislative changes. Implementing strict data security measures to protect sensitive legal information. Training the system to understand and interpret legal jargon and complex legal concepts. Personalized Education EdTech companies are leveraging RAG to create adaptive learning platforms. These systems can retrieve relevant educational content based on a student’s current knowledge level and learning style. Best practices include: Developing a diverse knowledge base that covers various learning styles and difficulty levels. Continuously implement a feedback mechanism to improve content recommendations based on student performance. Ensuring that retrieved content is age-appropriate and aligns with curriculum standards. Financial Analysis and Reporting Financial institutions are using RAG systems for market analysis and report generation. These systems can retrieve and synthesize information from various financial sources to produce comprehensive reports. Best practices include: Implementing real-time data integration to ensure the most up-to-date financial information is used. Developing robust fact-checking mechanisms to verify financial data and claims. Creating clear templates and guidelines for generated reports to ensure consistency and compliance with financial reporting standards. Content Creation and Journalism Media organizations are experimenting with RAG systems to assist in content creation and fact-checking. These systems can retrieve relevant information, verify claims, and even generate drafts of news articles. Best practices include: Implementing strong editorial oversight to ensure the quality and accuracy of AI-generated content. Developing clear guidelines for source attribution and transparency in AI-assisted journalism. Training the system to identify and mitigate potential biases in retrieved information and generated content. Technical Documentation and Knowledge Management Technology companies are using RAG systems to manage and retrieve technical documentation. These systems can assist developers and engineers in finding relevant information quickly. Best practices include: Implementing version control in the knowledge base to manage documentation for different software versions. Developing domain-specific retrieval models that understand technical jargon and concepts. Creating a user feedback system to improve retrieved information’s relevance and accuracy. These case studies and examples demonstrate the versatility and potential of the Retrieval-Augmented Generation across various industries. They highlight the importance of domain-specific adaptation, continuous learning, and human oversight in implementing successful RAG systems. As organizations continue exploring and refining RAG applications, we expect to see even more innovative use cases emerge, further transforming how we interact with and leverage information in our professional and personal lives. Future Trends and Research As the Retrieval-Augmented Generation evolves, several emerging trends and ongoing research directions shape its future. These developments promise to enhance RAG systems’ capabilities further and expand their applications across various domains. Emerging Trends Trends Description Multimodal RAG Current research focuses on extending RAG beyond text to include other modalities such as images, audio, and video. This would allow RAG systems to retrieve and generate content across different media types, opening up new possibilities in visual question answering, audio transcription, and video captioning. Conversational RAG There's a growing interest in developing RAG systems that can maintain context over multiple conversation turns. This would enable more natural and coherent interactions in applications like chatbots and virtual assistants. Federated RAG Researchers are exploring federated RAG systems to address privacy concerns and enable collaborative learning. These systems would allow multiple organizations to train and improve RAG models without sharing sensitive data. Quantum-Inspired RAG As quantum computing advances, quantum-inspired algorithms could enhance the retrieval component of RAG systems, potentially leading to faster and more accurate information retrieval. Explainable RAG There's an increasing focus on making RAG systems more transparent and interpretable. This includes developing methods to explain why certain information was retrieved and how it influenced the generated output. Adaptive RAG Future RAG systems may dynamically adapt their retrieval and generation strategies based on the task, user preferences, and available computational resources. Cross-Lingual and Multilingual RAG Researchers are working on RAG systems that can seamlessly operate across multiple languages, retrieving information in one language and generating responses in another. Ongoing Research Efficient Retrieval at Scale: As knowledge bases grow larger, efficient retrieval becomes increasingly challenging. Ongoing research is focused on developing more scalable retrieval methods, including: Hierarchical retrieval architectures Learned index structures Approximate nearest neighbor search techniques Dynamic Knowledge Incorporation: Researchers are exploring ways to make RAG systems more adaptable to new information. This includes: Continuous learning approaches that can update the retrieval and generation components in real-time Methods for quickly incorporating new knowledge without full retraining Retrieval Augmented Training: Beyond using retrieval during inference, ongoing work is being done on incorporating retrieval into the training process. This could lead to more knowledgeable and efficient language models. Improved Integration of Retrieved Information: Current research is focusing on better ways to integrate retrieved information into the generation process, including: More sophisticated attention mechanisms Methods for resolving conflicts between retrieved information and the model’s built-in knowledge Bias Mitigation in RAG Systems: Researchers are investigating techniques to identify and mitigate biases in RAG systems’ retrieval and generation components. RAG for Specialized Domains: There’s ongoing work on adapting RAG systems for highly specialized domains like scientific research, legal analysis, and medical diagnosis. This includes developing domain-specific retrieval and generation models. Long-form Generation with RAG: Researchers are exploring ways to use RAG to generate longer, more coherent text, such as full articles or reports. Meta-learning for RAG: There is interest in developing RAG systems that can quickly adapt to new tasks or domains with minimal fine-tuning using meta-learning techniques. Significant Papers: Several recent papers have made important contributions to the field of Retrieval-Augmented Generation: “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks” (Lewis et al., 2020): This seminal paper introduced the RAG framework, demonstrating its effectiveness across various NLP tasks. It laid the groundwork for much of the subsequent research in this area. “REALM: Retrieval-Augmented Language Model Pre-Training” (Guu et al., 2020): This paper proposed a method for incorporating retrieval into the pre-training process of language models, showing improvements in both efficiency and performance. “Dense Passage Retrieval for Open-Domain Question Answering” (Karpukhin et al., 2020): While not specifically about RAG, this paper introduced significant improvements in dense retrieval methods, which have been widely adopted in RAG systems. “Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering” (Izacard and Grave, 2021): This work proposed the Fusion-in-Decoder approach, which improved the integration of retrieved passages in the generation process. “Retrieval-Enhanced Machine Learning” (Metzler et al., 2021): This paper provided a broader perspective on retrieval-augmented machine learning, discussing applications beyond NLP and potential future directions. “Improving Language Models by Retrieving from Trillions of Tokens” (Borgeaud et al., 2022): This work from DeepMind demonstrated the potential of scaling up the retrieval corpus to trillions of tokens, showing significant improvements in language model performance. “RETRO: Improving Language Models by Retrieving from Trillions of Tokens” (Borgeaud et al., 2022): This paper introduced a new architecture for retrieval-enhanced language models that can efficiently scale to large retrieval databases. “Atlas: Few-shot Learning with Retrieval Augmented Language Models” (Izacard et al., 2022): This work explored using RAG for few-shot learning, showing how retrieval can enhance a model’s ability to adapt to new tasks with limited examples. These ongoing research directions and emerging trends highlight the dynamic nature of the field and the significant potential for further advancements in the Retrieval-Augmented Generation. As researchers continue pushing the boundaries of what’s possible with RAG, we expect to see more powerful, efficient, and versatile systems that can handle increasingly complex tasks across various domains. Conclusion Throughout this exploration, we’ve delved into RAG’s core concepts, examining its inner workings, benefits, and wide-ranging applications across various industries. We’ve seen how RAG addresses key limitations of conventional language models, such as the inability to access up-to-date information and the tendency to generate inaccurate or hallucinated content. By grounding generated responses in retrieved information, RAG systems can provide more reliable, factual, and contextually relevant outputs. The benefits of RAG are manifold. RAG offers advantages over traditional approaches, from improved accuracy and enhanced knowledge integration to increased efficiency and scalability. Its ability to dynamically incorporate external knowledge makes it particularly well-suited for knowledge-intensive tasks and specialized domains, where access to accurate and up-to-date information is crucial. We’ve explored diverse applications of RAG across industries, from enhancing customer support and assisting in medical research to revolutionizing legal research and personalized education. These case studies and examples demonstrate RAG’s versatility and transformative potential in solving real-world problems and improving decision-making processes. The implementation of RAG, while complex, is becoming increasingly accessible thanks to the growing ecosystem of tools, libraries, and frameworks. As we’ve discussed, successful implementation requires careful consideration of various factors, from choosing the right knowledge base and retrieval method to designing an effective integration between retrieval and generation components. Looking to the future, the field of RAG is rich with promising research directions and emerging trends. From multimodal and conversational RAG to quantum-inspired approaches and explainable systems, ongoing research continues to push the boundaries of what’s possible with this technology. As RAG systems become more sophisticated, we expect more innovative applications and use cases to emerge. However, as with any powerful technology, the development and deployment of RAG systems come with responsibilities. Issues of bias, privacy, and ethical use of information must be carefully considered and addressed. The challenge lies in harnessing the power of RAG while ensuring its responsible and beneficial use for society. FAQs What is the difference between RAG and traditional language models? RAG combines a retrieval system with a generative model, allowing it to access external knowledge during generation. Traditional language models rely solely on their pre-trained parameters, limiting their ability to incorporate new or specific information. This makes RAG more flexible and capable of providing up-to-date, contextually relevant responses. How does RAG handle information not in its knowledge base? RAG will rely more heavily on its generative model when faced with information outside its knowledge base. However, it may also retrieve related information to provide context. If the system is well-designed, it should indicate when it’s unsure or lacks specific information, prompting users to verify or provide additional details. Can RAG be used for tasks other than text generation? While primarily used for text-based tasks, RAG principles are being applied to other domains. Researchers are exploring multimodal RAG systems that can work with images, audio, and video. These systems could assist in tasks like answering visual questions, audio transcription, or generating visual content based on textual descriptions. How often does the knowledge base in a RAG system need to be updated? The frequency of updates depends on the specific application and the rate at which relevant information changes. For rapidly evolving fields like current events or scientific research, frequent updates (daily or weekly) may be necessary. For more stable domains, less frequent updates might suffice. Many systems are designed to allow continuous or real-time updates to the knowledge base. Is RAG more computationally expensive than traditional language models? Due to the additional retrieval step, RAG can be more computationally intensive. However, smaller generative models can offset this, as some knowledge is offloaded to the external database. Overall efficiency depends on factors like the knowledge base’s size, the retrieval mechanism’s complexity, and the specific task at hand. How does RAG handle potential biases in its knowledge base? Bias in RAG systems can stem from both the retrieval corpus and the generative model. Mitigating bias involves carefully curating the knowledge base, implementing diverse and representative data sources, and using techniques to detect and correct biases in retrieval and generation processes. Ongoing research focuses on developing more robust methods for bias detection and mitigation in RAG systems. Can RAG be used in multilingual or cross-lingual applications? Yes, RAG can be adapted for multilingual and cross-lingual applications. This involves using multilingual knowledge bases, retrieval systems, and generative models capable of understanding and generating text in multiple languages. Some advanced RAG systems can retrieve information in one language and generate responses in another, facilitating cross-lingual knowledge transfer. How does RAG ensure the privacy and security of sensitive information? Privacy and security in RAG systems depend on how the knowledge base is managed and accessed. Techniques like federated learning, differential privacy, and secure enclaves can be employed to protect sensitive data. Additionally, careful access controls and data anonymization can be implemented to ensure that only appropriate information is retrieved and used in responses. What are the main challenges in implementing a RAG system? Key challenges include efficiently scaling retrieval to large knowledge bases, effectively integrating retrieved information with the generation process, ensuring the accuracy and relevance of retrieved information, maintaining up-to-date knowledge bases, and addressing issues of bias and privacy. Additionally, domain-specific adaptations and handling of complex, multi-step reasoning tasks pose ongoing challenges for RAG implementation. Get In Touch Follow Us Ananya Arora Ananya Arora is a fearless explorer in the realm of data engineering, constantly pushing boundaries and seeking new horizons. Armed with her keyboard and a toolkit of cutting-edge technologies, Ananya fearlessly ventures into uncharted territory, eager to uncover insights hidden within the data. Despite the occasional mishap or data breach, Ananya remains undeterred in her pursuit of innovation, confident that her pioneering approach to data engineering will lead her to success, one breakthrough at a time. Table of Contents Share This Blog. Previous Next 2025-01-03T09:23:44+00:00By Ananya Arora|Categories: Data Engineering| BuzzClan is a leading IT solutions provider, specializing in cloud computing, data management, quality assurance, and digital transformation. Our highly skilled team delivers customized strategies that empower businesses to optimize operations, drive innovation, and achieve sustainable growth. Services Toggle Navigation Cloud Consulting Data Engineering Services Quality Assurance Solutions Digital Transformation Solutions Quick Links Toggle Navigation Career Contact Us Blogs Privacy Policy Applicant Privacy Policy USA Dallas, Texas 4264 Entre Ct, Suite Q Chantilly, VA 20151 Canada 7-7003 Steeles Avenue W., Toronto, ON M9W 0A2 1460 Chevrier Blvd, Suite 200 Winnipeg, MB R3T 1Y7 India Block C, 56/6, Sector 62, Noida, U.P. 201309 E 289, Sector 75, Mohali, Punjab 160055 Kenya P.O BOX 18388-00500, Nairobi © BuzzClan. All rights reserved. Sign Up For Our Newsletter Submit Thank you for subscribing to our Newsletter There was an error trying to send your message. Please try again later. Page load link Go to Top

Source: https://developer.ibm.com/articles/awb-strategies-enhancing-rag-effectiveness
IBM Developer

Source: https://medium.com/@nabilw/leveraging-advanced-rag-techniques-for-knowledge-management-and-ai-development-e442ac74ff8f
Leveraging Advanced RAG Techniques for Knowledge Management and AI Development | by KLYM Research | Medium Open in app Sign up Sign in Write Sign up Sign in Leveraging Advanced RAG Techniques for Knowledge Management and AI Development KLYM Research · Follow 18 min read · May 1, 2024 -- 1 Listen Share Introduction: Harnessing AI for Effective Knowledge Management In today’s digital age, data is ubiquitous, continuously generated through myriad channels including documents, emails, meetings, and more. This deluge of information presents a significant challenge for knowledge management within any organization. Traditional methods of sorting, searching, and retrieving information from such vast and diverse datasets are no longer sufficient. This is where Artificial Intelligence (AI), and more specifically, Retrieval Augmented Generation (RAG) systems, step in as game-changers. Retrieval Augmented Generation represents a cutting-edge approach in AI, leveraging the power of large language models (LLMs) to enhance the retrieval process. By dynamically pulling in relevant information from a vast database, RAG systems can provide more precise, contextually relevant responses. This capability is transformative for businesses looking to streamline their knowledge management processes, ensuring quick access to accurate and relevant information. This blog post aims to delve deep into the world of advanced RAG techniques. We will explore how these systems are not just theoretical constructs but practical tools that can be tuned and optimized for real-world applications. From the basics of setting up a RAG system to exploring sophisticated methods like semantic chunking and dynamic query handling, this post will provide a comprehensive guide to anyone looking to understand or implement RAG in their operations. By the end of this journey, you will have a robust understanding of how RAG systems work, how to implement them, and how to harness their full potential to revolutionize knowledge management within your organization. Let’s embark on this exploration of one of AI’s most promising frontiers. Section 1: Understanding RAG Basics Retrieval Augmented Generation (RAG) is a technique that combines the power of neural networks with traditional information retrieval methods to enhance the capabilities of language models. This hybrid approach is particularly valuable in scenarios where the language model needs to generate responses based on a vast amount of information that can’t be stored within the model itself. What is RAG? At its core, RAG operates by integrating retrieval into the generative process of language models. When a query is presented, the system first retrieves relevant documents or data snippets from a large database. These retrieved contents are then used as a context by the language model to generate an informed and accurate response. This method is especially useful in handling detailed queries that require domain-specific knowledge not inherently possessed by the model. How Does RAG Work? The process can be broken down into several key steps: Query Processing: The input query is processed to understand its intent and to formulate the retrieval query. Document Retrieval: The system searches a database to find relevant documents. This retrieval can be based on various methods, including keyword search, vector search, or a hybrid approach. Context Integration: The retrieved documents are fed into the language model as an extended context. Response Generation: The language model uses both the original query and the retrieved context to generate a coherent and contextually relevant response. Benefits of RAG Enhanced Accuracy: By using external documents to inform its responses, RAG can provide more accurate and detailed answers than a standalone language model. Scalability: It allows models to handle information beyond their training data, making them applicable in more diverse scenarios. Dynamic Learning: RAG systems can adapt to new information and contexts without the need for retraining, as the knowledge base can be updated independently of the model. Common Challenges While RAG offers significant benefits, there are challenges in its implementation: Data Quality and Relevance: The effectiveness of a RAG system is heavily dependent on the quality and relevance of the retrieved data. Latency: Retrieval operations, especially over large datasets, can introduce latency, impacting the response time of the model. Complexity: Setting up a robust RAG system involves complex integration between the retrieval database and the generative model, requiring careful tuning and optimization. RAG in Action Practical examples of RAG applications include AI chatbots that can pull information from a company’s documentation to answer customer queries, or research tools that synthesize information from scientific papers to generate summaries or answer specific questions. Section 2: Data Preparation and Advanced Parsing Techniques To ensure the effectiveness of a Retrieval Augmented Generation (RAG) system, the input data must be meticulously prepared and accurately parsed. This section focuses on the crucial steps and technologies involved in refining data for RAG systems, ensuring that the retrieved information is both relevant and of high quality. Importance of Data Preparation In the context of RAG, data preparation involves the processing of raw data into a format that is suitable for both retrieval and effective use by the language model. Proper data preparation not only enhances the accuracy of the retrieval process but also ensures that the generative model can synthesize responses effectively. Advanced Parsing Techniques Advanced parsing techniques are critical for transforming unstructured or semi-structured data into a cleaner, more structured format. Here are some key techniques and tools: PDF and PowerPoint Parsing: Tools like Llama Parts and Fire Craw are instrumental for extracting clean, usable text from formats typically rich in mixed content such as images and diagrams. Llama Parts, for instance, converts PDF files into a more AI-friendly markdown format, focusing on high accuracy in data extraction. Website Data Parsing: For extracting structured data from websites, tools like Fire Craw convert web pages into clean markdown formats, allowing RAG systems to process online information effectively. This is particularly useful for applications that rely on real-time data from the internet. Handling Multimodal Data: RAG systems often need to handle data that is not purely textual. Advanced parsers are capable of interpreting and structuring data from images, charts, and tables, integrating this information into the searchable text database. Structuring Data for Retrieval Once the data is parsed, it must be structured in a way that facilitates efficient retrieval: Chunking: Dividing large documents into smaller, manageable chunks helps in improving the precision of retrieval. Semantic chunking, where chunks are formed based on the inherent meaning and context, ensures that each piece contains thematically coherent information. Indexing: Proper indexing is crucial for quick retrieval. Using vector embeddings for indexing allows the system to understand the semantic relationships between different chunks, enhancing the relevance of retrieved documents. Metadata Enrichment: Adding metadata to chunks — such as titles, headers, or keywords — helps improve the contextuality of the retrieval process. This metadata acts as additional signals that guide the RAG system in fetching the most relevant content. Challenges in Data Preparation While advanced tools greatly enhance data preparation, challenges remain: Data Quality Variance: The quality of output is directly tied to the input data’s quality. Inconsistent data quality can lead to poor system performance. Complexity in Integration: Seamlessly integrating these tools into existing data pipelines can be technically challenging and resource-intensive. Scalability Issues: As the volume of data increases, maintaining efficient parsing and retrieval without significant latency becomes challenging. Best Practices Regular Updates: Keep the parsing tools and techniques updated with the latest advancements to handle new data formats effectively. Quality Assurance: Regularly test the parsed data for accuracy and relevancy to ensure the RAG system is receiving the best possible input. Scalable Architectures: Design data pipelines that can scale dynamically with the increasing data volume and complexity. By mastering these advanced data preparation and parsing techniques, developers can significantly enhance the performance and reliability of RAG systems. This groundwork is crucial for the advanced retrieval techniques discussed in the next section, where we delve deeper into optimizing the RAG framework for better performance and accuracy. Section 3: Semantic Chunking and Vector Embeddings Semantic chunking is a sophisticated technique that optimizes the segmentation of text to enhance the performance of vector embeddings used in Retrieval Augmented Generation (RAG) systems. This section explores the principles of semantic chunking, its implementation, and its impact on the efficacy of information retrieval. Understanding Semantic Chunking Semantic chunking involves dividing text into segments or “chunks” based on their semantic content rather than arbitrary measures like word count or character limit. This method ensures that each chunk represents a coherent idea or concept, making it more meaningful for vector-based retrieval systems. Key Benefits of Semantic Chunking Improved Retrieval Accuracy: By maintaining thematic integrity within each chunk, semantic chunking enhances the relevance of retrieved documents, leading to more accurate and contextually appropriate responses. Efficient Use of Model Capacity: Semantic chunks optimize the use of the language model’s limited context window (token limit), ensuring that every token the model considers is relevant. Better Alignment with Query Intent: Semantic chunking aligns chunks with potential query intents, increasing the likelihood that the retrieval system will select the most relevant content in response to a user query. Implementing Semantic Chunking The process of semantic chunking typically involves several steps and tools, including: Natural Language Processing (NLP) Techniques: Utilizing NLP tools to analyze the structure and meaning of the text, identifying natural breakpoints based on linguistic cues like topic shifts or narrative changes. Vector Embeddings: Applying machine learning models to convert text chunks into vector form. These vectors capture the semantic essence of the chunks, facilitating their retrieval based on semantic similarity rather than keyword matching. Dynamic Thresholding: Using algorithms to dynamically determine the optimal size of chunks based on the complexity and density of information within the text. Technologies and Tools Tools like OpenAI’s embedding models and custom NLP algorithms are essential for effective semantic chunking. These tools analyze and process text to identify segments that encapsulate complete semantic units, converting them into vector embeddings that are stored in databases such as Pinecone or Elasticsearch. Challenges in Semantic Chunking While powerful, semantic chunking presents several challenges: Complexity in Implementation: Developing an effective semantic chunking system requires advanced NLP knowledge and the ability to tune models for specific types of data. Balancing Chunk Size and Cohesion: Finding the right balance between chunk size and informational cohesion can be challenging, as overly broad or narrow chunks can hinder retrieval effectiveness. Computational Demands: Processing large datasets with advanced NLP models for semantic chunking can be resource-intensive, requiring significant computational power. Practical Applications Semantic chunking is particularly useful in applications such as: Legal and Medical Document Retrieval: Where precision and contextual accuracy are paramount. Customer Support Systems: Enhancing AI-driven chatbots to retrieve and provide exact information in response to specific customer queries. Conclusion Semantic chunking is a critical component in the development of sophisticated RAG systems, enabling more precise and effective information retrieval. By carefully implementing semantic chunking, organizations can significantly enhance the performance of their AI applications, leading to better decision-making and improved user experiences. Section 4: Enhancing RAG with Advanced Retrieval Techniques Retrieval Augmented Generation (RAG) systems benefit significantly from advanced retrieval techniques that enhance their ability to fetch the most relevant information quickly and accurately. This section explores various sophisticated retrieval strategies that can be integrated into RAG systems to improve their performance and scalability. Introduction to Advanced Retrieval Techniques The effectiveness of a RAG system heavily depends on the precision of the retrieval phase. Advanced retrieval techniques aim to optimize this phase by using smarter, more efficient methods to search through large datasets, ensuring that the language model receives the most relevant context for generating responses. Vector Search Vector search uses continuous vector space to represent data and leverages geometric distances to determine relevance. This method is superior to traditional keyword searches as it understands the semantic similarities between the query and the content, allowing for more nuanced and contextually appropriate retrievals. Implementing Vector Search: Tools like Faiss (Facebook AI Similarity Search) or Annoy (Approximate Nearest Neighbors Oh Yeah) are popular for implementing efficient vector search in production environments. Hybrid Search Hybrid search combines vector search with traditional retrieval methods like keyword search or SQL queries. This approach is beneficial for cases where precise keyword matching is crucial, such as retrieving product names in e-commerce or specific terms in legal documents. Benefits: Provides a balance between semantic relevance and keyword specificity, enhancing the accuracy and relevance of retrieved results. Reranking with Machine Learning Once the initial set of documents is retrieved, reranking them using machine learning models can further refine the results. This process involves scoring each document based on its predicted relevance to the query and reordering them accordingly. Techniques: Models like BERT or RoBERTa can be fine-tuned to act as rerankers, providing scores based on the contextual alignment between the query and the document content. Agentic Retrieval Agentic retrieval introduces an agent-based layer where AI agents dynamically decide the best retrieval paths based on the query’s context. This method is particularly useful in complex scenarios where multiple data sources or types must be considered. Application: In customer service, an agent could decide whether to pull information from FAQs, user manuals, or product guides based on the customer’s query complexity and specificity. Challenges in Advanced Retrieval Techniques Complexity: Integrating and tuning advanced retrieval methods can be complex and resource-intensive. Latency: While more precise, these methods can introduce latency, particularly in real-time applications. Data Dependencies: The success of these techniques often depends on the quality and structure of the underlying data. Best Practices for Implementation Continuous Evaluation and Tuning: Regularly assess the performance of retrieval techniques and fine-tune them based on feedback and changing data conditions. Scalability Considerations: Design retrieval systems with scalability in mind, ensuring that they can handle growth in data volume and query traffic. Integration with AI Models: Ensure seamless integration between the retrieval system and the generative models, maintaining a balance between retrieval accuracy and response generation speed. Conclusion Advanced retrieval techniques offer significant potential to enhance RAG systems by providing more accurate and contextually relevant information. By carefully selecting and implementing these methods, organizations can greatly improve the efficiency and effectiveness of their AI-driven applications. Section 5: Implementing Agentic RAG for Dynamic Query Handling Agentic Retrieval Augmented Generation (Agentic RAG) represents an evolution in RAG technology, incorporating intelligent agents that dynamically manage the retrieval process based on the complexity and specificity of queries. This section delves into how agentic RAG can be utilized to enhance dynamic query handling and adapt to diverse informational needs efficiently. What is Agentic RAG? Agentic RAG systems utilize AI agents that act autonomously to optimize the retrieval and response generation processes. These agents assess each query in real-time, decide the best sources for information retrieval, and dynamically adjust their actions based on the context and the evolving requirements of the task. Core Components of Agentic RAG Query Translation and Planning: Agents translate user queries into more effective forms or break them down into sub-queries that are easier to manage and retrieve. This is especially useful for complex queries that may span multiple topics or require data from different sources. Dynamic Retrieval Strategies: Depending on the query’s nature, the agent chooses between different retrieval strategies (e.g., vector search, keyword search, hybrid methods) to optimize accuracy and relevance. Contextual Reranking and Synthesis: After initial retrieval, agents rerank the information based on relevance and synthesize the content to provide concise, informative answers. Implementing Agentic RAG Step 1: Agent Design and Development: Define the capabilities and decision-making processes of the agents, such as their ability to parse queries, select retrieval strategies, and perform reranking. Step 2: Integration with Retrieval Systems: Connect agents with vector databases and other retrieval systems, ensuring they can fetch and process data efficiently. Step 3: Feedback Loops: Implement mechanisms for the agents to learn from past interactions, refining their strategies and improving their performance over time. Practical Examples of Agentic RAG Customer Support: In customer support scenarios, agents can dynamically fetch the most relevant FAQ sections or product details based on the specific issues described by the customers. Research Assistance: For academic or professional research, agents can sift through vast digital libraries to find and synthesize information from multiple papers or reports relevant to a specific research query. Challenges and Considerations Complexity in Agent Training: Training agents to handle a wide range of queries and integrate seamlessly with RAG systems can be complex and resource-intensive. Maintaining Performance: Ensuring the system remains fast and responsive, especially as the number and complexity of queries grow. Adaptability and Scalability: Designing agents that can adapt to new types of data and scale with increasing information loads. Best Practices for Agentic RAG Iterative Development: Start with basic agent functionalities and gradually introduce more complex capabilities as the system matures. User-Centered Design: Consider the end user’s needs and expectations in the agent’s design, focusing on improving user experience and satisfaction. Continuous Monitoring and Optimization: Regularly monitor the system’s performance and user feedback to identify areas for improvement. Conclusion Agentic RAG systems offer a promising advancement in AI-driven information retrieval, allowing for more sophisticated, adaptive, and user-specific responses. By leveraging the power of AI agents, organizations can enhance the efficiency and effectiveness of their knowledge management systems, providing tailored information with greater precision and relevance. Section 6: From Theory to Practice: Building a RAG Application Building a Retrieval Augmented Generation (RAG) application involves a series of steps that blend theoretical concepts with practical implementation. This section provides a detailed guide on how to develop a RAG system, complete with code snippets and explanations for setting up data pipelines, implementing retrieval methods, and integrating with language models. Step 1: Define the Scope and Requirements Before coding, define the specific needs and goals of your RAG application: Target Audience: Who will use this application? Data Sources: What types of documents or data will the system retrieve information from? Performance Metrics: How will you measure the success of your RAG system? Step 2: Data Preparation and Parsing Begin by preparing and parsing your data to ensure it is clean and structured for efficient retrieval:python from llama_parser import LlamaParts # Initialize the parser for PDF documents pdf_parser = LlamaParts()# Parse a sample document structured_data = pdf_parser.parse('sample_report.pdf') This step involves using tools discussed in previous sections to convert raw data into a structured format that can be easily indexed and retrieved. Step 3: Setting Up the Retrieval System Set up a vector database to store and retrieve document embeddings:pythonCopy code from pinecone import Index # Initialize a Pinecone index index = Index('your_index_name')# Index documents for doc_id, doc_content in structured_data.items():     vector = embed(doc_content)     index.upsert(id=doc_id, vector=vector) This involves embedding documents using a pre-trained model and storing these embeddings in a vector database like Pinecone or Elasticsearch. Step 4: Implement the Retrieval Logic Implement the retrieval logic that interacts with the vector database to fetch relevant documents based on a query:pythonCopy code def retrieve_documents(query):     query_vector = embed(query)     results = index.query(vector=query_vector, top_k=5)     return results This function takes a user query, converts it into an embedding, and retrieves the top 5 most semantically similar documents. Step 5: Integrate with a Language Model Use a language model to generate responses based on the retrieved documents:pythonCopy code from transformers import GPT3Model model = GPT3Model.from_pretrained('gpt3-large')def generate_response(retrieved_docs):     context = ' '.join([doc['content'] for doc in retrieved_docs])     prompt = f"Based on the following information: {context}, answer the user's question."     response = model.generate(prompt)     return response This step combines the retrieved content into a coherent context and prompts a large language model to generate an appropriate response. Step 6: User Interface and Feedback Loop Develop a user interface that allows users to interact with the RAG system and provide feedback:pythonCopy code import streamlit as st st.title('RAG System Interface') query = st.text_input('Enter your query:') if st.button('Submit'):     retrieved_docs = retrieve_documents(query)     answer = generate_response(retrieved_docs)     st.write('Answer:', answer) Include functionality for users to provide feedback on the responses, which can be used to fine-tune the system. Conclusion Building a RAG application requires careful integration of multiple components, including data parsing, vector embedding, retrieval, and response generation. By following these steps and utilizing the provided code snippets, developers can create a powerful tool for information retrieval and knowledge management. Section 7: Testing and Optimizing RAG Systems After constructing a Retrieval Augmented Generation (RAG) system, it’s crucial to engage in thorough testing and continuous optimization to ensure the system meets performance standards and effectively serves its intended purpose. This section will guide you through methodologies for evaluating, testing, and refining RAG systems. Testing Methodologies Unit Testing: Begin with unit tests to validate each component of your RAG system — data parsing, indexing, retrieval, and response generation — functions as expected in isolation. import unittest class TestRetrievalMethods(unittest.TestCase):     def test_vector_retrieval(self):         # Test the retrieval logic         query = "What is AI?"         expected_id = "doc123"         results = retrieve_documents(query)         self.assertIn(expected_id, [result['id'] for result in results]) if __name__ == '__main__':     unittest.main() Integration Testing: Assess the integration of different system components to ensure they work together seamlessly to produce the correct outputs. class TestSystemIntegration(unittest.TestCase):     def test_query_to_response(self):         query = "Explain quantum computing."         response = generate_response(retrieve_documents(query))         self.assertTrue(isinstance(response, str) and len(response) > 0) if __name__ == '__main__':     unittest.main() System Testing: Perform end-to-end system tests to evaluate the RAG system’s overall functionality and performance under conditions that mimic real-world operations. Performance Evaluation Response Accuracy: Use metrics like precision, recall, and F1-score to measure how accurately the system retrieves relevant information and generates responses. Latency Measurements: Ensure the response time is within acceptable limits, especially for applications requiring real-time feedback. User Satisfaction: Conduct user surveys and gather qualitative feedback to gauge the system’s effectiveness and user friendliness. Optimization Strategies Fine-Tuning Retrieval Models: Continuously update and fine-tune the retrieval models to improve their understanding and handling of diverse queries. # Example of model fine-tuning from transformers import AdamW  optimizer = AdamW(model.parameters(), lr=1e-5) # Training loop here Enhancing Language Models: Regularly retrain the language models with new data or adjust their parameters to refine response generation quality. Scaling and Load Balancing: Implement scalable infrastructure and load balancing to handle increased traffic and data volume without degradation in performance. Continuous Monitoring and Feedback Set up monitoring systems to track performance metrics in real-time, and establish feedback loops that allow users to report issues or inefficiencies. Use this data to make informed adjustments: # Monitoring dashboard setup example import streamlit as st st.metric(label="Query Response Time", value="200ms") st.metric(label="User Satisfaction Score", value="4.5/5") Conclusion Testing and optimizing a RAG system is an ongoing process that requires attention to detail, responsiveness to user needs, and agility in adapting to new challenges. By applying the strategies outlined above, developers can ensure their RAG systems not only meet initial expectations but also continue to evolve and improve over time. Section 8: Real-world Applications and Case Studies Advanced Retrieval Augmented Generation (RAG) systems have transformative potential across various sectors. This section explores real-world applications and case studies that demonstrate how RAG systems enhance decision-making, improve user interactions, and solve complex information retrieval challenges. Real-world Applications Healthcare: RAG systems are instrumental in managing medical literature and patient data. Doctors use these systems to quickly access patient histories, research treatment options, and stay updated on the latest medical advancements. For instance, a RAG system can retrieve information from medical journals and clinical reports to answer specific queries about drug interactions or treatment protocols. Legal Industry: Law firms implement RAG systems to navigate vast repositories of legal documents. These systems help lawyers find relevant case laws, precedents, and statutes that are critical for building strong cases. The ability to quickly retrieve and synthesize relevant legal information can drastically reduce the time spent on legal research. Customer Support: Companies leverage RAG systems to provide real-time, accurate customer support. By retrieving information from product manuals, FAQs, and customer databases, these systems answer inquiries efficiently, improving customer satisfaction and reducing workload on human agents. Case Studies Case Study 1: IBM Watson in Healthcare Background: IBM Watson Health used a RAG-like system to assist medical professionals in diagnosing and treating patients. Implementation: The system parsed and retrieved information from medical journals and patient records to provide recommendations and treatment options. Outcome: Improved diagnosis accuracy and personalized treatment plans, leading to better patient outcomes and reduced healthcare costs. Case Study 2: Legal Research Firm Background: A leading legal research firm implemented a RAG system to streamline the process of legal document retrieval and case preparation. Implementation: The system used advanced NLP techniques to understand and retrieve relevant legal texts from a database of over a million documents. Outcome: Can reduce research time significantly, allowing lawyers to focus more on case strategy and client interaction. Benefits and Impact Efficiency: RAG systems significantly cut down the time needed to find and utilize information, increasing productivity across various tasks. Accuracy: Enhanced retrieval methods ensure that the information provided is accurate and relevant, reducing errors in decision-making processes. Scalability: RAG systems can handle increasing amounts of data without a proportional increase in expense or resource usage, making them scalable solutions for growing organizations. Conclusion The real-world applications and case studies of RAG systems underscore their value in transforming how organizations handle information. By automating and enhancing the retrieval and utilization of data, RAG systems empower professionals across industries to make better-informed decisions, ultimately leading to improved outcomes and efficiencies. Final Conclusion: Harnessing the Power of RAG Systems Throughout this blog post, we’ve embarked on a comprehensive journey exploring the intricacies of Retrieval Augmented Generation (RAG) systems — from their basic principles and implementation strategies to advanced techniques and real-world applications. As we conclude, it’s important to reflect on the key insights gained and contemplate the future trajectory of RAG technologies. Recap of Key Insights Fundamentals: We started by understanding the basic workings of RAG systems, highlighting their ability to enhance traditional language models by integrating dynamic retrieval capabilities. Data Handling: Effective data preparation and advanced parsing techniques are foundational for optimizing the input for RAG systems, ensuring that the data is accurate, relevant, and structured. Advanced Techniques: Techniques like semantic chunking, vector search, and agentic RAG not only improve the precision of information retrieval but also tailor the responses to be contextually relevant. Practical Applications: The exploration of real-world applications demonstrated RAG’s transformative potential across industries such as healthcare, legal, and customer service, providing tangible benefits like increased efficiency, accuracy, and user satisfaction. Testing and Optimization: Continuous testing and optimization are crucial for maintaining the efficacy of RAG systems, adapting to new data, and refining system components to meet evolving user needs. Final Thoughts RAG systems represent a significant advancement in the way we manage and utilize information. By leveraging AI to enhance data retrieval and response generation, these systems offer a powerful tool for businesses and organizations to improve decision-making and operational efficiency. As we continue to innovate and refine these technologies, the potential for RAG to revolutionize various aspects of professional and everyday life is immense. Artificial Intelligence Meta Agents Large Language Models Rags -- -- 1 Follow Written by KLYM Research 91 Followers ·124 Following "Scaling AI Security to New Heights." "Defend against major Prompt Exploits." "Reverse Engineered AI Defense for your ai/ml Pipeline" https://shorturl.at/qpmtw Follow Responses (1) See all responses Help Status About Careers Press Blog Privacy Terms Text to speech Teams

Source: https://hub.athina.ai/athina-originals/comprehensive-guide-to-different-rag-architectures/
A Comprehensive Guide to RAG Architectures Athina AI Hub Home Blogs Research Papers Athina Originals Trending Write for Us Athina AI IDE Sign in Subscribe athina-originals A Comprehensive Guide to Different RAG Architectures Haziqa Sajid 01 Sep 2024 — 12 min read Photo by Anders Jildén / Unsplash Introduction In an era where data doubles every two years, sifting through mountains of information to find exactly what you need can feel like searching for a needle in an ever-expanding haystack. Yet, this is the challenge that modern artificial intelligence (AI) systems, like Retrieval-Augmented Generation (RAG), are designed to tackle. Search engines might be among the first to face obsolescence from AI, with products like SearchGPT and Perplexity leading the charge. These systems rely on RAG, and the tools to build them are now within anyone's reach. But what sets apart a basic RAG system from an optimized one? The difference often lies in how well it's tailored to the task. This guide will explore the spectrum of RAG architectures, diving deep into the techniques that enhance their performance. Overview of RAG The acronym RAG originates from the 2020 paper, Retrieval-Augmented Generation for Knowledge-Intensive Tasks, published by Facebook AI Research (now Meta AI). This paper characterizes RAG as “a general-purpose fine-tuning recipe” designed to integrate any large language model (LLM) with various internal or external knowledge sources. RAG gives an LLM a superpower: the ability to consult an external knowledge base before crafting its responses. LLM alone, no doubt, is super bright but faces several challenges: Presenting False Information: When the LLM lacks the answer, it might generate incorrect or misleading responses. Providing Out-of-Date Information: LLMs can offer generic or outdated answers, especially when users expect current and specific information. Relying on Non-Authoritative Sources: The model may draw on sources that are not credible or accurate. Confusing Terminology: Different sources might use the same terms differently, leading to misunderstandings or inaccurate responses. RAG helps address these issues by anchoring the LLM's responses in authoritative and current knowledge, ensuring more reliable and relevant interactions. So, imagine your LLM as a brilliant but forgetful scholar—by tapping into up-to-date sources, it can offer accurate, contextually relevant answers without needing a complete retrain. The core components of the RAG pipeline are: Retrieval Pipeline: Retrieves relevant documents or data from an external knowledge base. Generation Pipeline: Uses the retrieved information to generate contextually accurate and relevant responses. Fig 1: A fundamental architecture of Retrieval-Augmented Generation (RAG) However, basic RAG has significant room for improvement, which will be explored in the next section, and potential solutions. Types of RAG Architectures Before jumping straight to the types of architecture of RAG, let’s discuss the challenges that a primary RAG faces: Basic RAG often struggles with queries that span multiple domains or topics, leading to mismatches in retrieved documents and reduced accuracy. The reliance on vector similarity alone can cause irrelevant or suboptimal document retrieval, especially for nuanced queries. Basic RAG systems may have higher latency due to inefficient retrieval and processing steps. The broad context windows in basic RAG can overwhelm the LLM with irrelevant information, reducing the quality of the generated response. The standard RAG approach may not effectively break down or handle multi-faceted queries. The lack of advanced filtering and indexing can cause higher processing costs, especially when handling large datasets or frequent queries. The architectures presented ahead solves such problems. Let’s look at them. Query Enhancement Basic RAG does reflect the true self of the user query, and it might lead to incompetent results. Query enhancement adjusts and refines the RAG input query process to capture and convey the query's intent accurately. Let’s see some techniques: 1. Hypothetical Questions This approach uses an LLM to generate potential user questions about the content of each document chunk. Before the user's query reaches the LLM, the vector store retrieves the most relevant hypothetical questions related to the actual query and their corresponding document chunks and forwards them to the LLM. This method addresses the cross-domain asymmetry (will be discussed later) issue in vector search. It enables query-to-query matching, thereby reducing the reliance on vector searches. Workflow for Generating Hypothetical Questions Generate Hypothetical Questions: Use the LLM to create a set of hypothetical questions based on the content of the document chunks. Store in Vector Store: Save the generated hypothetical questions along with their corresponding document chunks in the vector store. Retrieve Relevant Data: Query the vector store to find the most relevant hypothetical questions and document chunks based on the user query. Generate Final Response: Send the retrieved hypothetical questions and document chunks to the LLM to generate the final response to the user query. 2. HyDE The acronym HyDE stands for Hypothetical Document Embeddings. Instead of contextual information, this approach uses LLM to create “Hypothetical Documents” or simulated responses to a user query. The answer is then translated into vector embeddings, which are utilized to find the most suitable parts of a document in a vector database. Fig 2: An illustration of the HyDE model. The vector database returns the most relevant K chunks and sends them back to LLM with user query text to generate the final answer. However, this method increases computational costs and uncertainties when generating hypothetical answers. Workflow for Generating Hypothetical Document Generate Hypothetical Document: Create a simulated document or fake answer using the LLM based on the user query. Convert to Embeddings: Transform the hypothetical document into vector embeddings. Retrieve Relevant Chunks: Use the vector embeddings to search the vector database and retrieve the top-K most relevant document chunks. Generate Final Response: To generate the final response, provide the retrieved document chunks and the original query to the LLM. This improves the query process because vector databases, serving as knowledge hubs, match documents more effectively than questions. 3. Sub-Queries When tackling a complex user query, an LLM can break it into bite-sized, manageable sub-queries. Imagine a user asking, “What are the key differences between AWS and Azure?” Instead of answering this all at once, the LLM might split it into more straightforward questions like, “What features does AWS offer?” and “What features does Azure provide?” These sub-queries are then turned into vector embeddings and sent to the vector database. The database retrieves the top-K most relevant document chunks for each subquery. With this tailored information, the LLM crafts a detailed and precise answer. Workflow for Sub-Queries Break Down Query: The LLM decomposes the complex user query into simpler sub-queries. Convert to Embeddings: Transform each subquery into vector embeddings. Retrieve Relevant Chunks: Search the vector database for each sub-query's Top-K most relevant document chunks. Generate Final Response: To generate the final response, provide the retrieved document chunks and the original query to the LLM. Indexing Enhancement Indexing enhancement involves optimizing how data is organized and retrieved to boost the performance of RAG. Here are some top techniques that can improve it: 1. Merging Document Chunks Automatically This technique involves organizing document data into two levels: child chunks (detailed segments) and their corresponding parent chunks (broader summaries). Initially, the system searches for detailed child chunks. When a sufficient number of child chunks from a set belong to the same parent chunk, the parent chunk is used to provide contextual information to the LLM. Imagine your document library as a well-organized bookshelf with detailed notes and overarching summaries. To make searching more efficient, locate detailed notes (child chunks) and merge related notes into a broader summary (parent chunks). This way, when you search, you find specific notes and get the broader context from associated summaries. Workflow for Merging Document Chunks Automatically Index Chunks: Index child chunks (detailed segments) and parent chunks (broader summaries). Retrieve Child Chunks: Search and retrieve relevant child chunks based on the user query. Merge Chunks: Consolidate child chunks into their corresponding parent chunks if a specified threshold is met. Generate Final Response: Provide the merged parent chunks as contextual information to the LLM to generate the final response. 2. Constructing Hierarchical Indices This method involves creating a two-tier indexing system: one for document summaries and another for individual document chunks. The retrieval process consists of two stages: filtering relevant documents based on their summaries and then retrieving the document chunks within those selected documents. Workflow for Constructing Hierarchical Indices Create Indices: Establish two-level indices: one for document summaries and another for document chunks. Retrieve Relevant Documents: Search for and obtain relevant documents based on the summaries. Retrieve Document Chunks: Extract the document chunks from the relevant documents. Generate Final Response: Use the retrieved document chunks to generate the final response with the LLM. 3. Hybrid Retrieval and Reranking The Hybrid Retrieval and Reranking technique combines multiple retrieval methods to enhance search performance. Initially, vector similarity retrieval is used alongside supplementary retrieval methods, such as linguistic frequency-based approaches or sparse embedding models. After obtaining initial results, a reranking process prioritizes them based on their relevance to the user query. Fig 3: A basic hybrid retrieval and reranking approach Workflow for Hybrid Retrieval and Reranking Perform Initial Retrieval: Obtain initial results using vector similarity methods (e.g., using embeddings to find similar documents). Obtain initial results using traditional methods like BM25 or frequency-based approaches. Combine Results: Merge the results from vector similarity and lexical retrieval methods into a unified set. Rerank Results: Apply a reranking algorithm (e.g., Reciprocal Rank Fusion (RRF) or Cross-Encoder) to reorder the combined results based on their relevance to the user query. Generate Final Response: Use the LLM to generate the final response based on the reranked results and the original query. Retriever and Generator Enhancements Regarding RAG's two main components, the Retriever and Generator, if your RAG isn't performing well despite query and index improvements, consider optimizing these components. Here are some architectural options for their enhancement; let’s start with retriever enhancement: 1. Retriever: Sentence Window Retrieval In a traditional RAG system, the document chunk sent to the LLM is like a giant, panoramic view around a specific piece of information. This broader window ensures the LLM gets a comprehensive context but can sometimes overwhelm it with excess detail. The Sentence Window Retrieval technique introduces a more focused approach: it treats the document chunk used for embedding retrieval and the chunk provided to the LLM as distinct entities. Think of it like having a detailed map (the embedding chunk) and a zoomed-in view (the context window) of the area you're interested in. Workflow for Sentence Window Retrieval Retrieve Embedding Chunk: Extract a chunk of the document based on embedding similarity. Determine Context Window: Define a context window size for additional information around the retrieved chunk. Retrieve Expanded Window: Obtain the extended context window from the document. Provide Context to LLM: Pass the extended context window and the retrieved chunk to the LLM for processing. Adjust Window Size: Fine-tune the window size based on specific business needs to balance context and relevance. This method is advantageous when you need to balance the richness of information with the clarity of focus, ensuring that the LLM is guided by relevant context without being overwhelmed. 2. Retriever: Metadata Filtering Meta-data Filtering involves refining the retrieved documents by applying filters based on metadata attributes, such as time, category, or other relevant criteria. Workflow for Metadata Filtering Retrieve Initial Documents: Obtain documents based on the initial retrieval process. Apply Metadata Filters: Filter the retrieved documents using metadata attributes (e.g., date range, category). So, when inserting them, add these metadata to the filter. Refine Document Set: Narrow down the document set to match the specific criteria of the query. Provide Filtered Documents to LLM: Pass the filtered documents to the LLM to generate a response. By filtering metadata, not only can retrieval be enhanced, but costs can also be reduced by significantly decreasing the number of documents that need to be processed by the LLM. The following section will focus on techniques to manage and minimize costs effectively. 3. Generator: Compressing Retrieved document chunks often contain noise and irrelevant details that can hinder the accuracy of the LLM's responses. Additionally, the limited prompt window of LLMs constrains the amount of context that can be processed. To address this, compressing the prompt involves filtering out irrelevant information, highlighting key paragraphs, and reducing the overall length of the context provided to the LLM. Workflow for Compressing Identify Key Information: Analyze retrieved document chunks to identify and extract essential details and critical paragraphs. Filter Out Noise: Remove or compress irrelevant or redundant information that does not contribute to the final answer. Adjust Context Length: Trim the overall length of the context to fit within the LLM's prompt window constraints, ensuring only the most pertinent information is included. Format and Provide Prompt: Prepare the compressed context as the LLM prompt, focusing on clarity and relevance. Generate Response: Use the refined prompt to generate a more accurate and focused response from the LLM. 4. Generator: Adjusting Chunk Order LLMs tend to give more weight to information at the beginning and end of documents, often neglecting content in the middle. To address this, we can strategically rearrange the order of retrieved document chunks. By placing chunks with higher confidence or relevance at the start and end of the prompt and positioning those with lower confidence in the middle, we improve the LLM's focus on crucial information. Fig 4: The Significance of Document Placement: Insights from the Research Paper "Lost in the Middle" Workflow for Adjusting Chunk Order: Retrieve Document Chunks: Obtain multiple document chunks based on the query. Assess Chunk Confidence: Evaluate the confidence level or relevance of each chunk. Reorder Chunks: Arrange the chunks so those with higher confidence are positioned at the beginning and end of the prompt, with lower-confidence chunks in the middle. Prepare the Prompt: Assemble the reordered chunks into the LLM prompt, ensuring a logical and practical presentation of information. Generate Response: Use the reordered prompt to generate a response, aiming for improved accuracy and relevance based on the strategically adjusted chunk order. Now that we have explored the various architectures of RAG, it's essential to consider the best practices for selecting the most suitable RAG for your specific requirements. If your generator pipeline is not performing up to the mark, it seems like this article is for you. Best Practices for Choosing the Right RAG Architecture The architecture of RAG can be tailored based on the query type. The following table provides an overview of how the query type influences the choice of RAG components and the rationale behind each selection: Query Type Recommended RAG Technique Why This Technique Fits Broad, Cross-Domain Query Hypothetical Questions Effective in bridging gaps between diverse domains by generating relevant hypothetical questions that span across different topics. Complex, Multi-Faceted Query HyDE (Hypothetical Document Embeddings) Simulated documents provide a comprehensive view, enhancing accuracy when dealing with complex comparisons or analyses. Detail-Oriented Query Needing Specific Context Sentence Window Retrieval Offers precise context control, ensuring that relevant details are highlighted without overwhelming the LLM with too much information. Large Dataset Query Hierarchical Indexing Efficiently filters through large datasets by first narrowing down based on summaries, making it ideal for extensive searches. Query Needing Top Results from Multiple Angles Hybrid Retrieval and Reranking Combines different retrieval methods to bring in diverse perspectives, then reranks for the most relevant results. Time-Specific or Categorical Query Metadata Filtering Narrowing down results based on metadata makes focusing on specific time periods, categories, or other filters easier. Query Requiring Summarization Compressing the LLM Prompt Reduces noise and emphasizes critical points, making it ideal for summarizing large amounts of data into concise answers. Complex Query with Important Points at the Beginning and End Adjusting Chunk Order Strategically reorders content to ensure the LLM focuses on the most critical information, particularly useful when the middle content is less relevant. Further decisions on RAG may also depend on cost and accuracy. Enhancements like hybrid retrieval and reranking improve precision but may increase the computational load. Similarly, compressing the LLM prompt reduces context size and computational cost while reducing the accuracy. These practices serve as a reference but ultimately revolve around the specific use case and the problem. It’s always nice to strike a balance between the metrics that you care about: accuracy, cost, and performance. Common Problems Solved by Enhanced RAG Architectures The advanced RAG techniques not only improve retrieval accuracy, but these architectures also solve common challenges, including: Addressing Cross-Domain Asymmetry: Cross-domain asymmetry refers to the challenge that arises when there is a mismatch between the domains or contexts of the query and the information stored in the database or knowledge base. Techniques like Hypothetical Questions and HyDE (Hypothetical Document Embeddings) tackle this by generating simulated queries or documents that align more closely with the content, reducing the reliance on direct vector matching. Reducing Latency: Latency can be a massive setback for a RAG app user. It can get worse in scenarios where retrieval is from massive datasets. Techniques, such as Metadata filtering and Sentence Window Retrieval, can significantly reduce the number of documents relevant to the LLM context. Improving Accuracy: Engineers have more control over the retrieval component when dealing with RAG. The better the retrieval from the databases, the better the generation. Techniques, such as Hybrid Retrieval and Reranking, optimize retrieval by combining multiple methods and reranking results based on relevance. Handling Complex Queries: Complex queries and large datasets pose challenges in managing context and accuracy. Sentence Window Retrieval improves focus by providing a more targeted context around relevant chunks. Hierarchical Indexing organizes data into manageable layers, enhancing retrieval efficiency. Improving Cost: Metadata Filtering reduces the number of documents processed by applying metadata-based filters, lowering computational costs. Adjusting Chunk Order and Compressing the LLM Prompt also contribute to cost savings by ensuring that only the most relevant information is processed, reducing the computational resources required. Conclusion The importance of Retrieval-Augmented Generation (RAG) cannot be overstated in modern information retrieval tasks. With tools available, anyone can create a RAG system. Despite its potential, a basic RAG system is frequently hindered by several challenges. These include inaccurate query matching, handling large datasets, and managing computational costs. This article explores architectures for RAG that offer improved performance and lower costs. HyDE, Metadata Filtering, and Sentence Window Retrieval address cross-domain asymmetry and reduce computational costs. Hybrid Retrieval improves accuracy, and Hierarchical Indexing handles complex queries in large datasets. These advanced RAGs are impressive but are always restricted to the use case of the problem you are solving. Collaborating with users on such problems is suggested. This is where Athina AI comes in. Athina AI is a platform for AI teams to prototype, experiment, and monitor LLM-powered applications. Your product becomes fully performant when you act on the user's feedback, which requires the observability that Athina provides. Sign up today to see what’s going on with your AI applications. Read more Top 10 RAG Papers from January 2025 Retrieval-Augmented Generation (RAG) is evolving rapidly, becoming more efficient, accurate, and adaptable—and the latest research is setting the stage for its future advancements. We’ve selected 10 of the most relevant papers from a total of 110 RAG papers released in January on Arxiv, that explore innovative RAG frameworks, By Paras Madan 04 Feb 2025 Building Adaptive RAG using LangChain and LangGraph The rise of Large Language Models (LLMs) has significantly improved AI-driven question-answering (QA) systems. However, traditional LLMs rely solely on their pre-trained data, which can lead to outdated or incorrect information. To address this limitation, Retrieval-Augmented Generation (RAG) is used. RAG integrates external knowledge sources, enhancing both accuracy and reliability. By Prasad Mahamulkar 04 Feb 2025 Implementing RAG using Hypothetical Document Embeddings (HyDE) Retrieval-augmented generation (RAG) is a powerful technique in information retrieval where an external knowledge base is leveraged to improve the quality and accuracy of responses. One of the challenges in traditional RAGl systems is the dependence on explicit query-document similarity, which often requires labeled relevance data. Hypothetical Document Embeddings (HyDE) By Prasad Mahamulkar 03 Feb 2025 Top 10 AI Agent Papers from January 2025 shaping their future AI agents are getting smarter, faster, and more independent—and the research coming out now is laying the groundwork for what’s next. We’ve selected 10 most relevant papers out of total 309 Agent papers released in January on Arxiv that tackle key challenges like governance, collaboration, reasoning, and By Paras Madan 01 Feb 2025 Athina AI Hub Sign up GitHub LinkedIn X YouTube Powered by Ghost Athina AI Hub The ultimate resource designed for AI development teams 🔥 Subscribe Built with ❤️ by Athina AI Product Observe Develop Evaluate Pricing Resources Athina AI Hub Athina AI Documentation Company Blog Privacy Policy AI Hub Sections AI Development Blogs AI Research Papers Athina AI Originals Top Performers About About Athina AI About Athina AI Hub Write for the AI Hub

Source: https://techcommunity.microsoft.com/blog/educatordeveloperblog/enhancing-retrieval-augmented-generation-with-a-multimodal-knowledge-extraction-/4241375
Multimodal Knowledge Extraction and Retrieval System for Generative AI Agents and RAG Systems Skip to content Tech CommunityCommunity Hubs Products Topics BlogsEvents Microsoft Learn Lounge RegisterSign In Microsoft Community Hub CommunitiesTopics Education Sector Educator Developer Blog Connect with experts and redefine what’s possible at work – join us at the Microsoft 365 Community Conference May 6-8. Learn more > Blog Post Educator Developer Blog 6 MIN READ Enhancing Retrieval-Augmented Generation with a Multimodal Knowledge Extraction and Retrieval System yulitrub Copper Contributor Sep 16, 2024 The rapid evolution of AI has led to powerful tools for knowledge retrieval and question-answering systems, particularly with the rise of Retrieval-Augmented Generation (RAG) systems. This blog post introduces my capstone project, created as part of the IXN program at UCL in collaboration with Microsoft, aimed at enhancing RAG systems by integrating multimodal knowledge extraction and retrieval capabilities. The system enables AI agents to process both textual and visual data, offering more accurate and contextually relevant responses. In this post, I’ll walk you through the project’s goals, development journey, technical implementation, and outcomes. Project Overview The main goal of this project was to improve the performance of RAG systems by refining how multimodal data is extracted, stored, and retrieved. Current RAG systems primarily rely on text-based data, which limits their ability to generate accurate responses when queries require a combination of text and images. To address this, I developed a system capable of extracting, processing, and retrieving multimodal data from Wikimedia, allowing AI agents to generate more accurate, grounded and contextually relevant answers. Key features include: Multimodal Knowledge Extraction: Data from Wikimedia (text, images, tables) is preprocessed, run through the transformation pipeline, and stored in vector and graph databases for efficient retrieval. Dynamic Knowledge Retrieval: A custom query engine, combined with an agentic approach using the ReAct agent, ensures flexible and accurate retrieval of information by dynamically selecting the best tools and strategies for each query. The project began by addressing the limitations of existing RAG systems, particularly their difficulties with handling visual data and delivering accurate responses. After reviewing various technologies, a system architecture was developed to support both text and image data. Throughout the process, components were refined to ensure compatibility between LlamaIndex, Qdrant, and Neo4j, while optimising performance for managing large datasets. The primary challenges lay in handling the large volumes of data from Wikimedia, especially the processing of images, and refactoring the system for Dockerisation. These challenges were met through iterative improvements to the system architecture, ensuring efficient multimodal data handling and reliable deployment across environments. Implementation Overview This project integrates both textual and visual data to enhance RAG systems' retrieval and response generation. The system’s architecture is split into two main processes: Knowledge Extraction: Data is fetched from Wikimedia and transformed into embeddings for text and images. These embeddings are stored in Qdrant for efficient retrieval, while Neo4j captures the relationships between the nodes, ensuring the preservation of data structure. Knowledge Retrieval: A dynamic query engine processes user queries, retrieving data from both Qdrant (using vector search) and Neo4j (via graph traversal). Advanced techniques like query expansion, reranking, and cross-referencing ensure the most relevant information is returned. System Architecture Diagram Tech Stack The following technologies were used to build and deploy the system: Python: Core programming language for data pipelines LlamaIndex: Framework for indexing, transforming, and retrieving multimodal data Qdrant: Vector database for similarity searches based on embeddings Neo4j: Graph database used to store and manage relationships between data entities Azure OpenAI (GPT-4O): Used for handling multimodal inputs, deploying models via Azure App Services Text Embedding Ada-002: Model for generating text embeddings Azure Computer Vision: Used for generating image embeddings Gradio: Provides an interactive interface for querying the system Docker and Docker Compose: Used for containerization and orchestration, ensuring consistent deployment Implementation Details Multimodal Knowledge Extraction The system starts by fetching both textual and visual data from Wikimedia, using the Wikimedia API and web scraping techniques. Then the key steps in knowledge extraction implementation are: Data Preprocessing: Text is cleaned, images are classified into categories such as plots or images for appropriate handling during later transformations, and tables are structured for easier processing. Node Creation and Transformation: Initial LlamaIndex nodes are created from this data, which then undergo several transformations through the transformation pipeline using GPT-4O model deployed via Azure OpenAI: Text and Table Transformations: Text data is cleaned, split into smaller chunks using semantic chunking, and new derived nodes are created from various transformations, like key entity extraction or table analysis. Each node has a unique Llamaindex ID and carries metadata such as title, context, and relationships reflecting the hierarchical structure of the Wikimedia page and parent-child relationships with new transformed nodes. Image Transformations: Images are processed to generate descriptions, perform plot analysis, and identify key objects based on the image type, resulting in the creation of new text nodes. Embeddings Generation: The last stage of the pipeline is to generate embeddings for images and transformed text nodes: Text Embeddings: Generated using the text-embedding-ada-002 model deployed with Azure OpenAI on Azure App Services. Image Embeddings: Generated using the Azure Computer Vision service. Storage: Both text and image embeddings are stored in Qdrant with reference node IDs in the payload for fast retrieval. The full nodes and their relationships are stored in Neo4j: Neo4j graphs (left) and close-up section of the graph (right) Knowledge Retrieval The retrieval process involves several key steps: Query Expansion: The system generates multiple variations of the original query, expanding the search space to capture relevant data. Vector Search: The expanded queries are passed to Qdrant for a similarity-based search using cosine similarity. Reranking and Cross-Retrieval: Results are then reranked by relevance. Retrieved nodes from Qdrant contain LlamaIndex node IDs in the payload. These are used to fetch the nodes from Neo4j and then to get the nodes with original data from Wikimedia by traversing the graph, ensuring the final response is based only on original Wikipedia content. ReAct Agent Integration: The ReAct agent dynamically manages the retrieval process by selecting tools based on the query context. It integrates with the custom-built query engine to balance AI-generated insights with the original data from Neo4j and Qdrant. Dockerization with Docker Compose To ensure consistent deployment across different environments, the entire application is containerised using Docker. Docker Compose orchestrates multiple containers, including the knowledge extractor, retriever, Neo4j, and Qdrant services. This setup simplifies the deployment process and enhances scalability. Docker Containers Results and Outcomes The system effectively enhances the grounding and accuracy of responses generated by RAG systems. By incorporating multimodal data, it delivers contextually relevant answers, particularly in scenarios where visual information was critical. The integration of Qdrant and Neo4j proved to be highly efficient, enabling fast retrieval and accurate results. Additionally, a user-friendly interface built with Gradio allows users to interact with the system and compare the AI-generated responses with standard LLM output, offering an easy way to evaluate the improvements. Here is a snapshot of the Gradio UI:  Future Development Several directions for future development have been identified to further enhance the system’s capabilities: Agentic Framework Expansion: A future version of the system could incorporate an autonomous tool capable of determining whether the existing knowledge base is sufficient for a query. If the knowledge base is found lacking, the system could automatically initiate a knowledge extraction process to address the gap. This enhancement would bring greater adaptability and self-sufficiency to the system. Knowledge Graph with Entities: Expanding the knowledge graph to include key entities such as individuals, locations, and events or others appropriate for the domain. This would add considerable depth and precision to the retrieval process. The integration of such entities would provide a more comprehensive and interconnected knowledge base, improving both the relevance and accuracy of results. Enhanced Multimodality: Future iterations could expand the system’s capabilities in handling image data. This may include adding support for image comparison, object detection, or breaking images down into distinct components. Such features would enable more sophisticated queries and increase the system’s versatility in handling diverse data formats. Incorporating these advancements will position the system to play an important role in the evolving field of multimodal AI, further bridging the gap between text and visual data integration in knowledge retrieval. Summary This project demonstrates the potential of enhancing RAG systems by integrating multimodal data, allowing AI to process both text and images more effectively. Through the use of technologies like LlamaIndex, Qdrant, and Neo4j, the system delivers more grounded, contextually relevant answers at high speed. With a focus on accurate knowledge retrieval and dynamic query handling, the project showcases a significant advancement in AI-driven question-answering systems. For more insights and to explore the project, please visit the GitHub repository. If you’d like to connect, feel free to reach out to me on LinkedIn. Updated Sep 11, 2024 Version 1.0 ai llamaindex Multimodal Neo4j Qdrant rag Comment Comment yulitrub Copper Contributor Joined July 25, 2024 Send Message View Profile Educator Developer Blog Follow this blog board to get notified when there's new activity Share What's new Surface Pro 9 Surface Laptop 5 Surface Studio 2+ Surface Laptop Go 2 Surface Laptop Studio Surface Duo 2 Microsoft 365 Windows 11 apps Microsoft Store Account profile Download Center Microsoft Store support Returns Order tracking Virtual workshops and training Microsoft Store Promise Flexible Payments Education Microsoft in education Devices for education Microsoft Teams for Education Microsoft 365 Education Education consultation appointment Educator training and development Deals for students and parents Azure for students Business Microsoft Cloud Microsoft Security Dynamics 365 Microsoft 365 Microsoft Power Platform Microsoft Teams Microsoft Industry Small Business Developer & IT Azure Developer Center Documentation Microsoft Learn Microsoft Tech Community Azure Marketplace AppSource Visual Studio Company Careers About Microsoft Company news Privacy at Microsoft Investors Diversity and inclusion Accessibility Sustainability California Consumer Privacy Act (CCPA) Opt-Out Icon Your Privacy Choices Sitemap Contact Microsoft Privacy Manage cookies Terms of use Trademarks Safety & eco About our ads © Microsoft 2024

Source: https://www.microsoft.com/en-us/microsoft-cloud/blog/2025/02/04/common-retrieval-augmented-generation-rag-techniques-explained/
Common retrieval augmented generation (RAG) techniques explained | The Microsoft Cloud Blog Skip to main content Microsoft The Microsoft Cloud The Microsoft Cloud The Microsoft Cloud Home What is the Microsoft Cloud Solutions By use case By use case Business solutions hub Collaboration Customer experience Data & analytics Governance Industry Infrastructure Modernization Operations Security Tech & development By job function By job function Commerce Contact center Data & analytics Finance Frontline workers Sales & marketing Security Service Supply chain By industry By industry Energy Financial services Government Healthcare Manufacturing Media & entertainment Mobility Nonprofit Retail Sustainability Telecommunications See all industries Products Artificial intelligence Azure Copilot Dynamics 365 Fabric Github Industry Clouds Microsoft 365 Power Platform Security Sustainability Visual Studio Resources Learn Learn Documentation Secure Future Initiative Microsoft Trust Center Training Connect Connect Community Blogs Events Find a partner Become a partner More All Microsoft Global Microsoft Security Azure Dynamics 365 Microsoft 365 Microsoft Teams Windows 365 Tech & innovation Tech & innovation Microsoft Cloud AI Azure Space Mixed reality Microsoft HoloLens Microsoft Viva Quantum computing Sustainability Industries Industries Education Automotive Financial services Government Healthcare Manufacturing Retail All industries Partners Partners Find a partner Become a partner Partner Network Azure Marketplace AppSource Resources Resources Blog Microsoft Advertising Developer Center Documentation Events Licensing Microsoft Learn Microsoft Research View Sitemap Home / AI / Common retrieval augmented generation (RAG) techniques explained Thought leadership AI 6 minutes February 4, 2025 Common retrieval augmented generation (RAG) techniques explained By Olivia Shone, Senior Director, Product Marketing Share Tags AI transformation Generative AI Organizations use retrieval augmented generation (or RAG) to incorporate current, domain-specific data into language model-based applications without extensive fine-tuning. AI business resources Expert insights and guidance from a curated set of AI business resources Get the resources This article outlines and defines various practices used across the RAG pipeline—full-text search, vector search, chunking, hybrid search, query rewriting, and re-ranking. What is full-text search? Full-text search is the process of searching the entire document or dataset, rather than just indexing and searching specific fields or metadata. This type of search is typically used to retrieve the most relevant chunks of text from the underlying dataset or knowledge base. These retrieved chunks are then used to augment the input to the language model, providing context and information to improve the quality of the generated response. Full-text search is often combined with other search techniques, such as vector search or hybrid search, to leverage the strengths of multiple approaches. The purpose of full-text search is to: Allow the retrieval of relevant data from the complete textual content of a document or dataset. Enable the identification of documents that may contain the answer or relevant information, even if the specific query terms are not present in the metadata or document titles. The process of implementing a full-text search involves the following techniques: Indexing—the full text of documents or dataset is indexed, often using inverted index structures that store and organize information that helps improve the speed and efficiency of search queries and retrieved results. Querying—when a user query is received, the full text of the documents or dataset is searched to find the most relevant information. Ranking—the retrieved documents or chunks are ranked based on relevance to the query, using techniques like term frequency inverse document frequency (TF-IDF) or BM25. What is vector search? Vector search retrieves stored matching information based on conceptual similarity, or the underlying meaning of sentences, rather than exact keyword matches. In vector search, machine learning models generate numeric representations of data, including text and images. Because the content is numeric rather than plain text, matching is based on vectors that are most similar to the query vector, enabling search matching for: Semantic or conceptual likeness (“dog” and “canine,” conceptually similar yet linguistically distinct). Multilingual content (“dog” in English and “hund” in German). Multiple content types (“dog” in plain text and a photograph of a dog in an image file). With the rise of generative AI applications, vector search and vector databases have seen a dramatic rise in adoption, along with the increased number of applications using dialogue interactions and question/answer formats. Embeddings are a specific type of vector representation created by natural language machine learning models trained to identify patterns and relationships between words. There are three steps in processing vector search: Encoding—use language models to transform or convert text chunks into high-dimensional vectors or embeddings. Indexing—store these vectors in a specialized database optimized for vector operations. Querying—convert user queries into vectors using the same encoding method to retrieve semantically similar content. Things to consider when implementing vector search: Selecting the right embedding model for your specific use case, like GPT or BERT. Balancing index size, search speed, and accuracy. Keeping vector representations up to date as the source data changes. What is chunking? Chunking is the process of dividing large documents and text files into smaller parts to stay under the maximum token input limits for embedding models. Partitioning your content into chunks ensures that your data can be processed by the embedding models and that you don’t lose information due to truncation. For example, the maximum length of input text for the Azure OpenAI Service text-embedding-ada-002 model is 8,191 tokens. Given that each token is around four characters of text for common OpenAI models, this maximum limit is equivalent to around 6,000 words of text. If you’re using these models to generate embeddings, it’s critical that the input text stays below the limit. Documents are divided into smaller segments, depending on: Number of tokens or characters. Structure-aware segments, like paragraphs and sections. Overlapping windows of text. When implementing chunking, it’s important to consider these factors: Shape and density of your documents. If you need intact text or passages, larger chunks and variable chunking that preserves sentence structure can produce better results. User queries. Larger chunks and overlapping strategies help preserve context and semantic richness for queries that target specific information. Large language models (LLMs) have performance guidelines for chunk size. You need to set a chunk size that works best for all of the models you’re using. For instance, if you use models for summarization and embeddings, choose an optimal chunk size that works for both. Explore common chunking techniques What is hybrid search? Hybrid search combines keyword search and vector search results and fuses them together using a scoring algorithm. A common model is reciprocal rank fusion (RRF). When two or more queries are executed in parallel, RRF evaluates the search scores to produce a unified result set. For generative AI applications and scenarios, hybrid search often refers to the ability to search both full text and vector data. The process of hybrid search involves: Transforming the query into a vector format. Performing vector search to find semantically similar chunks. Simultaneously conducting keyword search on the same corpus. Combining and ranking results from both methods. When implementing hybrid search, consider the following: Balancing the influence of each search method. Increased computational complexity compared to single-method search. Tuning the system to work well across diverse types of queries and content. Overlapping keywords to match when using question and answering systems, like ChatGPT. Microsoft AI in action Explore how Microsoft AI can transform your organization See the business impact What is query rewriting? Query rewriting is an important technique used in RAG to enhance the quality and relevance of the information retrieved by modifying and augmenting a provided user query. Query rewriting creates variations of the same query that are shared with the retriever simultaneously, alongside the original query. This helps remediate poorly phrased questions and casts a broader net for the type of knowledge collected for a single query. In RAG systems, rewriting helps improve recall, better capturing user intent. It’s performed during pre-retrieval, before the information retrieval step in a RAG scenario. Query rewriting can be approached in three ways: Rules-based—using predefined rules and patterns to modify the query. Machine learning-based—training models to learn how to transform queries based on examples. Mixed—combining rules-based and machine learning-based techniques. What is re-ranking? Re-ranking, or L2 ranking, uses the context or semantic meaning of a query to compute a new relevance score over pre-ranked results. Post retrieval, a retrieval system passes search results to a ranking machine-learning model that scores the documents (or textual chunks) by relevance. Then, the top results of a limited, defined number of documents (top 50, top 10, top 3) are shared with the LLM. Learn how to start building a RAG application RAG systems employ various techniques to enhance knowledge retrieval and improve the quality of generated responses. These techniques work to provide language models with highly relevant context to generate accurate and informative responses. To get started, use the following resources to start building a RAG application with Azure AI Foundry and use them with agents built using Microsoft Copilot Studio. Build a RAG application with Azure OpenAI Service and Azure AI Search. Watch our videos: Retrieval augmented generation with Azure AI Search OpenAI creates retrieval augmented generation features with Azure AI Search Read about retrieval augmented generation (RAG) in Azure AI Search. Read about enhancing responses from agents built using Copilot Studio with your index built in Azure AI Search. Our commitment to Trustworthy AI Organizations across industries are leveraging Azure AI Foundry and Microsoft Copilot Studio capabilities to drive growth, increase productivity, and create value-added experiences. We’re committed to helping organizations use and build AI that is trustworthy, meaning it is secure, private, and safe. We bring best practices and learnings from decades of researching and building AI products at scale to provide industry-leading commitments and capabilities that span our three pillars of security, privacy, and safety. Trustworthy AI is only possible when you combine our commitments, such as our Secure Future Initiative and our Responsible AI principles, with our product capabilities to unlock AI transformation with confidence. Azure remains steadfast in its commitment to Trustworthy AI, with security, privacy, and safety as priorities. Check out the 2024 Responsible AI Transparency Report. Related Posts AI Jan 30 5 min read Accelerate employee AI skilling: Insights from Microsoft chevron_right Our experience has yielded some widely applicable takeaways that can be helpful to organizations that want to build AI skills. Microsoft Source AI Jan 22 4 min read Making it easier for companies to build and ship AI people can trust chevron_right Generative AI is transforming many industries, but businesses often struggle with how to create and deploy safe and secure AI tools as technology evolves. Azure AI Jan 22 3 min read Coldplay evolves the fan experience with Microsoft AI chevron_right To coincide with Coldplay's latest project, the band collaborated with Microsoft to produce an AI-powered experience that lets fans interact with their new album in a unique and personal way. Explore Microsoft Cloud solutions Discover how the most trusted and comprehensive cloud can help you meet the challenges of a rapidly changing world. Learn more about Microsoft Cloud solutions Connect with us on social What's new Surface Pro Surface Laptop Surface Laptop Studio 2 Surface Laptop Go 3 Microsoft Copilot AI in Windows Explore Microsoft products Windows 11 apps Microsoft Store Account profile Download Center Microsoft Store support Returns Order tracking Certified Refurbished Microsoft Store Promise Flexible Payments Education Microsoft in education Devices for education Microsoft Teams for Education Microsoft 365 Education How to buy for your school Educator training and development Deals for students and parents Azure for students Business Microsoft Cloud Microsoft Security Dynamics 365 Microsoft 365 Microsoft Power Platform Microsoft Teams Microsoft 365 Copilot Small Business Developer & IT Azure Microsoft Developer Documentation Microsoft Learn Microsoft Tech Community Azure Marketplace AppSource Visual Studio Company Careers About Microsoft Company news Privacy at Microsoft Investors Diversity and inclusion Accessibility Sustainability English (United States) Your Privacy Choices Opt-Out Icon Your Privacy Choices Your Privacy Choices Opt-Out Icon Your Privacy Choices Consumer Health Privacy Sitemap Contact Microsoft Privacy Manage cookies Terms of use Trademarks Safety & eco Recycling About our ads © Microsoft 2025

Source: https://www.mdpi.com/2079-9292/14/1/47
CRP-RAG: A Retrieval-Augmented Generation Framework for Supporting Complex Logical Reasoning and Knowledge Planning Next Article in Journal Improved Soft-Starting Method for Doubly Fed Induction Machines Based on Standstill Rotor-Side Synchronization Previous Article in Journal A High Reliability Ferroresonant Inverter with Improved Efficiency and Wider Input Voltage Journals Active Journals Find a Journal Journal Proposal Proceedings Series Topics Information For Authors For Reviewers For Editors For Librarians For Publishers For Societies For Conference Organizers Open Access Policy Institutional Open Access Program Special Issues Guidelines Editorial Process Research and Publication Ethics Article Processing Charges Awards Testimonials Author Services Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings Series About Overview Contact Careers News Press Blog Sign In / Sign Up Notice You can make submissions to other journals here. clear Notice You are accessing a machine-readable page. In order to be human-readable, please install an RSS reader. Continue Cancel clear All articles published by MDPI are made immediately available worldwide under an open access license. No special permission is required to reuse all or part of the article published by MDPI, including figures and tables. For articles published under an open access Creative Common CC BY license, any part of the article may be reused without permission provided that the original article is clearly cited. For more information, please refer to https://www.mdpi.com/openaccess. Feature papers represent the most advanced research with significant potential for high impact in the field. A Feature Paper should be a substantial original Article that involves several techniques or approaches, provides an outlook for future research directions and describes possible research applications. Feature papers are submitted upon individual invitation or recommendation by the scientific editors and must receive positive feedback from the reviewers. Editor’s Choice articles are based on recommendations by the scientific editors of MDPI journals from around the world. Editors select a small number of articles recently published in the journal that they believe will be particularly interesting to readers, or important in the respective research area. The aim is to provide a snapshot of some of the most exciting work published in the various research areas of the journal. Original Submission Date Received: . You seem to have javascript disabled. Please note that many of the page functionalities won't work as expected without javascript enabled. clear zoom_out_map search menu Journals Active Journals Find a Journal Journal Proposal Proceedings Series Topics Information For Authors For Reviewers For Editors For Librarians For Publishers For Societies For Conference Organizers Open Access Policy Institutional Open Access Program Special Issues Guidelines Editorial Process Research and Publication Ethics Article Processing Charges Awards Testimonials Author Services Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings Series About Overview Contact Careers News Press Blog Sign In / Sign Up Submit Search for Articles: Title / Keyword Author / Affiliation / Email Journal All Journals Acoustics Acta Microbiologica Hellenica (AMH) Actuators Adhesives Administrative Sciences Adolescents Advances in Respiratory Medicine (ARM) Aerobiology Aerospace Agriculture AgriEngineering Agrochemicals Agronomy AI Air Algorithms Allergies Alloys Analytica Analytics Anatomia Anesthesia Research Animals Antibiotics Antibodies Antioxidants Applied Biosciences Applied Mechanics Applied Microbiology Applied Nano Applied Sciences Applied System Innovation (ASI) AppliedChem AppliedMath Aquaculture Journal Architecture Arthropoda Arts Astronomy Atmosphere Atoms Audiology Research Automation Axioms Bacteria Batteries Behavioral Sciences Beverages Big Data and Cognitive Computing (BDCC) BioChem Bioengineering Biologics Biology Biology and Life Sciences Forum Biomass Biomechanics BioMed Biomedicines BioMedInformatics Biomimetics Biomolecules Biophysica Biosensors Biosphere BioTech Birds Blockchains Brain Sciences Buildings Businesses C (Journal of Carbon Research) Cancers Cardiogenetics Catalysts Cells Ceramics Challenges ChemEngineering Chemistry Chemistry Proceedings Chemosensors Children Chips CivilEng Clean Technologies (Clean Technol.) Climate Clinical and Translational Neuroscience (CTN) Clinical Bioenergetics Clinics and Practice Clocks & Sleep Coasts Coatings Colloids and Interfaces Colorants Commodities Complications Compounds Computation Computer Sciences & Mathematics Forum Computers Condensed Matter Conservation Construction Materials Corrosion and Materials Degradation (CMD) Cosmetics COVID Craniomaxillofacial Trauma & Reconstruction (CMTR) Crops Cryo Cryptography Crystals Current Issues in Molecular Biology (CIMB) Current Oncology Dairy Data Dentistry Journal Dermato Dermatopathology Designs Diabetology Diagnostics Dietetics Digital Disabilities Diseases Diversity DNA Drones Drugs and Drug Candidates (DDC) Dynamics Earth Ecologies Econometrics Economies Education Sciences Electricity Electrochem Electronic Materials Electronics Emergency Care and Medicine Encyclopedia Endocrines Energies Energy Storage and Applications (ESA) Eng Engineering Proceedings Entropy Environmental and Earth Sciences Proceedings Environments Epidemiologia Epigenomes European Burn Journal (EBJ) European Journal of Investigation in Health, Psychology and Education (EJIHPE) Fermentation Fibers FinTech Fire Fishes Fluids Foods Forecasting Forensic Sciences Forests Fossil Studies Foundations Fractal and Fractional (Fractal Fract) Fuels Future Future Internet Future Pharmacology Future Transportation Galaxies Games Gases Gastroenterology Insights Gastrointestinal Disorders Gastronomy Gels Genealogy Genes Geographies GeoHazards Geomatics Geometry Geosciences Geotechnics Geriatrics Glacies Gout, Urate, and Crystal Deposition Disease (GUCDD) Grasses Green Health Hardware Healthcare Hearts Hemato Hematology Reports Heritage Histories Horticulturae Hospitals Humanities Humans Hydrobiology Hydrogen Hydrology Hygiene Immuno Infectious Disease Reports Informatics Information Infrastructures Inorganics Insects Instruments Intelligent Infrastructure and Construction International Journal of Environmental Research and Public Health (IJERPH) International Journal of Financial Studies (IJFS) International Journal of Molecular Sciences (IJMS) International Journal of Neonatal Screening (IJNS) International Journal of Orofacial Myology and Myofunctional Therapy (IJOM) International Journal of Plant Biology (IJPB) International Journal of Topology International Journal of Translational Medicine (IJTM) International Journal of Turbomachinery, Propulsion and Power (IJTPP) International Medical Education (IME) Inventions IoT ISPRS International Journal of Geo-Information (IJGI) J Journal of Ageing and Longevity (JAL) Journal of Cardiovascular Development and Disease (JCDD) Journal of Clinical & Translational Ophthalmology (JCTO) Journal of Clinical Medicine (JCM) Journal of Composites Science (J. Compos. Sci.) Journal of Cybersecurity and Privacy (JCP) Journal of Dementia and Alzheimer's Disease (JDAD) Journal of Developmental Biology (JDB) Journal of Experimental and Theoretical Analyses (JETA) Journal of Eye Movement Research (JEMR) Journal of Functional Biomaterials (JFB) Journal of Functional Morphology and Kinesiology (JFMK) Journal of Fungi (JoF) Journal of Imaging (J. Imaging) Journal of Intelligence (J. Intell.) Journal of Low Power Electronics and Applications (JLPEA) Journal of Manufacturing and Materials Processing (JMMP) Journal of Marine Science and Engineering (JMSE) Journal of Market Access & Health Policy (JMAHP) Journal of Mind and Medical Sciences (JMMS) Journal of Molecular Pathology (JMP) Journal of Nanotheranostics (JNT) Journal of Nuclear Engineering (JNE) Journal of Otorhinolaryngology, Hearing and Balance Medicine (JOHBM) Journal of Parks Journal of Personalized Medicine (JPM) Journal of Pharmaceutical and BioTech Industry (JPBI) Journal of Respiration (JoR) Journal of Risk and Financial Management (JRFM) Journal of Sensor and Actuator Networks (JSAN) Journal of the Oman Medical Association (JOMA) Journal of Theoretical and Applied Electronic Commerce Research (JTAER) Journal of Vascular Diseases (JVD) Journal of Xenobiotics (JoX) Journal of Zoological and Botanical Gardens (JZBG) Journalism and Media Kidney and Dialysis Kinases and Phosphatases Knowledge LabMed Laboratories Land Languages Laws Life Limnological Review Lipidology Liquids Literature Livers Logics Logistics Lubricants Lymphatics Machine Learning and Knowledge Extraction (MAKE) Machines Macromol Magnetism Magnetochemistry Marine Drugs Materials Materials Proceedings Mathematical and Computational Applications (MCA) Mathematics Medical Sciences Medical Sciences Forum Medicina Medicines Membranes Merits Metabolites Metals Meteorology Methane Methods and Protocols (MPs) Metrics Metrology Micro Microbiology Research Micromachines Microorganisms Microplastics Microwave Minerals Mining Modelling Modern Mathematical Physics Molbank Molecules Multimodal Technologies and Interaction (MTI) Muscles Nanoenergy Advances Nanomanufacturing Nanomaterials NDT Network Neuroglia Neurology International NeuroSci Nitrogen Non-Coding RNA (ncRNA) Nursing Reports Nutraceuticals Nutrients Obesities Oceans Onco Optics Oral Organics Organoids Osteology Oxygen Parasitologia Particles Pathogens Pathophysiology Pediatric Reports Pets Pharmaceuticals Pharmaceutics Pharmacoepidemiology Pharmacy Philosophies Photochem Photonics Phycology Physchem Physical Sciences Forum Physics Physiologia Plants Plasma Platforms Pollutants Polymers Polysaccharides Populations Poultry Powders Proceedings Processes Prosthesis Proteomes Psychiatry International Psychoactives Psychology International Publications Quantum Beam Science (QuBS) Quantum Reports Quaternary Radiation Reactions Real Estate Receptors Recycling Regional Science and Environmental Economics (RSEE) Religions Remote Sensing Reports Reproductive Medicine (Reprod. Med.) Resources Rheumato Risks Robotics Ruminants Safety Sci Scientia Pharmaceutica (Sci. Pharm.) Sclerosis Seeds Sensors Separations Sexes Signals Sinusitis Smart Cities Social Sciences Société Internationale d’Urologie Journal (SIUJ) Societies Software Soil Systems Solar Solids Spectroscopy Journal Sports Standards Stats Stresses Surfaces Surgeries Surgical Techniques Development Sustainability Sustainable Chemistry Symmetry SynBio Systems Targets Taxonomy Technologies Telecom Textiles Thalassemia Reports Therapeutics Thermo Time and Space Tomography Tourism and Hospitality Toxics Toxins Transplantology Trauma Care Trends in Higher Education Tropical Medicine and Infectious Disease (TropicalMed) Universe Urban Science Uro Vaccines Vehicles Venereology Veterinary Sciences Vibration Virtual Worlds Viruses Vision Waste Water Wild Wind Women World World Electric Vehicle Journal (WEVJ) Youth Zoonotic Diseases Article Type All Article Types Article Review Communication Editorial Abstract Book Review Brief Communication Brief Report Case Report Clinicopathological Challenge Comment Commentary Concept Paper Conference Report Correction Creative Data Descriptor Discussion Entry Essay Expression of Concern Extended Abstract Field Guide Giants in Urology Guidelines Hypothesis Interesting Images Letter New Book Received Obituary Opinion Perspective Proceeding Paper Project Report Protocol Registered Report Reply Retraction Short Note Study Protocol Systematic Review Technical Note Tutorial Urology around the World Viewpoint Advanced Search Section Special Issue Volume Issue Number Page Logical OperatorOperator AND OR Search Text Search Type All fields Title Abstract Keywords Authors Affiliations Doi Full Text References add_circle_outline remove_circle_outline Journals Electronics Volume 14 Issue 1 10.3390/electronics14010047 Submit to this Journal Review for this Journal Propose a Special Issue ► ▼ Article Menu Article Menu Academic Editor Fabio Grandi Subscribe SciFeed Recommended Articles Related Info Link Google Scholar More by Authors Links on DOAJ Xu, K. Zhang, K. Li, J. Huang, W. Wang, Y. on Google Scholar Xu, K. Zhang, K. Li, J. Huang, W. Wang, Y. on PubMed Xu, K. Zhang, K. Li, J. Huang, W. Wang, Y. /ajax/scifeed/subscribe Article Views Citations - Table of Contents Altmetric share Share announcement Help format_quote Cite question_answer Discuss in SciProfiles Need Help? Support Find support for a specific problem in the support section of our website. Get Support Feedback Please let us know what you think of our products and services. Give Feedback Information Visit our dedicated information section to learn more about MDPI. Get Information clear JSmol Viewer clear first_page Download PDF settings Order Article Reprints Font Type: Arial Georgia Verdana Font Size: Aa Aa Aa Line Spacing:    Column Width:    Background: Open AccessArticle CRP-RAG: A Retrieval-Augmented Generation Framework for Supporting Complex Logical Reasoning and Knowledge Planning by Kehan Xu Kehan Xu SciProfiles Scilit Preprints.org Google Scholar 2,†, Kun Zhang Kun Zhang SciProfiles Scilit Preprints.org Google Scholar 3,†, Jingyuan Li Jingyuan Li SciProfiles Scilit Preprints.org Google Scholar 1,*, Wei Huang Wei Huang SciProfiles Scilit Preprints.org Google Scholar 2 and Yuanzhuo Wang Yuanzhuo Wang SciProfiles Scilit Preprints.org Google Scholar 4,* 1 School of Computer and Artificial Intelligence, Beijing Technology and Business University, Beijing 102401, China 2 School of Information Science and Engineering, Yanshan University, Qinhuangdao 066104, China 3 Tencent WeChat AI—Pattern Recognition Center Tencent Inc., Beijing 100085, China 4 Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100005, China * Authors to whom correspondence should be addressed. † These authors contributed equally to this work. Electronics 2025, 14(1), 47; https://doi.org/10.3390/electronics14010047 Submission received: 20 November 2024 / Revised: 12 December 2024 / Accepted: 23 December 2024 / Published: 26 December 2024 (This article belongs to the Section Computer Science & Engineering) Download keyboard_arrow_down Download PDF Download PDF with Cover Download XML Download Epub Browse Figures Review Reports Versions Notes Abstract : The Retrieval-Augmented Generation (RAG) framework enhances Large Language Models (LLMs) by retrieving relevant knowledge to broaden their knowledge boundaries and mitigate factual hallucinations stemming from knowledge gaps. However, the RAG Framework faces challenges in effective knowledge retrieval and utilization; invalid or misused knowledge will interfere with LLM generation, reducing reasoning efficiency and answer quality. Existing RAG methods address these issues by decomposing and expanding queries, introducing special knowledge structures, and using reasoning process evaluation and feedback. However, the linear reasoning structures limit complex thought transformations and reasoning based on intricate queries. Additionally, knowledge retrieval and utilization are decoupled from reasoning and answer generation, hindering effective knowledge support during answer generation. To address these limitations, we propose the CRP-RAG framework, which employs reasoning graphs to model complex query reasoning processes more comprehensively and accurately. CRP-RAG guides knowledge retrieval, aggregation, and evaluation through reasoning graphs, dynamically adjusting the reasoning path based on evaluation results and selecting knowledge-sufficiency paths for answer generation. CRP-RAG outperforms the best LLM and RAG baselines by 2.46 in open-domain QA, 7.43 in multi-hop reasoning, and 4.2 in factual verification. Experiments also show the superior factual consistency and robustness of CRP-RAG over existing RAG methods. Extensive analyses confirm its accurate and fact-faithful reasoning and answer generation for complex queries. Keywords: retrieval-augmented generation (RAG); large language models (LLMs); reasoning graph; complex thought transformation; knowledge utilization 1. Introduction Large Language Models (LLMs) [1,2,3] have demonstrated remarkable performance in knowledge reasoning and exhibited proficiency across various task domains [4,5]. However, due to the uneven distribution of knowledge in the training data of LLMs, the parameterized knowledge stored within them constitutes only a subset of the world’s knowledge [6], indicating the existence of knowledge boundaries for LLMs. When LLMs attempt to answer questions beyond their knowledge boundaries, they may suffer from factual hallucinations due to the lack of corresponding knowledge, leading to the generation of content inconsistent with facts and compromising the accuracy of their answers. The Retrieval-Augmented Generation (RAG) framework [7,8] addresses this by retrieving external knowledge bases composed of external information, extracting non-parameterized knowledge, and incorporating it into model prompts, thereby embedding new knowledge into LLMs to expand their knowledge boundaries [9]. Despite its significant success in open-domain question answering tasks, the RAG framework still faces two major challenges. Challenge 1: The reasoning process of the RAG framework is disturbed by irrelevant knowledge. The retrieval results of the RAG framework often contain documents related to the query but irrelevant to reasoning [10]. As illustrated in Figure 1, a paragraph describing the color of horses owned by Joséphine de Beauharnais (Napoleon’s first wife) cannot serve as evidence to answer “What color was Napoleon’s horse?”, yet it may still be retrieved. This irrelevant knowledge will interfere with the reasoning process of the RAG framework, ultimately leading to hallucination phenomena during answer generation. Challenge 2: The RAG framework lacks the ability for knowledge planning. The RAG framework lacks the ability to understand relationships between knowledge and plan strategies for their utilization, resulting in the misuse of relevant knowledge during reasoning. Incorrect timelines and scopes of knowledge usage will lead to erroneous generation by the RAG framework. To address Challenge 1, current research advocates decomposing the reasoning process of complex queries [11,12] into several independent linear reasoning paths [13,14] to explain retrieval intent, aiming to enhance the reasoning relevance of the RAG framework’s retrieval results. To tackle Challenge 2, current research introduces special knowledge structures to pre-construct relationships between knowledge during the knowledge base construction phase [15,16]. On the other hand, some studies propose designing and integrating new RAG workflows, dynamically planning and formulating knowledge and reasoning strategies through dynamic programming and evaluating system states [17,18]. However, current RAG research still faces two issues. Issue 1: Existing RAG research does not support modeling the reasoning process and thought transformations for complex queries. The linear reasoning structures employed in current RAG research often do not support complex thought transformations involving multiple linear reasoning paths, such as switching between paths, backtracking, and other complex reasoning behaviors [19]. This results in RAG methods based on linear reasoning structures being unable to adjust reasoning strategies when knowledge is insufficient, leading to factual hallucinations during the generation process. Issue 2: There is a gap between the knowledge acquisition and usage process and the reasoning process in existing RAG research. Existing RAG research relies on static criteria unrelated to reasoning, such as pre-constructed knowledge structures and independent strategy evaluation modules, for knowledge retrieval and planning. However, the knowledge retrieval and planning within the reasoning process changes dynamically with the reasoning strategy. Therefore, static knowledge retrieval and planning often lead to these processes being isolated from the reasoning process, ultimately making it difficult for relevant knowledge to effectively support the reasoning process of the RAG framework. Addressing these key issues, our work’s aims are as follows: (1) design a complex reasoning structure for a RAG method to model complex queries; (2) guide the knowledge retrieval and utilization of the RAG method based on this structure; and (3) evaluate knowledge effectiveness within the reasoning process, dynamically adjusting the reasoning strategy of the RAG method to select the reasoning paths supported by valid knowledge. We propose CRP-RAG, a RAG framework supporting complex reasoning and knowledge planning. CRP-RAG models complex query reasoning through a reasoning graph, guiding knowledge retrieval, utilization planning, and strategy adjustment. It consists of the following three modules: Reasoning Graph Construction (GC), Knowledge Retrieval and Aggregation (KRA), and Answer Generation (AG). The GC module constructs a reasoning graph for the comprehensive and flexible representation of reasoning path relationships. The KRA module builds complex connections among knowledge based on the reasoning graph structure, conducting knowledge retrieval and aggregation at the level of reasoning graph nodes to ensure relevance between knowledge utilization and the reasoning process. The AG module evaluates knowledge effectiveness and selects valid reasoning paths for LLM reasoning and answer generation. Compared to existing RAG methods that rely on knowledge structure construction, sub-query decomposition, and self-planning, CRP-RAG expands and optimizes the solution space for complex queries by constructing nonlinear reasoning structures, aiming to enhance the reasoning capability of the RAG framework in response to such queries. Furthermore, CRP-RAG guides knowledge retrieval, evaluation, and reasoning strategy formulation based on reasoning logic, aligning relevant knowledge with the reasoning process to ensure the effective support of the reasoning process. Compared to the best-performing state-of-the-art LLM and RAG baselines, CRP-RAG achieves an average performance gain of 2.46 in open-domain question answering tasks—7.43 in multi-hop reasoning tasks, and 4.2 in factual verification tasks. Furthermore, experiments on factual consistency and robustness demonstrate that CRP-RAG exhibits superior factual consistency and robustness compared to the existing RAG methods. Extensive analyses show that CRP-RAG can perform accurate and fact-faithful reasoning and answer generation based on complex queries. We summarize our contributions as follows: (1) We are the first to propose a comprehensive modeling of the RAG reasoning process based on a reasoning graph and introduce the reasoning graph into the RAG approach to support more complex thought transformations. This provides a novel perspective for improving RAG in the context of complex queries. (2) We introduce a reasoning graph-based approach to guide knowledge retrieval, knowledge utilization, and answer generation, enhancing the rationality of reasoning and knowledge strategy formulation within the RAG framework during complex reasoning processes. (3) We conduct extensive experiments to demonstrate the effectiveness of our method. The experimental results showcase the excellent performance of the reasoning-guided RAG framework in complex reasoning and question-answering tasks, further proving the significant role of reasoning-guided knowledge planning in improving the reasoning capabilities of RAG for complex queries. This paper will sequentially introduce the related work pertinent to this study (Section 2), the detailed methodology design of CRP-RAG (Section 3), the primary experimental details and results (Section 4), the experiments and discussions regarding other aspects of CRP-RAG (Section 5), and the conclusions drawn from our research (Section 6). 2. Related Work 2.1. Retrieval-Augmented Generation Based on the Knowledge Structure The retrieval results of the RAG framework often consist of multiple unstructured knowledge documents, which are concatenated within prompt templates to assist the reasoning and generation processes of LLMs. Consequently, the knowledge associations among these unstructured documents necessitate additional reasoning by LLMs during the generation process. LLMs are prone to errors when understanding implicitly expressed knowledge associations, leading to the misuse of knowledge and, ultimately, erroneous decisions in the generation process. Therefore, some studies advocate for modeling the associations between knowledge through predefined knowledge structures, thereby forming a structured knowledge system. This system is then utilized as a prompt to guide LLMs in deeply understanding the interconnections among knowledge during the reasoning process and planning optimal knowledge utilization strategies within the framework of the knowledge system. In terms of knowledge structure selection, existing research often employs knowledge structures such as text templates, knowledge trees, and knowledge graphs to model the associations between knowledge. Text templates distill and summarize knowledge collections through the textual reconstruction of knowledge, during which LLMs perform knowledge distillation, summarization, and structuring according to instructions, explicitly expressing and expanding important knowledge association information through natural language [20,21]. Fine-tuning models based on text-template instructions enhance their understanding of users’ knowledge preferences [22]. Knowledge trees model the parent–child and hierarchical relationships between knowledge, improving efficiency in retrieval and knowledge utilization [16,23]. On the other hand, knowledge graphs model the entity associations between knowledge to assist LLMs in understanding the detailed associations of relevant knowledge within retrieval results [24,25,26,27,28]. Additionally, some studies design specific knowledge structures tailored to specific generation task goals to improve the performance of RAG in those tasks. CANDLE [29] extends existing knowledge bases by constructing concepts and instances of existing knowledge and establishing associations between abstract and instance knowledge. Thread [15] constructs associations between existing knowledge across different action decisions for action decision-making problems. Buffer of Thoughts [30] and ARM-RAG [31] extract general principles from knowledge and model the logical relationships between knowledge and experience. Due to the computational cost associated with dynamic knowledge modeling, most existing research tends to separate knowledge modeling from the reasoning process, performing the static modeling of knowledge during the knowledge base construction phase. However, some studies argue that the interaction between dynamic knowledge modeling and the knowledge retrieval process can further enhance model generation performance and improve the flexibility of knowledge utilization in the RAG method. They advocate for knowledge modeling after obtaining retrieval results. RECOMP [20] and BIDER [21] propose knowledge distillation based on existing retrieval results, obtaining more precise and abundant relevant knowledge and its associated information through knowledge aggregation. However, the knowledge structures employed and designed using existing knowledge structure modeling methods are independent of the answer generation and reasoning processes of RAG, which leads to the omission of logical relationships among knowledge during the reasoning process in the modeling stage. This triggers the improper use of knowledge by LLMs. 2.2. Retrieval-Augmented Generation Based on Query Decomposition The queries input into the RAG framework often exhibit ambiguity in expression and complexity in knowledge requirements. These complex knowledge needs and expressions are not represented at the semantic level, making it difficult for the retrieval process to understand them. When faced with complex queries, query decomposition methods typically perform reasoning and logical analysis in natural language to obtain an explicit representation of the user’s knowledge needs, thereby guiding and expanding the content of the retrieval results. Existing research on query decomposition in RAG methods includes reconstructive decomposition and expansive decomposition. Reconstructive decomposition focuses on ambiguous expressions and logical information in user queries, guiding LLMs to reformulate queries based on prompts [12,32]. LLMs deconstruct and analyze the knowledge needs of queries based on their own parametric knowledge and reasoning abilities. Compared to expansive decomposition, reconstructive decomposition demonstrates stronger preference alignment and self-correction capabilities. During the decomposition process, LLMs can achieve self-evolution and correction based on their own feedback [33,34,35] or refine reconstructive results through iterative query reformulation [36]. On the other hand, expansive decomposition decomposes queries into several sub-queries to expand the retrieval solution space based on specific decomposition rules [37,38] or structures [39,40]. By defining specific decomposition rules, processes, and structures, expansive decomposition can better ensure the thematic consistency of sub-queries and exhibit greater robustness. However, existing query decomposition methods often rely on LLMs to perform decomposition and reconstruction based on prompts. The implicit reasoning of LLMs may pose two issues as follows: (1) The decomposition and reconstruction of queries by LLMs are independent of the reasoning process, lacking explicit reasoning constraints, which can easily lead to errors during the decomposition of user queries. (2) The inexplicability of LLMs’ implicit reasoning results in the potential for topic drift in the reconstructed results of existing query decomposition and reconstruction methods, which will affect the effectiveness of knowledge retrieval and use. 2.3. Thinking and Planning in Retrieval-Augmented Generation The RAG framework expands the knowledge boundaries of LLMs. However, due to their “knowledge retrieval–answer generation” workflow, RAG must perform knowledge retrieval for each query, leading to the neglect of LLMs’ intrinsic parametric knowledge and potential adverse effects from irrelevant knowledge [18]. Therefore, planning for knowledge retrieval and utilization and assessing and perceiving own knowledge boundaries can enhance the efficiency of knowledge retrieval and utilization in RAG. The RAG frameworks based on planning and self-reflection extend the workflow of RAG into a nonlinear evaluation and decision-making process. By calculating and assessing metrics such as the knowledge adequacy of RAG and the factual consistency of generated answers during knowledge retrieval and answer generation, and making subsequent behavioral decisions based on the evaluation results, these methods dynamically adjust the workflow of RAG, thereby improving their efficiency. The current self-planning and reflective RAG framework primarily aims to plan and select retrieval occasions, as well as plan and correct generated content. The planning and selection of retrieval timing involve assessing metrics such as the adequacy of model parametric knowledge [17,18,41,42] and the effectiveness of retrieval results [43,44], thereby evaluating the value of knowledge retrieval and planning the timing and scope of retrieval. On the other hand, planning and correcting generated content involves assessing the quality of answers based on metrics such as the factual consistency [18,44] and accuracy [45,46] of the generated content. Based on these evaluations, the framework determines whether the generated content requires correction and employs iterative retrieval, answer expansion, and decomposition to expand and correct the answer content. Current RAG planning and self-reflection methods primarily focus on evaluating the effectiveness of the knowledge retrieval process and retrieval results, thereby adjusting the generation strategy. Based on the idea of self-reflection in RAG frameworks, we believe that the knowledge-based reasoning process of LLMs should also be evaluated. By incorporating process evaluation results, RAG frameworks should gain the ability to dynamically adjust their reasoning strategies, ensuring the rationality of path decisions during the reasoning process. 2.4. Reasoning Structure of LLMs LLMs possess powerful reasoning abilities, but their reasoning processes during answer generation are often uninterpretable. Therefore, explaining and enhancing LLMs’ reasoning capabilities pose significant challenges for improving their performance and practical applications. Based on LLMs’ instruction-following abilities, prompt engineering for reasoning enhancement has found that specific reasoning-enhanced prompts [47] can significantly improve the interpretability and accuracy of LLMs’ reasoning. Following these findings, some studies propose guiding LLMs to perform explicit instruction-based reasoning through prompts, achieving remarkable experimental results. However, reasoning rules in reasoning prompts often fail to fully guide LLMs in modeling the complete reasoning process. Hence, current research advocates for guiding LLMs to achieve more complete and accurate reasoning modeling through the design of reasoning structures. Unlike the linear reasoning structure represented by Chain of Thought (CoT) [13,48], CoT-SC [49] combines linear reasoning structures into a set of linear reasoning chains through the extensive sampling of reasoning steps, thereby expanding LLMs’ reasoning path selection space, enhancing the representation ability of reasoning structures, and broadening the range of reasoning operations that LLMs can choose. Meanwhile, Tree of Thought (ToT) [14] constructs the reasoning process as a tree, combining linear reasoning paths into a traceable multi-linear reasoning structure, further improving the representation ability of reasoning structures and expanding the reasoning operations available to LLMs. Graph of Thought (GoT) [19] defines and simulates reasoning graph structures, using nonlinear reasoning structures to support complex reasoning operations such as collaborative path reasoning among LLMs and reasoning backtrace among different paths. Inspired by GoT, this study designs a reasoning graph construction method suitable for the RAG method, avoiding the possibility of circular reasoning in GoT and further improving the efficiency of LLMs in complex reasoning. We believe that reasoning graphs can represent complex reasoning processes comprehensively and flexibly. Therefore, we use reasoning graphs to guide the reasoning path selection, knowledge retrieval, and utilization planning in the RAG method. 3. Method In this section, we introduce the framework design and reasoning process of CRP-RAG (Section 3.1), along with the structures and workflows of its three primary modules, the Reasoning Graph Construction (GC) Module (Section 3.2), the Knowledge Retrieval and Aggregation (KRA) Module (Section 3.3), and the Answer Generation (AG) Module (Section 3.4). The overall architecture of CRP-RAG is illustrated in Figure 2. 3.1. Preliminary For a given query q, the CRP-RAG framework initially models the reasoning process by iteratively constructing a reasoning graph G. The formulation for the reasoning graph construction process is defined by Equation (1). G = G C ( q ) (1) Given that both the question and the answer in a question-answering task should be unequivocally defined, the reasoning process in such tasks should not involve circular reasoning. Therefore, G = { V , E } is a directed acyclic reasoning graph, where V represents the set of nodes in the reasoning graph, with v i ∈ V denoting the node that represents a specific reasoning step in the process, expressed in natural language. E represents the set of edges in the reasoning graph, with e i ∈ E indicating the sequential relationship between reasoning steps. The knowledge retrieval and aggregation module operates on each node in V, retrieving and aggregating knowledge for all nodes to form an aggregated knowledge set K. The formulation for the knowledge retrieval and aggregation process is defined by Equation (2). K = K R A ( V , q ) (2) Notably, k i ∈ K represents the relevant knowledge obtained after knowledge retrieval and aggregation for the corresponding reasoning graph node v i , and it serves as the context to support the reasoning step associated with the node. The answer generation module evaluates the adequacy of knowledge for all nodes in V and, based on the evaluation results, selects knowledge-sufficiency reasoning paths to guide LLMs in completing the reasoning and answer generation, yielding the answer a. The formulation for this process is defined in Equation (3). a = A G ( K , V , q ) (3) 3.2. Reasoning Graph Construction Given a specific query q, the reasoning graph construction module iteratively explores all reasoning possibilities, storing all potential reasoning steps as graph nodes and merging similar reasoning steps to construct the reasoning graph G. G is a refined representation of all reasoning possibilities, guiding knowledge retrieval, utilization, and reasoning path selection. Specifically, the reasoning graph construction module starts iteration with the user query q and, based on the reasoning graph G n ′ = { V n ′ , E n ′ } at the end of the n − t h iteration, each iteration of the module consists of the following two steps: new node generation and node merging. New Node Generation: The new node generation step involves creating several new nodes for each sink node in V n ′ of the reasoning graph. These new nodes represent the next specific reasoning steps when the existing reasoning processes are taken as known conditions. The formula for generating new nodes for a particular sink node v i is expressed by Equation (4). V n n e w ( i ) = L L M p r o m p t g e n , v i , q (4) L L M ( ) leverages LLMs to generate text content based on input instruction information. V n n e w ( i ) denotes the set of new nodes generated based on v i , and p r o m p t g e n represents the prompt templates for generating new nodes as detailed in Appendix A. To ensure that the reasoning graph explores all possible reasoning paths as comprehensively as possible, we refrain from using greedy decoding during the LLMs’ generation process and instead employ sampling to enhance the diversity of the content generated by the LLMs. After generating new nodes for all sink nodes, the system obtains several sets of new nodes, V n n e w = [ V n n e w ( 1 ) , V n n e w ( 2 ) , … , V n n e w ( k ) ] , where the length of V n n e w is consistent with the number of sink nodes in V n ′ . Each element in the V n n e w is a set of new nodes generated based on the corresponding sink node. Node Merging: Due to the potential presence of reasoning step nodes with similar topics among all newly generated nodes, the system merges similar nodes to reduce redundant information in G and updates their connectivity status with the corresponding sink nodes. Specifically, the system performs node merging for each new node in all sets of V n n e w iteratively, resulting in V n n e w = [ v n n e w ( 1 ) , v n n e w ( 2 ) , … , v n n e w ( m ) ] , where m is the total number of nodes in all new node sets, and v n n e w ( i ) ∈ V n n e w represents a new node generated based on a certain sink node. For a new node v n n e w ( i ) , the node merging process involves calculating the similarity between it and all nodes in V n n e w one by one to determine whether nodes need to be merged (Equation (5)) and performing the merge operation if necessary (Equation (6)). s i m = e n c v n n e w i ⨀ e n c v n n e w j (5) v n c o m b i n e i = L L M p r o m p t c o m b i n e , v n n e w i , v n n e w j , q i f s i m > t h e r e s h o l d c o m b i n e (6) The e n c ( ) function semantically encodes the new nodes based on a language model. The s i m represents the semantic similarity score between nodes, which is a real number ranging from 0 to 1 and is calculated through the inner product of their encodings. Moreover, v n c o m b i n e i is the merged node resulting from the combination of v n n e w i and v n n e w j , which replaces the original nodes in V n n e w and inherits their incoming relationships. p r o m p t c o m b i n e is the instruction template for node merging detailed in Appendix A, and t h e r e s h o l d c o m b i n e is a hyperparameter representing the similarity threshold that sets the lower limit for the semantic similarity required for node merging. After node merging, the system obtains the merged new node set V n c o m b i n e and its relationships E n n e w with the corresponding sink nodes, constructing the subgraph G n + 1 ′ = { V n ′ ∪ V n c o m b i n e , E n ′ ∪ E n n e w } at the end of the ( n + 1 ) − t h iteration. Iteration Ending Condition: The iteration terminates when all sink nodes in the subgraph G i ′ formed after the i-th iteration correspond to the final reasoning step. At this point, the constructed reasoning graph G is identical to the reasoning subgraph G i ′ . 3.3. Knowledge Retrieval and Aggregation The Knowledge Retrieval and Aggregation process performs knowledge retrieval and aggregation for each node in V, forming an aggregated knowledge set K. The length of the set K is consistent with the length of the set V. Each k i ∈ K represents the relevant knowledge obtained through knowledge retrieval and aggregation for the corresponding node v i , serving as reasoning context to assist LLMs in performing reasoning under the topic of v i . For any node v i ∈ V , KRA acquires its relevant knowledge k i through the following two steps: knowledge retrieval and knowledge aggregation. Knowledge Retrieval: KRA initially performs knowledge retrieval based on each node in V, obtaining a retrieval result set D = [ d 1 , d 2 , … , d n ] . Each d i ∈ D represents the retrieval result set for the corresponding node v i , consisting of several related documents. For any node v i ∈ V , the formulation of knowledge retrieval is expressed as shown in Equation (7). d i = t o p k s i m i l a r i t y v i , r j i ∈ 0 , 1 , 2 , … , V − 1 , j ∈ 0 , 1 , 2 , … , R − 1 (7) Here, s i m i l a r i t y ( ) is the semantic similarity calculation function defined by Equation (5). The function top k ( ) returns the top k knowledge base documents with the highest similarity scores. R represents the external knowledge base being searched, and r j ∈ R denotes a document within the knowledge base. Knowledge Aggregation: To further extract key knowledge from d i and refine knowledge representation, the system performs knowledge refinement and aggregation on all retrieval result sets in D, forming an aggregated knowledge set K = [ k 1 , k 2 , … , k n ] . Each k i ∈ K is obtained from d i through knowledge aggregation. Knowledge refinement and aggregation are achieved by LLMs that generate knowledge summaries for the relevant documents in d i . The formulation for this process is shown in Equation (8). k i = L L M p r o m p t i n t e r g r a t i o n , d i , v i (8) p r o m p t i n t e g r a t i o n refers to the prompt template for knowledge aggregation provided in Appendix A. 3.4. Answer Generation Based on the reasoning graph G and the reasoning graph knowledge set K, the Answer Generation module first evaluates the knowledge sufficiency of each node v i in V. Based on the evaluation results, it selects a set of reasoning paths C = c 1 , c 2 , … , c p composed of knowledge-sufficient nodes for reasoning and answer generation. Each c i = s 1 , s 2 , … , s p represents a knowledge-sufficient reasoning path, where s 1 is a source node in G, s p is a sink node in G, and s i ∈ c i represents a reasoning step within the path. Specifically, the AG module consists of the following two steps: knowledge sufficiency evaluation, as well as reasoning path selection and answer generation. Knowledge Sufficiency Evaluation: The AG first calculates the textual perplexity of each node v i in V when LLMs perform reasoning based on the corresponding knowledge k i . This aims to quantify the sufficiency of the knowledge provided by k i during the reasoning process based on v i . If the textual perplexity is too high, it indicates that k i cannot provide sufficient knowledge support for LLMs to reason based on v i . Through the knowledge sufficiency evaluation, all nodes in V are divided into two subsets, V s u f f i c i e n t and V i n s u f f i c i e n t , based on whether their knowledge is sufficient. The formulas for evaluating the knowledge sufficiency of v i are shown in Equations (9) and (10). s c o r e = p e r p l e x i t y L L M p r o m p t e v a l u a t i o n , v i , k i (9) v i ∈ V s u f f i c i e n t , s c o r e < t h e r e s h o l d p e r p l e x i t y V i n s u f f i c i e n t , s c o r e ≥ t h e r e s h o l d p e r p l e x i t y (10) Here, s c o r e represents the textual perplexity of LLMs when executing a particular reasoning step. It evaluates the confidence level and factual adequacy of LLMs’ reasoning by leveraging the certainty of their generation probabilities. In addition, t h e r e s h o l d p e r p l e x i t y is a hyperparameter that represents the threshold for perplexity, and p r o m p t e v a l u a t i o n refers to the prompt template for knowledge evaluation provided in Appendix A. Reasoning Path Selection and Answer Generation: After obtaining V s u f f i c i e n t and V i n s u f f i c i e n t , the system selects several reasoning paths from the source nodes that satisfy the conditions to form a path set C, which serves as the reference reasoning paths for LLMs to generate answers. All reasoning paths c i = s 1 , s 2 , … , s p in the set satisfy the following three conditions: (1) s 1 is a source node in G; (2) s p is a sink node in G; and (3) any s i ∈ c i satisfies s i ∈ V s u f f i c i e n t . If all reasoning paths do not satisfy these three conditions, the knowledge base cannot support the reasoning and answering of the user queries, and the system will refuse to answer them. After obtaining the reasoning path set C, LLMs will perform iterative reasoning according to the order of reasoning steps in c i and ultimately generate an answer. The iteration starts with the user query q. During the n − t h iteration, assuming all previously reasoned steps are known conditions, c o n d i t i o n n , the sub-queries of the current reasoning step is s n , and its corresponding relevant knowledge is k n . The reasoning formulas for the n − t h iteration are shown in Equations (11) and (12). r e s u l t n = L L M c o n d i t i o n n , s n , k n (11) c o n d i t i o n n + 1 = c o n c a t e c o n d i t i o n n , r e s u l t n (12) The c o n c a t e ( ) function integrates the results of the n − t h iteration into the known conditions of the ( n + 1 ) − t h iteration using a template. The result generated based on the last reasoning step in c i serves as the answer based on the reasoning path c i . If the path set C contains multiple reasoning paths, the system generates an answer for each reasoning path. Subsequently, LLMs integrate these answers based on the user query and the answers generated for each reasoning path. The formula for the integration process is shown in Equation (13). a n s w e r = L L M p r o m p t a b s t r a c t , q , A (13) A = a 1 , a 2 , … , a m represents the set of answers generated for each reasoning path, and p r o m p t a b s t r a c t denotes the instruction template for answer integration and summarization provided in Appendix A. 4. Experiments This section introduces the selection of experimental datasets (Section 4.1), the baseline methods and evaluation metrics (Section 4.2), and other implementation details (Section 4.3). The experimental results (Section 4.4) demonstrate the superior performance of CRP-RAG in specific tasks. 4.1. Dataset We validate the performance of CRP-RAG on the following three downstream tasks: open-domain question answering, multi-hop reasoning, and factual verification. Open-Domain Question Answering: The open-domain question answering (ODQA) typically involves single-hop reasoning requiring open-world knowledge, assessing the model’s knowledge boundaries and its ability to acquire knowledge from external sources. This paper evaluates CRP-RAG’s ODQA performance using the following three typical datasets: (1) Natural Questions (NQ) [50], sourced from real-user search queries, consisting of approximately 300,000 questions, with the required open-domain knowledge drawn from extensive Wikipedia articles. (2) TriviaQA [51] comprises queries from news and social media searches across a wide range of domains, encompassing nearly 95,000 questions, where the necessary open-domain knowledge is distributed across diverse news articles and social media interactions. (3) WebQuestions (WQ) [52], composed of questions posed by real users on Google search engines and their associated web browsing behaviors, challenging models to acquire open-domain knowledge from extensive user web interactions. Multi-Hop Reasoning: For multi-hop reasoning tasks, models must perform multi-step reasoning based on questions while ensuring the rationality of knowledge retrieval and utilization at each step. This assesses the model’s reasoning capabilities and its ability to use and plan knowledge based on parametric and external sources. This paper evaluates CRP-RAG’s multi-hop reasoning performance using the following two typical datasets: (1) HotpotQA [53] introduces the concept of cross-document information integration for complex queries and is a widely used multi-hop reasoning dataset. The questions in this dataset exhibit complex characteristics such as ambiguous references and nested logic, requiring models to perform multi-step inference and ensure rational knowledge acquisition and utilization at each step. (2) 2WikiMultiHopQA [54] is a multi-hop reasoning dataset based on Wikipedia, comprising complex questions requiring multi-hop reasoning across multiple Wikipedia entries, necessitating models to perform multi-hop inference and complex question parsing based on Wikipedia articles. Factual Verification: For factual verification tasks, models are required to judge the correctness of given facts and generate explanations based on existing knowledge. In this context, models often need to locate judgment criteria in existing knowledge and perform backward reasoning based on the presented facts. Compared to multi-hop reasoning tasks, factual verification tasks assess a model’s backward reasoning abilities. This study evaluates model performance on factual verification using the FEVER dataset [55], which contains 145,000 Wikipedia-based statements. Models are required to collect evidence to support or refute these statements by leveraging parametric knowledge and acquiring knowledge from Wikipedia, with verification labels for each statement being “Supported”, “Refuted”, or “Not Enough Info”. This assesses the model’s ability to extract factual evidence from multiple documents based on statements and make factual judgments by reasonably utilizing this evidence. 4.2. Baselines and Metrics To comprehensively evaluate and demonstrate the superiority of CRP-RAG across various downstream tasks, this study selects several representative LLM-based question-answering methods as baselines. Vanilla LLMs: In this study, we evaluate the performance of vanilla LLMs in downstream tasks based on their inherent knowledge boundaries and reasoning abilities without external knowledge support. Specifically, we use Vanilla LLMs and LLMs enhanced with Tree-of-Thought (ToT) reasoning as baseline methods. (1) Vanilla LLMs rely on their parametric knowledge to implicitly reason according to task instructions and guide the recitation of parametric knowledge and answer generation through implicit reasoning processes. (2) LLMs Enhanced with ToT reasoning reconstruct the implicit reasoning process through trees based on their parametric knowledge, thereby improving the LLMs’ reasoning capabilities. RALM Framework: To evaluate the reasoning and knowledge planning capabilities of various RALMs (Retrieval-Augmented Language Models) frameworks in downstream tasks, this study selects four groups of representative RALM frameworks. (1) The Vanilla RALM Framework aligns with the RAG method but replaces the generative language model with LLMs to enhance reasoning and knowledge planning. (2) The Query Decomposition RALM Framework decomposes queries into sub-queries before knowledge retrieval to better represent the retrieval needs of user queries. This study chooses IRCoT [56] and ITER-REGEN [57] as baselines for question decomposition-based RALM, both of which use iterative retrieval to expand query information and improve the quality of retrieval results. (3) The Knowledge Structure RALM Framework models complex knowledge relationships in the knowledge base by designing special knowledge structures and prompting LLMs with knowledge associations between retrieval results through context. This study selects RAPTOR [16] and GraphRAG [28] as baselines for knowledge structure RALMs. RAPTOR constructs unstructured knowledge into knowledge trees to model hierarchical relationships between knowledge, while GraphRAG constructs unstructured knowledge into knowledge graphs, defining entities and entity relationships. (4) The Self-Planning RALM Framework evaluates indicators such as the value of relevant knowledge and the factual consistency of generated content during the retrieval and generation processes of RALMs and makes dynamic action decisions based on evaluation results to guide the RALM framework for reasonable knowledge retrieval and answer generation. This study chooses Think-then-Act [17] and Self-RAG [18] as baselines for self-planning RALMs. Think-then-Act decides whether to rewrite user queries and perform additional retrievals by evaluating the clarity and completeness of queries and the LLMs’ ability to answer them. Self-RAG implicitly evaluates retrieval occasions based on LLMs’ parametric knowledge and dynamically updates generated content by assessing the knowledge validity of retrieval results, the factual consistency of answers, and the value of answers. Evaluation Metrics: To evaluate the experimental results in open-domain question answering (QA) and multi-hop reasoning QA, which are both open-ended generation formats, we adopt the following three QA evaluation metrics: (1) The Exact Match (EM) score assesses the accuracy of the QA by checking whether the gold answer appears in the model’s generated content. (2) The F1 score evaluates the QA accuracy by comparing the word overlap between the gold answer and the model’s generated content. (3) The Acc-LM score assesses the answers’ accuracy by comparing the relationship between the gold answer and the model’s generated content using a frozen LLMs API, determining whether the model’s content conveys the same meaning as the gold answer. The mathematical representations of the three evaluation metrics are given by Equations (14), (15), and (16), respectively. For the fact verification task, which resembles a classification format, we use the Acc-LM score to compare the gold answer with the model’s classification results, evaluating the correctness of the classification. E x a c t M a t c h S c o r e = 1 , i f g o l d _ a n s w e r ∈ a n s w e r 0 , o t h e r w i s e (14) R o u g e − L = 2 × L C S ( a n s w e r , g o l d _ a n s w e r ) a n s w e r + g o l d _ a n s w e r F 1 S c o r e = 1 , R o u g e − L ≥ t h e r e s h o l d f 1 0 , R o u g e − L < t h e r e s h o l d f 1 (15) A c c − L M S c o r e = 1 , i f L L M ( p r o m p t e v a l u a t i o n , q , a n s w e r , g o l d _ a n s w e r ) = c o r r e c t 0 , o t h e r w i s e (16) The a n s w e r refers to the generated content by the model, g o l d _ a n s w e r represents the golden answer provided by the dataset, L C S ( ) denotes the function that defines the length of the Longest Common Subsequence (LCS), the threshold is a predefined F1 score threshold, and the p r o m p t e v a l u a t i o n is the prompt template used to guide LLMs in evaluating the answers. 4.3. Implementation Details Given our study’s reliance on frozen LLMs, we combined the training and test sets of all datasets into a single experimental test set without any model training. Additionally, we employed GLM-4-plus [58] as the generative LLM for CRP-RAG and all the baselines, using BGE-large-en [59] as the retrieval model and the Wikipedia knowledge base dump from April 2024 [60]. Due to the instruction sensitivity of LLMs, all baselines supporting external knowledge retrieval adopted a prompt-based knowledge fusion approach, retrieving the top five documents per query. To reduce model output uncertainty and enhance experiment reproducibility, except for our GC module, other LLMs generated outputs without sampling, with a temperature of 0.1. This study deployed and conducted the main experiments on two NVIDIA Tesla A40 GPUs, and deployed and conducted other experiments on two GeForce RTX 4090 GPUs. The remaining experimental settings for baseline methods were consistent with their original papers. 4.4. Main Results The experimental results for the three downstream tasks are presented in Table 1. The results demonstrate that CRP-RAG achieves superior performance compared to all baseline methods across all downstream tasks. Notably, the performance advantage is more pronounced in tasks with higher reasoning complexity, such as multi-hop reasoning and fact verification, indicating that CRP-RAG significantly enhances the complex reasoning capabilities of the RALM framework. This underscores the substantial performance improvement attributed to CRP-RAG’s dynamic adjustment of reasoning strategies and knowledge planning based on the reasoning graph. Specifically, CRP-RAG demonstrates significant performance improvements over Vanilla LLMs and ToT LLMs across all downstream tasks, demonstrating its effectiveness in providing external knowledge support and expanding the knowledge boundaries of LLMs. Compared to the Vanilla RALM baseline, CRP-RAG still exhibits notable performance gains, highlighting the effectiveness of the reasoning graph in representing complex relationships among knowledge and guiding the reasoning of LLMs. Furthermore, CRP-RAG shows more pronounced performance advantages in multi-hop reasoning and fact verification tasks when compared to the query decomposition RALM framework. We argue that existing query decomposition methods are independent of the RALMs reasoning framework and do not enhance the knowledge retrieval performance of RALMs during complex reasoning. Experiments also confirm that the reasoning graph can serve as an associative structure for reasoning and knowledge in complex reasoning tasks, assisting RALM in achieving knowledge retrieval based on the reasoning process, thereby enhancing their knowledge retrieval performance in complex reasoning. Moreover, CRP-RAG outperforms RALM frameworks with knowledge structure design in these two complex reasoning tasks, proving that constructing complex relationships among knowledge based on the reasoning process can further improve the performance of RALMs. When compared to the self-planning RALM framework, which also performs dynamic behavior decision-making, CRP-RAG further constrains the solution space of the reasoning process by the reasoning graph, reducing the uncertainty of the reasoning flow. By centering knowledge retrieval, utilization, and reasoning strategy formulation around the reasoning graph, CRP-RAG demonstrates that constructing the solution space based on the reasoning graph for knowledge retrieval, utilization, and answer generation can significantly enhance the performance of the RALM framework. 5. Discussion This section presents the experiments and analyses focusing on the details of CRP-RAG, further demonstrating the superiority of the CRP-RAG framework. The experiments encompass ablation studies to evaluate the effectiveness of each module within the CRP-RAG framework (Section 5.1); robustness experiments to assess CRP-RAG’s resilience against noise interference (Section 5.2); factual consistency experiments to evaluate CRP-RAG’s confidence level and factual fidelity in generating responses based on retrieved contexts (Section 5.3); performance experiments of CRP-RAG under sparse computational resources to evaluate its performance and framework design efficacy in resource-constrained environments (Section 5.4); reasoning graph structure evaluation experiments to assess the rationality of the reasoning structure utilized in CRP-RAG’s reasoning graphs (Section 5.5); and efficiency experiments of CRP-RAG to evaluate its temporal and computational efficiency (Section 5.6). After conducting the experimental analysis, we analyze CRP-RAG through case studies (Section 5.7) and discuss its limitations (Section 5.8). 5.1. Ablation Study We conducted a series of ablation experiments on CRP-RAG to ascertain the impact of each module on performance, further validating the effectiveness of our proposed method. Based on the CRP-RAG framework, we designed three ablation experimental groups targeting the KRA and AG modules for comparison with the original experimental group. Experiments involving the GC module are detailed and analyzed in Section 5.5. The ablation experimental groups include the following: (1) Knowledge Aggregation Ablation, which removes the knowledge aggregation phase in KRA and replaces it with the concatenation of retrieval results from the knowledge base. (2) Knowledge Evaluation Ablation, which disables the knowledge sufficiency evaluation phase in the AG module and replaces it with a breadth-first search to select the shortest and longest paths from the source node to the sink node in the reasoning graph as the target reasoning paths, bypassing knowledge evaluation and reasoning path selection. (3) Iterative Reasoning Ablation, which modifies the iterative reasoning approach in the answer generation phase of the GC module to a one-shot answer generation based on reasoning path prompts, eliminating the explicit multi-hop reasoning process of LLMs. We selected HotPotQA and FEVER as datasets for the ablation experiments, using Acc-LM as the evaluation metric. All other experimental settings in the ablation groups remained consistent with the main experiment. The ablation study results, presented in Table 2, indicate that all modules significantly contribute to the method’s performance. Notably, the knowledge aggregation ablation exhibits a substantial performance drop compared to CRP-RAG, demonstrating that the knowledge aggregation phase effectively reduces irrelevant and ineffective information within retrieval results, enhancing the quality of relevant knowledge through explicit knowledge distillation. Furthermore, both the knowledge evaluation and iterative reasoning ablations result in even more severe performance declines compared to the knowledge aggregation ablation. This suggests that knowledge evaluation and reasoning path selection aid LLMs in reasoning and knowledge utilization under the guidance of knowledge-sufficiency reasoning paths, mitigating factual hallucinations arising from knowledge scarcity during LLM reasoning. Additionally, iterative reasoning assists LLMs in better understanding the description of reasoning paths and conducting fine-grained reasoning based on these paths. 5.2. Robustness Analysis of CRP-RAG Given the inherent knowledge boundaries of LLMs, the reasoning graph construction in the GC module and the knowledge retrieval and aggregation in the KRA module are susceptible to generating LLM-induced noise. To demonstrate the robustness of CRP-RAG against such noise, we integrated partial noise into both GC and KRA modules, analyzed CRP-RAG’s workflow under these conditions, and evaluated its ability to resist noise interference. We conducted experiments on the HotPotQA and FEVER datasets, evaluating performance based on the average Acc-LM scores across datasets. To assess the robustness of CRP-RAG, we set up the following two interference groups: (1) Incorrect reasoning graph node construction, where we selected a percentage of nodes from the reasoning graph generated by the GC module and replaced them with interfering nodes generated by LLMs using unrelated task instructions. (2) Irrelevant retrieval results in the reasoning process, where we selected a percentage of nodes in the KRA module and replaced their associated knowledge summaries with unrelated text generated by LLMs using unrelated task instructions. The percentage of selected nodes was represented by the proportion of total nodes, with the constraint that selected nodes could not be source nodes to ensure normal reasoning process initiation. To guarantee the existence of viable reasoning paths, we limited the maximum percentage of selected nodes to 50% of the total, conducting experiments with a 10% increment as the evaluation criterion. As shown in Figure 3, CRP-RAG’s performance remains nearly constant compared to the no-interference condition when the number of interfering nodes does not exceed 40%. However, a significant performance drop occurs when the interference reaches 50%, where CRP-RAG mostly refuses to answer questions, indicating that most reasoning paths in the graph are insufficient for reasoning due to a lack of knowledge. For fewer than 50% of interfering nodes, CRP-RAG discards affected paths and dynamically selects unperturbed, knowledge-sufficiency paths for reasoning and answer generation. This phenomenon is more pronounced during knowledge retrieval and aggregation in the KRA module, where CRP-RAG refuses to answer most questions when interference exceeds 30%, indicating the widespread knowledge insufficiency of reasoning paths. Based on the experimental results, we conclude that CRP-RAG exhibits salient robustness, manifested in two aspects as illustrated in Figure 4. Firstly, in scenarios with lesser interference, CRP-RAG discards distracted reasoning paths and selects knowledge sufficiency, undisturbed paths for reasoning, and answer generation. Secondly, in cases of high interference, CRP-RAG refuses to generate answers due to unavailable reasoning graphs or insufficient knowledge, thereby avoiding the influence of interfering information that could lead to erroneous answers. 5.3. Perplexity and Retrieval Faithfulness Analysis of CRP-RAG The generation of confidence and factual consistency based on knowledge is a crucial standard for assessing the performance of RALM frameworks. Therefore, we analyze CRP-RAG’s knowledge acquisition and utilization capabilities by evaluating its perplexity during reasoning and generation, and the factual consistency between its answers and relevant knowledge. We analyze the confidence of our method’s answers by computing the average perplexity of CRP-RAG compared to baseline approaches on the HotPotQA and FEVER datasets. Additionally, we assess the factual consistency of our method’s generated answers by evaluating whether the rationales behind the outputs from CRP-RAG and various RAG baselines stem from retrieved relevant knowledge. Factual consistency is quantified by the percentage of generated samples whose rationales originate from relevant knowledge among all generated samples. The perplexity results, as shown in Table 3, indicate that CRP-RAG achieves significantly lower average generation perplexity than other baselines across both datasets. This demonstrates that knowledge retrieval and utilization based on a reasoning process better supports the reasoning and answer generation of LLMs, notably alleviating the issue of knowledge deficiency during their reasoning. As shown in Table 4, 92% of the generation results produced by CRP-RAG across both datasets are grounded in relevant knowledge obtained during the Knowledge Retrieval and Aggregation (KRA) module. This underscores the completeness and accuracy of the knowledge system derived from the KRA phase. Furthermore, the Answer Generation (AG) module appropriately utilizes this knowledge, which supports the reasoning and answer generation processes of the LLMs. 5.4. Performance Analysis of Frameworks in Resource-Scarce Environments CRP-RAG enhances the complex query reasoning capabilities of the RAG framework through a collaborative approach with multiple LLMs. Whether the performance gains originate from the workflow design of the framework requires further discussion. Additionally, in resource-constrained environments, the LLMs in CRP-RAG may not be deployable or usable. Therefore, we propose methods for constructing and deploying CRP-RAG under resource-sparse conditions and conduct experiments to analyze its performance. Specifically, in resource-constrained environments, we construct the CRP-RAG framework using multiple language models with fewer than 3 billion parameters. The resource-constrained CRP-RAG framework involves modifications to the following three modules: (1) In the Graph Construction (GC) module, new node generation is conducted through text generation by a T5 model fine-tuned on reasoning chains, while node merging employs a text extraction approach using a T5 model, fine-tuned on multiple datasets to extract critical information from multiple node texts for summarization. (2) In the Knowledge Retrieval and Aggregation (KRA) module, the knowledge summarization model leverages text extraction by a T5 model, fine-tuned on multiple datasets. (3) In the Answer Generation (AG) module, answer generation and reasoning are performed through text generation by a T5 model, fine-tuned on multiple datasets. The resource-constrained CRP-RAG framework requires only the deployment of three T5 models, with computational resource consumption being just 12.5% of that of the resource-intensive CRP-RAG framework. This represents a significantly smaller resource demand compared to the resource-intensive environment. We compare and analyze the performance of the resource-constrained CRP-RAG framework against the best-performing baselines within the CRP-RAG framework and the main experiments, using the HotPotQA and FEVER datasets. The experimental results, presented in Table 5, demonstrate that the resource-constrained CRP-RAG framework still achieves significant performance gains compared to the best baseline performances. This validates the rationality of the CRP-RAG framework design and its inherent low model dependency. Notably, the performance gains are even more pronounced when evaluated using the EM metric, further highlighting the significant improvement in factual consistency of the generated content by CRP-RAG. 5.5. Analysis of the Effectiveness of Graph Structure for Reasoning The GC module guides the action planning of complex reasoning processes by constructing reasoning graphs. However, reasoning graphs’ capability to model and guide the complex reasoning process still needs to be validated. We evaluate the influence of different reasoning structures used to model the reasoning process on the performance of RALMs through experiments. Utilizing the HotPotQA and FEVER datasets and the Acc-LM score, we modified and tested CRP-RAG with the following four distinct reasoning structures: (1) CRP-RAG constructing reasoning graphs based on the GC module to direct reasoning, knowledge retrieval, and utilization. (2) CRP-RAG (Tree), where the GC module is replaced by a reasoning tree construction module, guiding reasoning, knowledge retrieval, and utilization through a reasoning tree. (3) CRP-RAG (Chain), substituting the GC module with a reasoning chain construction module, directing reasoning, knowledge retrieval, and utilization via a set of reasoning chains. (4) CRP-RAG (Text Chunk), where the GC module is replaced by a user-query-based text rewriting module, degrading CRP-RAG into a self-reflective RALM framework relying on question rewriting and perplexity evaluation. As shown in Table 6, CRP-RAG outperforms other reasoning structures on both datasets, with a more significant advantage when the reasoning structure is degraded to chains and texts. The analysis of generated samples reveals the following two key advantages of the reasoning graph over other reasoning structures, as shown in Figure 5: (1) More rational knowledge retrieval and utilization. As a nonlinear structure, the reasoning graph represents complex relationships between reasoning steps more comprehensively and accurately. Knowledge retrieval based on reasoning graphs will recall the finer-grained relevant knowledge, ensuring retrieval completeness. Additionally, knowledge utilization based on the reasoning graph guarantees rationality by the reasoning process. (2) Ability to answer a broader range of complex queries through complex thought transformations. Non-graph reasoning structures construct and integrate one or multiple independent linear reasoning paths to model the reasoning process. When confronted with knowledge insufficient of reasoning paths for complex queries, CRP-RAG based on linear reasoning structures will decline to answer due to its inability to adjust the reasoning strategy, resulting in misassessments of reasoning knowledge adequacy within the RALM framework. In contrast, CRP-RAG based on the reasoning graph can dynamically adjust its reasoning strategy by combining solution spaces from multiple reasoning steps in the reasoning graph, selecting knowledge-sufficiency reasoning steps to form reasoning paths, and thus answering a wider range of complex queries. 5.6. Efficiency Analysis CRP-RAG ensures the robustness, accuracy, and factual consistency of the question-answering process through multiple knowledge retrieval and content generation iterations of the language model. However, the efficiency issues during its operation still require experimental verification and analysis. Therefore, we will evaluate the overall efficiency of the CRP-RAG framework and compare it with similar methods. Additionally, we will analyze the efficiency of CRP-RAG in practical use from the perspective of input examples and propose potential optimizations for time efficiency. Overall Efficiency Evaluation and Analysis: Since the primary time cost of CRP-RAG during the inference process stems from its invocation of LLMs, the experiments adopt the average, minimum, and maximum number of LLM invocations as the evaluation criteria for its efficiency. Comprehensive assessments of efficiency differences among methods are conducted on the open-domain question answering dataset NQ and the multi-hop reasoning dataset HotPotQA. Given that CRP-RAG achieves iterative dynamic knowledge evaluation and reasoning decision-making by constructing reasoning graphs, the Language Agent Tree Search (LATS) [61] is selected as the baseline for iterative reasoning decision-making. LATS realizes dynamic reasoning decisions by constructing a reasoning tree and dynamically evaluating reasoning along tree paths while updating decisions. Furthermore, Self-RAG [18] is chosen as the baseline for dynamic knowledge evaluation, assisting LLMs in making complex knowledge-based decisions through token-level dynamic knowledge evaluations. As shown in Table 7, due to the dynamic adjustment of reasoning strategies, CRP-RAG and LATS require iterative updates and evaluations of reasoning paths, with fine-grained adjustments to reasoning strategies on a per-inference-step basis. In contrast, Self-RAG only evaluates knowledge relevance, resulting in slightly higher minimum LLM invocations for CRP-RAG and LATS compared to Self-RAG. However, compared to LATS and Self-RAG, CRP-RAG exhibits fewer average LLM invocations. We observe that as the number of inference steps increases, the number of nodes in CRP-RAG’s reasoning graph increases linearly, leading to a linear increase in LLM invocations during knowledge aggregation and answer generation. In contrast, as the language agent tree in LATS becomes more complex, the LLM invocations for path evaluation, path backtracking, and path updating based on the tree increase nonlinearly and drastically with the increase in the number of reasoning paths and nodes in the tree. Due to the irrelevance of related knowledge and reasoning, Self-RAG requires more reflective steps when facing multi-hop reasoning queries, and the LLM invocations for these reflective steps increase nonlinearly with the increase in reasoning steps. Notably, CRP-RAG’s maximum LLM invocations are significantly fewer than those of the other baselines. This is because CRP-RAG constructs a pre-built acyclic reasoning solution space through the reasoning graph, eliminating cyclic reasoning in its reasoning process. In contrast, LATS’s path evaluation method and backtracking approach when solving complex queries may lead to cyclic reasoning by LLMs over several defined reasoning steps. Moreover, given the knowledge boundaries in the knowledge base, Self-RAG’s knowledge evaluation process can fall into iterative cycles of highly similar retrieval results, trapped in a loop of knowledge evaluation. In summary, the overall efficiency evaluation results indicate that, compared to other dynamic knowledge evaluation and reasoning decision-making methods, CRP-RAG does not exhibit significant differences in time efficiency for single-hop reasoning tasks. For multi-hop reasoning tasks, CRP-RAG’s overall efficiency does not decrease significantly with the complexity of the reasoning process. Improvements in handling cyclic reasoning issues make CRP-RAG’s efficiency more controllable. Therefore, CRP-RAG demonstrates certain advantages in overall efficiency compared to other baselines from almost all perspectives, without significant efficiency issues. Efficiency Analysis Based on Instances: The time efficiency of CRP-RAG in practical applications is related to the complexity of the input instances, thus analyzing efficiency issues based on input instances can better demonstrate the practical significance of the CRP-RAG method. Therefore, we classify the instances in the NQ and HotPotQA according to their reasoning complexity, analyze the efficiency of CRP-RAG under different reasoning complexities based on the classification results, and discuss the possible distribution of reasoning complexity in input instances in practical use. Specifically, since the data in NQ and HotPotQA is collected from actual user queries, we regard them as practical examples of open-domain question answering (QA) and multi-hop reasoning QA. Since the reasoning types defined in the datasets are all within three hops [53], we classify the questions in NQ and HotPotQA into single-hop reasoning questions (open-domain QA questions and entity comparison questions, accounting for 57.42% of the dataset), two-hop reasoning questions (single-bridge entity reasoning questions, accounting for 28% of the dataset), and three-hop reasoning questions (ambiguous reasoning questions and multi-bridge entity reasoning questions, accounting for 14.58% of the dataset). As shown in Figure 6, in practical use, single-hop and two-hop reasoning questions represent the main types of user questions. For single-hop reasoning questions, the average number of LLMs invocations for CRP-RAG is 3.12, while for two-hop reasoning questions, the average number of LLMs invocations increases to 6.67. Compared to other RAG methods, CRP-RAG maintains similar efficiency in single-hop reasoning questions without significant efficiency issues. However, for two-hop reasoning questions, CRP-RAG requires from two to four additional LLMs invocations, resulting in an additional 4 to 8 s of question answering delay. Considering the significant performance gain of CRP-RAG for multi-hop reasoning questions and the relatively small increase in delay, we believe that the delay is an effective trade-off between time and question-answering accuracy. For complex three-hop reasoning questions, the average number of CRP-RAG invocations is 10.25. However, in practical use, the RAG framework cannot correctly answer complex reasoning questions with three hops or more, and the iterative dynamic knowledge evaluation and reasoning decision framework will produce uncontrollable reasoning delays. CRP-RAG ensures the strong performance of RAG methods in complex queries, and its efficiency loss is still within a controllable range, retaining practical application significance. Therefore, in practical use, the efficiency issues of CRP-RAG are still due to an effective balance between performance and efficiency, and its efficiency issues do not affect the practical significance of CRP-RAG. 5.7. Case Study To better understand the working principles and performance advantages of CRP-RAG, we have selected a subset of generated samples from CRP-RAG and RAG based on various datasets, which are presented in Appendix B. Specifically, we have chosen samples from the following four scenarios: open-domain question answering, two-hops reasoning question answering, three-hops reasoning question answering, and question answering with distracting information. CRP-RAG initially generates a reasoning graph for each query, represented by several sextuples. Each sextuple encapsulates reasoning graph information related to a node in the graph, including node ID, content, predecessor node information, successor node information, whether it is a source node, whether it is a sink node, etc. Based on the node content of each node, CRP-RAG performs knowledge retrieval and aggregation, forming an aggregated knowledge document for the corresponding node, represented through a triplet consisting of (node ID, node knowledge document, and node knowledge evaluation result). After filtering nodes based on their knowledge evaluation results to identify knowledge-sufficient nodes, CRP-RAG selects a knowledge-sufficient reasoning path and proceeds with answer generation. Based on the operational examples provided in Appendix B, we observe the following: (1) In the context of single-hop reasoning for open-domain question answering, CRP-RAG performs additional query reformulation, knowledge integration, and evaluation during the retrieval process. Due to the shorter reasoning chain in single-hop reasoning, the reasoning graph generated by CRP-RAG is relatively small. Compared to the RAG framework, CRP-RAG expands the query based on the reasoning graph to enhance the relevance of the retrieved results to the question, and it further integrates and evaluates the retrieved knowledge, thereby ensuring adequate knowledge during the question-answering process. (2) In the scenario of multi-hop reasoning, CRP-RAG enhances the logical relevance between the retrieved results and the reasoning process through reasoning graph construction, and models complex relationships among knowledge through knowledge aggregation and evaluation. In contrast, the RAG framework is limited by the lack of logical relevance between the retrieved results and the reasoning process, resulting in ineffective knowledge support. (3) When faced with irrelevant information interference or queries beyond the knowledge boundary, the RAG framework is unable to evaluate the knowledge and filter out irrelevant information, leading to erroneous answers. However, the CRP-RAG framework employs knowledge evaluation to check the knowledge boundary and the validity of relevant knowledge for the question, determining the capability of LLMs to answer the question. If LLMs cannot answer the question with adequate knowledge, CRP-RAG will refuse to answer, thus protecting LLMs from the interference of irrelevant information. 5.8. Error Analysis and Limitations Despite the promising question-answering accuracy and factual consistency demonstrated by the proposed CRP-RAG framework, we aim to gain a deeper understanding of its bottlenecks and limitations to facilitate further improvements in future research. After analyzing failure cases, we have identified the following two scenarios where CRP-RAG may still underperform: (1) Over-reasoning for simple factual questions. While superior reasoning and knowledge planning generally lead to better performance, the expansion and optimization of the solution space for simple factual questions in CRP-RAG only yield marginal gains in its question-answering performance. We believe that these marginal gains in performance do not justify the additional computational costs incurred, indicating that CRP-RAG exhibits over-reasoning for simple factual questions, resulting in unnecessary computational expenditures. To mitigate this issue, CRP-RAG can apply the same knowledge retrieval, aggregation, and evaluation processes to the source nodes of the reasoning graph as to other nodes to assess the necessity of further expanding the knowledge graph. (2) Task-irrelevant knowledge evaluation. CRP-RAG’s knowledge evaluation process quantifies the certainty of the outputs generated by LLMs based on text perplexity to assess the adequacy of aggregated knowledge. However, its knowledge evaluation criteria are not task-specific, which limits CRP-RAG’s task adaptability under specific requirements. To address this issue, CRP-RAG can introduce more comprehensive knowledge evaluation standards and mechanisms during the knowledge evaluation stage, thereby enhancing the comprehensiveness and rationality of the knowledge evaluation. 6. Conclusions This paper introduces the CRP-RAG framework, which supports complex logical reasoning by modeling reasoning processes for complex queries through reasoning graphs. CRP-RAG guides knowledge retrieval, aggregation, and evaluation through reasoning graphs, dynamically adjusting the reasoning path according to evaluation results to select knowledge-sufficiency paths, and utilizes the knowledge along these paths to generate answers. Comprehensive evaluations across three tasks using multiple metrics demonstrate that CRP-RAG significantly outperforms existing strong baselines in text generation and question answering, with improvements in accuracy, factual consistency, and robustness of the generated content. Next, we will summarize the theoretical implications (Section 6.1), practical significance (Section 6.2), and framework challenges along with future work (Section 6.3) on CRP-RAG. 6.1. The Theoretical Implications of CRP-RAG The performance improvement offered by CRP-RAG over the RAG framework provides two theoretical insights. (1) At the knowledge level, the expanded solution space and flexible solution space transformation methods are effective bases for the knowledge retrieval and planning processes within the RAG framework. The essence of CRP-RAG in modeling complex queries through the construction of reasoning graphs lies in expanding and optimizing the solution space of the query through non-linear reasoning structures. For the RAG framework, the expansion of the solution space ensures the completeness of retrieval results, while the optimization of the solution space guarantees the rationality of modeling associations between knowledge and the utilization of LLMs’ knowledge. Therefore, the expanded solution space and more flexible solution space transformation methods can guide the RAG framework to conduct more efficient knowledge retrieval and planning. (2) At the reasoning level, knowledge planning and relationship construction based on knowledge boundaries are effective bases for designing reasoning strategies and complex thought transformations. LLMs tend to ignore their own knowledge boundaries during the reasoning process, leading to factual hallucinations when answering queries beyond their knowledge boundaries. CRP-RAG conducts knowledge evaluation based on reasoning graphs, essentially distilling knowledge planning and relationships and determining the knowledge boundaries of LLMs. LLMs will dynamically adjust path selection in the reasoning process based on the distilled knowledge planning and relationships, enabling the design of complex reasoning strategies and thought transformations within a clear knowledge boundary, thereby avoiding factual hallucinations. Thus, knowledge planning and the construction of knowledge relationships within knowledge boundaries are effective bases for LLMs’ reasoning. 6.2. The Practical Significance of CRP-RAG Based on the aforementioned experimental results, CRP-RAG offers three practical insights. (1) Reasoning-based query decomposition enhances the recall rate of relevant knowledge while ensuring its accuracy. While query decomposition can improve the recall rate of relevant knowledge, decomposition unrelated to reasoning may lead to the topic gap in knowledge retrieval, thereby reducing the accuracy of relevant knowledge. CRP-RAG performs reasoning-based query decomposition, ensuring factual consistency between relevant knowledge and the reasoning process while improving the recall rate. (2) Appropriate knowledge evaluation can mitigate the informational interference faced by LLMs during the reasoning process. Knowledge evaluation redefines the scope of relevant knowledge and knowledge utilization planning within the reasoning process, enhancing the relevance of accessible knowledge and reducing the interference from irrelevant knowledge faced by LLMs. (3) The flexibility of thought transformations in the RAG framework when confronted with complex queries is influenced by reasoning modeling. The completeness of reasoning modeling determines the flexibility of thought transformations in the RAG framework when dealing with complex queries. Reasoning modeling based on linear reasoning structures does not support nonlinear complex thought transformations, leading to a contraction of the solution space for complex queries. Therefore, more comprehensive reasoning modeling will improve the RAG framework’s ability to answer complex queries. 6.3. Challenges of CRP-RAG and Future Research Plans CRP-RAG still faces the following three challenges: (1) Time efficiency. Compared to the RAG framework, CRP-RAG enhances its performance in handling complex queries at the cost of time, and its time efficiency remains to be improved. (2) Adaptation from General Domains to Specific Domains. The current CRP-RAG still requires additional adaptation costs when adapting and aligning with specific domains. (3) Self-involving capability. As a zero-shot framework, the various modules of CRP-RAG currently cannot achieve autonomous evolution and coordinated updates based on environmental changes. Based on the aforementioned challenge, we plan to make the following optimizations in the future: (1) To address the issues of efficiency and computational cost, we will further refine the CRP-RAG framework to reduce its model deployment costs and enhance its time and resource efficiency during computation. (2) We will design a self-involving CRP-RAG framework based on knowledge editing, self-evolution theory, and reinforcement learning methods, enabling collaborative and autonomous evolution among the various modules of CRP-RAG in response to changes in the environment and data. (3) We will optimize the details of framework construction, such as knowledge evaluation, to enhance the alignment capability and adaptability of CRP-RAG in specific domains. Author Contributions Conceptualization, K.X. and K.Z.; methodology, K.X.; software, K.X.; validation, K.X.; formal analysis, K.X.; investigation, K.X.; resources, K.X. and W.H.; data curation, W.H.; writing—original draft preparation, K.X. and W.H.; writing—review and editing, K.Z., J.L. and Y.W.; visualization, K.X.; supervision, K.Z., J.L. and Y.W.; project administration, J.L. and Y.W.; funding acquisition, J.L. All authors have read and agreed to the published version of the manuscript. Funding This research received no external funding. Data Availability Statement The data used in this study were obtained from publicly available datasets [50,51,52,53,54,55]. Conflicts of Interest Author Kun Zhang was employed by the company Tencent WeChat AI—Pattern Recognition Center Tencent Inc. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. Correction Statement This article has been republished with a minor correction to the existing affiliation information. This change does not affect the scientific content of the article. Appendix A. Prompt Template In Appendix A, we will present the prompt templates for LLMs used in CRP-RAG. Appendix A.1. New Node Generation Please propose the next reasoning step to answer the given question based on the provided conditions. The content requirements are as follows: 1. The content of the next reasoning step must be grounded on the known conditions. 2. The content of the next reasoning step should be a sub-question or an explanation. The sub-question should be one that needs to be answered to address the given question based on the known conditions. The question is {Question}. The known conditions are {Known Conditions}. The generated examples are {Generated Examples}. Appendix A.2. Fusion Based on Similar Nodes Please merge the information from the multiple given text paragraphs to create a text summary that includes the complete content of all paragraphs. The content requirements are as follows: 1. Summarize and abstract the information commonly mentioned in each paragraph. 2. Integrate and list the content uniquely mentioned in each paragraph. 3. Annotate any conflicting information mentioned across the paragraphs. The question is {Question}. The collection of text paragraphs are {Set of Similar Nodes}. The generated examples are {Generated Example}. Appendix A.3. Knowledge Integration Please merge the relevant knowledge from the multiple search results provided and generate a knowledge summary based on the existing search result content. The content requirements are as follows: 1. Organize and summarize the knowledge related to the theme in the search results, and provide relevant concepts and examples of the knowledge. 2. Organize and summarize the knowledge that is not commonly mentioned in each search result by theme type and list them in bullet points. 3. When knowledge mentioned in the search results conflicts, judge the rationality of their content based on information such as time, location, and field, and delete unreasonable knowledge. If all conflicting knowledge is unreasonable, delete it all. The question is {Question}. The retrieval results are {Retrieval Results of Nodes}. The generated examples are {Generated Example}. Appendix A.4. Knowledge Sufficiency Evaluation Please answer the question according to the relevant knowledge. The question is {Question of Current Node} The relevant knowledge are {Relevant knowledge of Current Node}. Appendix A.5. Answer Abstracting Given that there are several corresponding answers to a question, all of which are derived through reasonable inference, please organize all the corresponding answers based on the question to form a final answer. The content requirements are as follows: 1. Merge answers with similar content. 2. List all different types of answers and provide the corresponding reasoning process and evidence for each answer. The question is: {Question}. The answer set is: {A set of answers generated through multiple reasoning paths}. Appendix B. Examples of Outputs Appendix B will present generation samples of CRP-RAG and the Vanilla RAG framework in several scenarios. We selected samples for ODQA generation involving single-hop reasoning, two-hop reasoning, three-hop reasoning, and QA generation based on distracting information for presentation. The questions in the open-domain QA and distracting information-based QA generation samples originate from the NQ dataset, while those in the multi-hop reasoning QA generation samples stem from the HotPotQA dataset. Appendix B.1. Open-Domain Question Answering Table A1. Sample generation of CRP-RAG in open-domain question answering, with green highlighting the correct and relevant facts in the generated samples, and orange highlighting the interfering facts. Table A1. Sample generation of CRP-RAG in open-domain question answering, with green highlighting the correct and relevant facts in the generated samples, and orange highlighting the interfering facts. Question: When did 10 rupees coin launched in India? Reasoning Graph Nodes: {“id”: 1, “content”: when did 10 rupees coin launched in India?, “if-start”: True, “if-end”: False, “next-step”: [2], “prev-step”: None} {“id”: 2, “content”: Look for information on 10 rupee coins and extract the date of 10 rupees coin first launched., “if-start”: False, “if-end”: True, “next-step”: None, “prev-step”: [1]} Relevant Knowledge: {“id”: 2, “relevant-knowledge”: The first 10 rupee coin in India was introduced in 2005. This coin was part of a series that included minting years from 2005 to 2007. The design of the coin features a bi-metallic composition with a copper-nickel center inside an aluminium-bronze ring., “perplexity”: 0.86} Answer: The 10 rupee coin was first launched in India in the year 2005. Table A2. Sample generation of Vanilla RAG in open-domain question answering, with green highlighting the correct and relevant facts in the generated samples, and orange hghlighting the interfering facts. Table A2. Sample generation of Vanilla RAG in open-domain question answering, with green highlighting the correct and relevant facts in the generated samples, and orange hghlighting the interfering facts. Question: When did 10 rupees coin launched in India? Relevant Context: Rupee is the common name for the currencies of India, Mauritius, Nepal, Pakistan, Seychelles, and Sri Lanka, and of former currencies of Afghanistan, Bahrain, Kuwait, Oman, the United Arab Emirates (as the Gulf rupee), British East Africa, Burma, German East Africa (as Rupie/Rupien), and Tibet. In Indonesia and the Maldives, the unit of currency is known as rupiah and rufiyaa, respectively, cognates of the word rupee. The Indian rupee and Pakistani rupee are subdivided into one hundred paise (singular paisa) or pice. The Indian rupee is the official currency in The Republic of India. The rupee is subdivided into 100 paise. The issuance of the currency is controlled by the Reserve Bank of India. The Reserve Bank manages currency in India and derives its role in currency management based on the Reserve Bank of India Act, 1934. In 2010, a new rupee sign was officially adopted. As its designer explained, it was derived from the combination of the Devanagari consonant ra and the Latin capital letter “R” without its vertical bar. The parallel lines at the top (with white space between them) are said to make an allusion to the flag of India, and also depict an equality sign that symbolises the nation’s desire to reduce economic disparity. The first series of coins with the new rupee sign started in circulation on 8 July 2011. Before this, India used “Rs” and “Re” as the symbols for multiple rupees and one rupee, respectively, and these symbols are still used in situations where the official symbol is unavailable. Answer: The 10 rupee coin was first launched in India on 8 July 2011. Appendix B.2. Two-Hops Reasoning Question Answering Table A3. Sample generation of CRP-RAG in two-hops reasoning question answering, with green highlighting for correct and relevant facts in the generated samples, and orange highlighting for interfering facts. Table A3. Sample generation of CRP-RAG in two-hops reasoning question answering, with green highlighting for correct and relevant facts in the generated samples, and orange highlighting for interfering facts. Question: What screenwriter with credits for “Evolution” co-wrote a film starring Nicolas Cage and Téa Leoni? Reasoning Graph Nodes: {“id”: 1, “content”: What screenwriter with credits for “Evolution” co-wrote a film starring Nicolas Cage and Té Leoni?, “if-start”: True, “if-end”: False, “next-step”: [2], “prev-step”: None} {“id”: 2, “content”: Find the screenwriter who worked on the movie “Evolution”., “if-start”: False, “if-end”: False, “next-step”: [4], “prev-step”: [1]} {“id”: 3, “content”: Find information about the movie co-written by Nicolas Cage and Téa Leoni., “if-start”: False, “if-end”: False, “next-step”: [4], “prev-step”: [1]} {“id”: 4, “content”: Find the screenwriter among those who worked on the movie “Evolution” who also co-wrote a movie with Nicolas Cage and Téa Leoni., “if-start”: False, “if-end”: True, “next-step”: None, “prev-step”: [2,3]} Relevant Knowledge: {“id”: 2, “relevant-knowledge”: The screenwriter for the movie “Evolution” was Don Jakoby. Furthermore, the script was later rewritten by David Diamond and David Weissman., “perplexity”: 0.44} {“id”: 3, “relevant-knowledge”: Nicolas Cage and Téa Leoni collaborated on the movie “The Family Man”, which was released in 2000, directed by Brett Ratner, and written by David Diamond and David Weissman. The film tells the story of Jack Campbell, played by Nicolas Cage, a successful Wall Street broker. He experiences a magical event on Christmas Eve that causes them to reevaluate their life and values, including their relationship with Kate, played by Téa Leoni, who was their girlfriend thirteen years prior., “perplexity”: 6.41} {“id”: 4, “relevant-knowledge”: David Diamond and David Weissman were involved in the production of the movie “The Family Man”, which stars Nicolas Cage and Téa Leoni. David Diamond and David Weissman served as the screenwriters for the film., “perplexity”: 2.94} Answer: The screenwriters who worked on “Evolution” and co-wrote a film starring Nicolas Cage and Téa Leoni are David Diamond and David Weissman. They were involved in writing “Evolution”, a sci-fi comedy film, and also co-wrote “The Family Man”, which starred Nicolas Cage and Téa Leoni. Table A4. Sample generation of Vanilla RAG in two-hops reasoning question answering, with green highlighting the correct and relevant facts in the generated samples, and orange highlighting the interfering facts. Table A4. Sample generation of Vanilla RAG in two-hops reasoning question answering, with green highlighting the correct and relevant facts in the generated samples, and orange highlighting the interfering facts. Question: What screenwriter with credits for “Evolution” co-wrote a film starring Nicolas Cage and Téa Leoni? Relevant Context: Evolution is a 2001 American science fiction comedy film directed by Ivan Reitman and written by David Diamond and David Weissman, based on a story by Don Jakoby. The film stars David Duchovny, Orlando Jones, Seann William Scott, Julianne Moore, and Ted Levine in lead roles. Evolution was released on 8 June 2001, by DreamWorks Pictures in the United States and Columbia Pictures internationally. The Family Man is a 2000 American romantic fantasy comedy-drama film. The film stars Nicolas Cage and Téa Leoni, with Don Cheadle, Saul Rubinek, and Jeremy Piven in supporting roles. At the 27th Saturn Awards, it was nominated for Best Fantasy Film and won Best Actress for Leoni. Answer: The screenwriter with credits for “Evolution” who co-wrote a film starring Nicolas Cage and Téa Leoni is Greg Kilday and David Diamond did not co-write Evolution, but did co-write the film “Bad Santa”. Appendix B.3. Three-Hops Reasoning Question Answering Table A5. Sample generation of CRP-RAG in three-hops reasoning question answering, with green highlighting the correct and relevant facts in the generated samples, and orange highlighting the interfering facts. Table A5. Sample generation of CRP-RAG in three-hops reasoning question answering, with green highlighting the correct and relevant facts in the generated samples, and orange highlighting the interfering facts. Question: What is the total number of members that the bands Letters to Cleo and Screaming Trees had when they were at their smallest? Reasoning Graph Nodes: {“id”: 1, “content”: What is the total number of members that the bands Letters to Cleo and Screaming Trees had when they were at their smallest?, “if-start”: True, “if-end”: False, “next-step”: [2,3], “prev-step”: None} {“id”: 2, “content”: Searching for information about Letters to Cleo to determine the changes in the number of band members., “if-start”: False, “if-end”: False, “next-step”: [4], “prev-step”: [1]} {“id”: 3, “content”: Collecting information on the band members of Screaming Trees., “if-start”: False, “if-end”: False, “next-step”: [5], “prev-step”: [1]} {“id”: 4, “content”: Gathering information about Screaming Trees and extracting details on the changes in the band’s membership., “if-start”: False, “if-end”: False, “next-step”: [6], “prev-step”: [2]} {“id”: 5, “content”: Based on the information about the band Screaming Trees, search for the membership information of the band Letters to Cleo., “if-start”: False, “if-end”: False, “next-step”: [6], “prev-step”: [3]} {“id”: 6, “content”: Add the sum of the minimum number of members from the bands Letters to Cleo and Screaming Trees., “if-start”: False, “if-end”: True, “next-step”: None, “prev-step”: [4,5]} Relevant Knowledge: {“id”: 2, “relevant-knowledge”: Letters to Cleo is an American rock band that formed in Boston, Massachusetts, in 1990, had a core lineup consisting of Michael Eisenstein, Kay Hanley, Stacy Jones, Greg McKenna, and Scott Riebling. Later, Tom Polce joined the band, bringing the total number of members to six. The band is best known for its blend of alternative rock, power pop, and punk influences, and it gained popularity in the mid-1990s, particularly with the release of its debut album, “Aurora Gory Alice,” in 1993., “perplexity”: 2.98} {“id”: 3, “relevant-knowledge”: Screaming Trees, an American rock band formed in Ellensburg, Washington, initially had four members: vocalist Mark Lanegan, guitarist Gary Lee Conner, bassist Van Conner, and drummer Mark Pickerel. Later, Pickerel was replaced by Barrett Martin, maintaining the band’s membership at four., “perplexity”: 1.30} {“id”: 4, “relevant-knowledge”: Screaming Trees was an American rock band formed in Ellensburg, Washington, in 1985. The band was part of the emerging grunge movement, alongside other notable Seattle-area bands like Nirvana, Pearl Jam, and Soundgarden. The original lineup of Screaming Trees included: Mark Lanegan: Lead vocals; Gary Lee Conner: Guitar, backing vocals; Van Conner: Bass, backing vocals and Mark Pickerel: Drums. “perplexity”: 5.57} {“id”: 5, “relevant-knowledge”: Letters to Cleo was an American rock band that formed in Boston, Massachusetts, in 1990. The band’s original lineup consisted of the following members: Kay Hanley—Lead Vocals; Michael Eisenstein—Guitar, Piano, Vocals; Greg McKenna—Drums; Stacy Jones—Bass Guitar; Tim Bradshaw—Guitar, Keyboards, Vocals., “perplexity”: 4.30} {“id”: 6, “relevant-knowledge”: The Letters to Cleo consisted of 5 members, and the Screaming Trees had 4 members. Therefore, the sum of the minimum number of members from both bands is 5 + 4 = 9. So, the minimum total number of members for Letters to Cleo and Screaming Trees combined is 9., “perplexity”: 0.44} Answer: The smallest lineup for the band Letters to Cleo, after numerous member changes, included five members: Kay Hanley, Greg McKenna, Mike Eisenstein on guitar, Stacy Jones on drums, and Scott Riebling on bass. For Screaming Trees, the smallest lineup during their most successful period included four members: Mark Lanegan on vocals, Gary Lee Conner on guitar, Van Conner on bass, and Barrett Martin on drums. Adding these numbers together, the total number of members that the bands Letters to Cleo and Screaming Trees had when they were at their smallest is 9. Table A6. Sample Generation of Vanilla RAG in three-hops reasoning question answering, with green highlighting the correct and relevant facts in the generated samples, and orange highlighting the interfering facts. Table A6. Sample Generation of Vanilla RAG in three-hops reasoning question answering, with green highlighting the correct and relevant facts in the generated samples, and orange highlighting the interfering facts. Question: What is the total number of members that the bands Letters to Cleo and Screaming Trees had when they were at their smallest? Relevant Context: Letters to Cleo is an American alternative rock band originating from Boston, Massachusetts, best known for the 1994 single, “Here and Now”, from their full-length debut album, Aurora Gory Alice. The band’s members are Kay Hanley, Greg McKenna, Michael Eisenstein, Stacy Jones, Scott Riebling, Tom Polce and Joe Klompus. Screaming Trees were an American rock band formed in Ellensburg, Washington, in 1984 by vocalist Mark Lanegan, guitarist Gary Lee Conner, bassist Van Conner, and drummer Mark Pickerel. Pickerel was replaced by Barrett Martin in 1991. Screaming Trees became known as one of the pioneers of grunge along with Melvins, Mudhoney, U-Men, Skin Yard, Soundgarden, Green River, and Malfunkshun, among others. Although widely associated with grunge, the band’s sound incorporated hard rock and psychedelic elements. The band changed its name to Letters to Cleo in 1990. The name “Letters to Cleo” is a reference to Hanley’s childhood pen pal. They spent their summers together in Canada, and they wrote each other letters to keep in touch over a ten-year period. At some point Cleo moved without giving Hanley her new mailing address, and Hanley’s letters to Cleo were returned to sender. Hanley rediscovered a box of these letters during the band-naming process, and the band adopted the name. Hanley called it “the least crappy of all the names” they came up with at the time. Answer: I am unable to verify the number of members of Screaming Trees and Letters to Cleo at their smallest. Appendix B.4. Question Answering with Information Interference Table A7. Sample generation of CRP-RAG in question answering with information interference, with green highlighting the correct and relevant facts in the generated samples, and orange highlighting the interfering facts. Table A7. Sample generation of CRP-RAG in question answering with information interference, with green highlighting the correct and relevant facts in the generated samples, and orange highlighting the interfering facts. Question: Who are the candidates for the 2024 US presidential election? Reasoning Graph Nodes: {“id”: 1, “content”: Who are the candidates for the 2024 US presidential election?, “if-start”: True, “if-end”: False, “next-step”: [2], “prev-step”: None} {“id”: 2, “content”: Find the information about the candidates’ names for the 2024 US presidential election., “if-start”: False, “if-end”: True, “next-step”: None, “prev-step”: [1]} Relevant Knowledge: {“id”: 2, “relevant-knowledge”: I apologize for not being able to summarize the information about the 2024 US presidential election, but I can provide the candidate information from the last US presidential election. In the 2020 election, the main candidates were Joe Biden from the Democratic Party and Donald Trump from the Republican Party. Joe Biden ultimately won the election and became the 46th President of the United States. There are likely to be many other candidates from various parties, including Republicans, Democrats, and third-party candidates. The field of candidates typically becomes clearer as the election approaches and during the primary season. For the most accurate and up-to-date information, I recommend checking reputable news sources or official election websites like the Federal Election Commission (FEC) website., “perplexity”: 874.30} Answer: [Refuse Answering] Table A8. Sample generation of Vanilla RAG in question answering with information interference, with green highlighting the correct and relevant facts in the generated samples, and orange highlighting the interfering facts. Table A8. Sample generation of Vanilla RAG in question answering with information interference, with green highlighting the correct and relevant facts in the generated samples, and orange highlighting the interfering facts. Question: Who are the candidates for the 2024 US presidential election? Relevant Context: The election of the president and for vice president of the United States is an indirect election in which citizens of the United States who are registered to vote in one of the fifty U.S. states or in Washington, D.C., cast ballots not directly for those offices, but instead for members of the Electoral College. These electors then cast direct votes, known as electoral votes, for president and for vice president. The candidate who receives an absolute majority of electoral votes (at least 270 out of 538, since the Twenty-Third Amendment granted voting rights to citizens of D.C.) is then elected to that office. If no candidate receives an absolute majority of the votes for president, the House of Representatives elects the president; likewise if no one receives an absolute majority of the votes for vice president, then the Senate elects the vice president. Answer: The United States presidential election is the process by which the president and vice president of the United States are selected. This election is held every four years, and the next one will take place in 2024. The election process is governed by the United States Constitution and laws of the federal and state governments. References Brown, T.B. Language models are few-shot learners. arXiv 2020, arXiv:2005.14165. [Google Scholar] Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H.W.; Sutton, C.; Gehrmann, S.; et al. Palm: Scaling language modeling with pathways. J. Mach. Learn. Res. 2023, 24, 1–113. [Google Scholar] Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. Llama 2: Open foundation and fine-tuned chat models. arXiv 2023, arXiv:2307.09288. [Google Scholar] Bang, Y.; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie, B.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; et al. A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics, Bali, Indonesia, 1–4 November 2023; Volume 1: Long Papers, pp. 675–718. [Google Scholar] Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. Training language models to follow instructions with human feedback. Adv. Neural Inf. Process. Syst. 2022, 35, 27730–27744. [Google Scholar] Huang, M.; Zhu, X.; Gao, J. Challenges in building intelligent open-domain dialog systems. ACM Trans. Inf. Syst. (TOIS) 2020, 38, 1–32. [Google Scholar] [CrossRef] Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.t.; Rocktäschel, T.; et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Adv. Neural Inf. Process. Syst. 2020, 33, 9459–9474. [Google Scholar] Guu, K.; Lee, K.; Tung, Z.; Pasupat, P.; Chang, M. Retrieval augmented language model pre-training. In Proceedings of the International Conference on Machine Learning, PMLR, Virtual, 13–18 July 2020; pp. 3929–3938. [Google Scholar] Ram, O.; Levine, Y.; Dalmedigos, I.; Muhlgay, D.; Shashua, A.; Leyton-Brown, K.; Shoham, Y. In-context retrieval-augmented language models. Trans. Assoc. Comput. Linguist. 2023, 11, 1316–1331. [Google Scholar] [CrossRef] Cuconasu, F.; Trappolini, G.; Siciliano, F.; Filice, S.; Campagnano, C.; Maarek, Y.; Tonellotto, N.; Silvestri, F. The power of noise: Redefining retrieval for rag systems. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, Washington, DC, USA, 14–18 July 2024; pp. 719–729. [Google Scholar] Mao, Y.; He, P.; Liu, X.; Shen, Y.; Gao, J.; Han, J.; Chen, W. Generation-Augmented Retrieval for Open-Domain Question Answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Virtual, 1–6 August 2021; Volume 1: Long Papers, pp. 4089–4100. [Google Scholar] Kim, M.; Park, C.; Baek, S. Augmenting Query and Passage for Retrieval-Augmented Generation using LLMs for Open-Domain Question Answering. arXiv 2024, arXiv:2406.14277. [Google Scholar] Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q.V.; Zhou, D. Chain-of-thought prompting elicits reasoning in large language models. Adv. Neural Inf. Process. Syst. 2022, 35, 24824–24837. [Google Scholar] Yao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T.; Cao, Y.; Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. Adv. Neural Inf. Process. Syst. 2024, 36, 11809–11822. [Google Scholar] An, K.; Yang, F.; Li, L.; Lu, J.; Cheng, S.; Si, S.; Wang, L.; Zhao, P.; Cao, L.; Lin, Q.; et al. Thread: A Logic-Based Data Organization Paradigm for How-To Question Answering with Retrieval Augmented Generation. arXiv 2024, arXiv:2406.13372. [Google Scholar] Sarthi, P.; Abdullah, S.; Tuli, A.; Khanna, S.; Goldie, A.; Manning, C.D. RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval. In Proceedings of the Twelfth International Conference on Learning Representations, Vienna, Austria, 7–11 May 2024. [Google Scholar] Shen, Y.; Jiang, H.; Qu, H.; Zhao, J. Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation. arXiv 2024, arXiv:2406.13050. [Google Scholar] Asai, A.; Wu, Z.; Wang, Y.; Sil, A.; Hajishirzi, H. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In Proceedings of the Twelfth International Conference on Learning Representations, Vienna, Austria, 7–11 May 2024. [Google Scholar] Besta, M.; Blach, N.; Kubicek, A.; Gerstenberger, R.; Podstawski, M.; Gianinazzi, L.; Gajda, J.; Lehmann, T.; Niewiadomski, H.; Nyczyk, P.; et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vancouver, BC, Canada, 20–27 February 2024; Volume 38, pp. 17682–17690. [Google Scholar] Xu, F.; Shi, W.; Choi, E. Recomp: Improving retrieval-augmented lms with compression and selective augmentation. arXiv 2023, arXiv:2310.04408. [Google Scholar] Jin, J.; Zhu, Y.; Zhou, Y.; Dou, Z. BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence. arXiv 2024, arXiv:2402.12174. [Google Scholar] Wang, Z.; Teo, S.X.; Ouyang, J.; Xu, Y.; Shi, W. M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions. arXiv 2024, arXiv:2405.16420. [Google Scholar] Goel, K.; Chandak, M. HIRO: Hierarchical Information Retrieval Optimization. arXiv 2024, arXiv:2406.09979. [Google Scholar] He, X.; Tian, Y.; Sun, Y.; Chawla, N.V.; Laurent, T.; LeCun, Y.; Bresson, X.; Hooi, B. G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. arXiv 2024, arXiv:2402.07630. [Google Scholar] Xu, Z.; Cruz, M.J.; Guevara, M.; Wang, T.; Deshpande, M.; Wang, X.; Li, Z. Retrieval-augmented generation with knowledge graphs for customer service question answering. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, Washington, DC, USA, 14–18 July 2024; pp. 2905–2909. [Google Scholar] Zhang, K.; Chen, C.; Wang, Y.; Tian, Q.; Bai, L. Cfgl-lcr: A counterfactual graph learning framework for legal case retrieval. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Long Beach, CA, USA, 6–10 August 2023; pp. 3332–3341. [Google Scholar] Chen, R.; Jiang, W.; Qin, C.; Rawal, I.S.; Tan, C.; Choi, D.; Xiong, B.; Ai, B. LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments. arXiv 2024, arXiv:2408.15903. [Google Scholar] Edge, D.; Trinh, H.; Cheng, N.; Bradley, J.; Chao, A.; Mody, A.; Truitt, S.; Larson, J. From local to global: A graph rag approach to query-focused summarization. arXiv 2024, arXiv:2404.16130. [Google Scholar] Wang, W.; Fang, T.; Li, C.; Shi, H.; Ding, W.; Xu, B.; Wang, Z.; Bai, J.; Liu, X.; Cheng, J.; et al. CANDLE: Iterative conceptualization and instantiation distillation from large language models for commonsense reasoning. arXiv 2024, arXiv:2401.07286. [Google Scholar] Yang, L.; Yu, Z.; Zhang, T.; Cao, S.; Xu, M.; Zhang, W.; Gonzalez, J.E.; Cui, B. Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models. arXiv 2024, arXiv:2406.04271. [Google Scholar] Melz, E. Enhancing llm intelligence with arm-rag: Auxiliary rationale memory for retrieval augmented generation. arXiv 2023, arXiv:2311.04177. [Google Scholar] Wang, K.; Duan, F.; Li, P.; Wang, S.; Cai, X. LLMs Know What They Need: Leveraging a Missing Information Guided Framework to Empower Retrieval-Augmented Generation. arXiv 2024, arXiv:2404.14043. [Google Scholar] Zhou, P.; Pujara, J.; Ren, X.; Chen, X.; Cheng, H.T.; Le, Q.V.; Chi, E.H.; Zhou, D.; Mishra, S.; Zheng, H.S. Self-discover: Large language models self-compose reasoning structures. arXiv 2024, arXiv:2402.03620. [Google Scholar] Sun, S.; Li, J.; Zhang, K.; Sun, X.; Cen, J.; Wang, Y. A novel feature integration method for named entity recognition model in product titles. Comput. Intell. 2024, 40, e12654. [Google Scholar] [CrossRef] Zhang, K.; Qiu, Y.; Wang, Y.; Bai, L.; Li, W.; Jiang, X.; Shen, H.; Cheng, X. Meta-cqg: A meta-learning framework for complex question generation over knowledge bases. In Proceedings of the 29th International Conference on Computational Linguistics, Gyeongju, Republic of Korea, 12–17 October 2022; pp. 6105–6114. [Google Scholar] Feng, J.; Tao, C.; Geng, X.; Shen, T.; Xu, C.; Long, G.; Zhao, D.; Jiang, D. Synergistic Interplay between Search and Large Language Models for Information Retrieval. arXiv 2023, arXiv:2305.07402. [Google Scholar] Shi, Z.; Zhang, S.; Sun, W.; Gao, S.; Ren, P.; Chen, Z.; Ren, Z. Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, Bangkok, Thailand, 11–16 August 2024; Volume 1: Long Papers, pp. 7339–7353. [Google Scholar] Yoran, O.; Wolfson, T.; Ram, O.; Berant, J. Making Retrieval-Augmented Language Models Robust to Irrelevant Context. In Proceedings of the Twelfth International Conference on Learning Representations, Vienna, Austria, 7–11 May 2024. [Google Scholar] Zhang, K.; Zeng, J.; Meng, F.; Wang, Y.; Sun, S.; Bai, L.; Shen, H.; Zhou, J. Tree-of-Reasoning Question Decomposition for Complex Question Answering with Large Language Models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vancouver, BC, Canada, 20–27 February 2024; Volume 38, pp. 19560–19568. [Google Scholar] Zhang, K.; Lin, X.; Wang, Y.; Zhang, X.; Sun, F.; Jianhe, C.; Tan, H.; Jiang, X.; Shen, H. Refsql: A retrieval-augmentation framework for text-to-sql generation. In Proceedings of the Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, 6–10 December 2023; pp. 664–673. [Google Scholar] Ding, H.; Pang, L.; Wei, Z.; Shen, H.; Cheng, X. Retrieve only when it needs: Adaptive retrieval augmentation for hallucination mitigation in large language models. arXiv 2024, arXiv:2402.10612. [Google Scholar] Su, W.; Tang, Y.; Ai, Q.; Wu, Z.; Liu, Y. DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models. arXiv 2024, arXiv:2403.10081. [Google Scholar] Yan, S.Q.; Gu, J.C.; Zhu, Y.; Ling, Z.H. Corrective retrieval augmented generation. arXiv 2024, arXiv:2401.15884. [Google Scholar] Liu, Y.; Peng, X.; Zhang, X.; Liu, W.; Yin, J.; Cao, J.; Du, T. RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback. arXiv 2024, arXiv:2403.06840. [Google Scholar] Kim, J.; Nam, J.; Mo, S.; Park, J.; Lee, S.W.; Seo, M.; Ha, J.W.; Shin, J. SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs. In Proceedings of the Twelfth International Conference on Learning Representations, Vienna, Austria, 7–11 May 2024. [Google Scholar] He, B.; Chen, N.; He, X.; Yan, L.; Wei, Z.; Luo, J.; Ling, Z.H. Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation. In Proceedings of the Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, FL, USA, 12–16 November 2024; pp. 10371–10393. [Google Scholar] Kojima, T.; Gu, S.S.; Reid, M.; Matsuo, Y.; Iwasawa, Y. Large language models are zero-shot reasoners. Adv. Neural Inf. Process. Syst. 2022, 35, 22199–22213. [Google Scholar] Zhang, Z.; Zhang, A.; Li, M.; Smola, A. Automatic Chain of Thought Prompting in Large Language Models. In Proceedings of the Eleventh International Conference on Learning Representations, Virtual, 22–29 April 2022. [Google Scholar] Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.V.; Chi, E.H.; Narang, S.; Chowdhery, A.; Zhou, D. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In Proceedings of the Eleventh International Conference on Learning Representations, Virtual, 22–29 April 2022. [Google Scholar] Kwiatkowski, T.; Palomaki, J.; Redfield, O.; Collins, M.; Parikh, A.; Alberti, C.; Epstein, D.; Polosukhin, I.; Devlin, J.; Lee, K.; et al. Natural questions: A benchmark for question answering research. Trans. Assoc. Comput. Linguist. 2019, 7, 453–466. [Google Scholar] [CrossRef] Joshi, M.; Choi, E.; Weld, D.S.; Zettlemoyer, L. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver, BC, Canada, 30 July–4 August 2017; Volume 1: Long Papers, pp. 1601–1611. [Google Scholar] Berant, J.; Chou, A.; Frostig, R.; Liang, P. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, WA, USA, 18–21 October 2013; pp. 1533–1544. [Google Scholar] Yang, Z.; Qi, P.; Zhang, S.; Bengio, Y.; Cohen, W.; Salakhutdinov, R.; Manning, C.D. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, 31 October–4 November 2018; pp. 2369–2380. [Google Scholar] Ho, X.; Nguyen, A.K.D.; Sugawara, S.; Aizawa, A. Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps. In Proceedings of the 28th International Conference on Computational Linguistics, Barcelona, Spain, 8–13 December 2020; pp. 6609–6625. [Google Scholar] Thorne, J.; Vlachos, A.; Christodoulopoulos, C.; Mittal, A. FEVER: A Large-scale Dataset for Fact Extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, New Orleans, LA, USA, 1–6 June 2018; Volume 1 (Long Papers), pp. 809–819. [Google Scholar] Trivedi, H.; Balasubramanian, N.; Khot, T.; Sabharwal, A. Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, Toronto, ON, Canada, 9–14 July 2023; Volume 1: Long Papers, pp. 10014–10037. [Google Scholar] Shao, Z.; Gong, Y.; Shen, Y.; Huang, M.; Duan, N.; Chen, W. Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy. In Proceedings of the Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, 6–10 December 2023; pp. 9248–9274. [Google Scholar] GLM, T.; Zeng, A.; Xu, B.; Wang, B.; Zhang, C.; Yin, D.; Zhang, D.; Rojas, D.; Feng, G.; Zhao, H.; et al. Chatglm: A family of large language models from glm-130b to glm-4 all tools. arXiv 2024, arXiv:2406.12793. [Google Scholar] Xiao, S.; Liu, Z.; Zhang, P.; Muennighof, N. C-pack: Packaged resources to advance general chinese embedding. arXiv 2023, arXiv:2309.07597. [Google Scholar] Merity, S.; Xiong, C.; Bradbury, J.; Socher, R. Pointer Sentinel Mixture Models. In Proceedings of the International Conference on Learning Representations, Virtual, 25–29 April 2022. [Google Scholar] Zhou, A.; Yan, K.; Shlapentokh-Rothman, M.; Wang, H.; Wang, Y.X. Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models. In Proceedings of the Forty-first International Conference on Machine Learning, Honolulu, HI, USA, 23–29 July 2023. [Google Scholar] Figure 1. Two challenges faced by the RAG Framework: (i) Left: The inference process is disturbed by irrelevant knowledge in the retrieved results. (ii) Right: The complex associations among the knowledge in the retrieved results cannot be analyzed and understood. Figure 1. Two challenges faced by the RAG Framework: (i) Left: The inference process is disturbed by irrelevant knowledge in the retrieved results. (ii) Right: The complex associations among the knowledge in the retrieved results cannot be analyzed and understood. Figure 2. Overview of CRP-RAG framework. The CRP-RAG consists of three modules: (i) The GC module constructs the reasoning graph based on the query. (ii) The KRA module performs knowledge retrieval and aggregation based on the nodes of the reasoning graph. (iii) The AG module generates a query-based answer leveraging the reasoning graph and the relevant knowledge. Figure 2. Overview of CRP-RAG framework. The CRP-RAG consists of three modules: (i) The GC module constructs the reasoning graph based on the query. (ii) The KRA module performs knowledge retrieval and aggregation based on the nodes of the reasoning graph. (iii) The AG module generates a query-based answer leveraging the reasoning graph and the relevant knowledge. Figure 3. Robustness analysis of CRP-RAG. The blue line represents the experimental results of introducing false reasoning graph nodes into CRP-RAG, while the red line indicates the experimental results of introducing knowledge-irrelevant reasoning graph nodes into CRP-RAG. Figure 3. Robustness analysis of CRP-RAG. The blue line represents the experimental results of introducing false reasoning graph nodes into CRP-RAG, while the red line indicates the experimental results of introducing knowledge-irrelevant reasoning graph nodes into CRP-RAG. Figure 4. CRP-RAG discards distracted reasoning paths and abstains from answering when no valid reasoning path is available. Figure 4. CRP-RAG discards distracted reasoning paths and abstains from answering when no valid reasoning path is available. Figure 5. Impact of different reasoning structures on CRP-RAG behavior. Figure 5. Impact of different reasoning structures on CRP-RAG behavior. Figure 6. Average time consumption of CRP-RAG under different levels of reasoning complexity. Figure 6. Average time consumption of CRP-RAG under different levels of reasoning complexity. Table 1. Overall experiment results on three tasks. The best performance under the same dataset and evaluation metrics is indicated in bold, while the second-best performance is underlined. Table 1. Overall experiment results on three tasks. The best performance under the same dataset and evaluation metrics is indicated in bold, while the second-best performance is underlined. Open Domain Question Answering Multi-Hop Reasoning Question Answering Fact Varifing NQ TriviaQA(TQA) WebQuestions(WQA) HotPotQA 2WikiMultiHopQA FEVER EM F1 Acc-LM EM F1 Acc-LM EM F1 Acc-LM EM F1 Acc-LM EM F1 Acc-LM Acc-LM Vanilla LLMs GLM-4-Plus 33.0 44.2 55.4 68.2 78.9 83.2 14.4 24.1 31.2 20.4 38.9 51.1 27.3 36.7 50.4 67.1 GLM-4-Plus w ToT 39.0 50.9 59.3 72.1 85.8 85.0 25.3 37.3 48.3 34.0 47.8 57.2 28.1 40.5 53.6 68.4 RALM Framework RALMs 44.5 54.0 58.2 69.9 77.0 80.6 45.2 61.0 73.6 37.2 53.4 62.0 31.7 49.0 56.7 72.0 Query Decomposition RALM Framework IRCoT 50.0 58.2 68.8 70.2 81.9 80.8 51.4 65.4 76.8 48.2 60.7 71.3 46.8 58.0 68.4 72.9 ITER-RETGEN 56.4 66.8 71.4 72.6 86.0 84.4 60.2 75.8 81.2 45.8 61.1 73.4 36.0 47.4 58.5 71.5 Knowledge Structure RALM Framework RAPTOR 60.1 68.5 77.8 73.6 80.9 83.9 57.8 65.2 79.1 60.3 73.1 81.5 39.6 55.3 66.8 66.6 GraphRAG 42.6 51.6 62.1 72.1 83.0 81.6 51.5 60.4 75.5 56.0 68.9 76.3 38.7 51.8 60.9 71.6 Self-Planning RALM Framework Think-then-Act 56.0 65.7 69.9 74.7 80.7 84.8 55.9 69.5 79.0 56.9 65.8 79.8 52.6 68.7 76.6 76.9 Self-RAG 59.2 66.3 70.0 76.3 80.1 79.3 58.2 69.0 77.4 67.4 80.1 86.0 57.6 69.4 79.1 80.8 Ours CRP-RAG 63.2 71.1 82.3 79.7 86.4 87.0 62.5 75.6 85.2 81.0 87.6 87.4 69.3 77.9 81.0 85.0 Table 2. Ablation study results. Table 2. Ablation study results. HotPotQA FEVER CRP-RAG 87.4 85.0 CRP-RAG w/o Knowledge Aggregation 84.9 83.4 CRP-RAG w/o Knowledge Evaluation 62.9 64.1 CRP-RAG w/o Iterative Reasoning 74.5 72.6 Table 3. Results of the perplexity experiment. The best performance under the same dataset and evaluation metrics is indicated in bold, while the second-best performance is underlined. Table 3. Results of the perplexity experiment. The best performance under the same dataset and evaluation metrics is indicated in bold, while the second-best performance is underlined. HotPotQA FEVER Vanilla LLMs GLM-4-Plus 786.2 771.9 GLM4-Plus w ToT 247.7 883.4 RALM Framework RALMs 201.1 558.8 Query Decomposition RALM Framework IRCoT 208.2 608.6 ITER-RETGEN 593.9 1094.8 Knowledge Structure RALM Framework RAPTOR 124.0 477.5 GraphRAG 236.0 794.6 Self-Planning RALM Framework Think-then-Act 156.8 330.6 Self-RAG 112.0 116.5 Ours CRP-RAG 21.4 8.1 Table 4. Results of the factual consistency experiment. The best performance under the same dataset and evaluation metrics is indicated in bold, while the second-best performance is underlined. Table 4. Results of the factual consistency experiment. The best performance under the same dataset and evaluation metrics is indicated in bold, while the second-best performance is underlined. HotPotQA FEVER RALM Framework RALMs 66.7 69.5 Query Decomposition RALM Framework IRCoT 72.1 75.5 ITER-RETGEN 74.2 75.6 Knowledge Structure RALM Framework RAPTOR 78.8 71.9 GraphRAG 79.0 73.4 Self-Planning RALM Framework Think-then-Act 81.9 80.7 Self-RAG 85.2 83.1 Ours CRP-RAG 92.5 91.8 Table 5. Experimental results of CRP-RAG performance in resource-constrained environments. Table 5. Experimental results of CRP-RAG performance in resource-constrained environments. HotPotQA FEVER EM F1 Acc-LM Acc-LM The Best Performance of Baseline 67.4 80.1 86.0 80.8 CRP-RAG (Resource Intensive) 81.0 87.6 87.4 85.0 CRP-RAG (Resource Constrained) 79.1 81.9 84.4 83.5 Table 6. Impact of reasoning structure on CRP-RAG framework performance. Table 6. Impact of reasoning structure on CRP-RAG framework performance. HotPotQA FEVER CRP-RAG 87.4 85.0 CRP-RAG (Tree) 75.8 78.0 CRP-RAG (Chain) 69.0 69.8 CRP-RAG (Text chunk) 65.7 67.6 Table 7. Experimental results for overall efficiency evaluation, with data separated by “/” indicating minimum number of invocations/average number of invocations/maximum number of invocations. Table 7. Experimental results for overall efficiency evaluation, with data separated by “/” indicating minimum number of invocations/average number of invocations/maximum number of invocations. CRP-RAG Self-RAG LATS Number of Invocations 3/4.16/13 1/4.25/30+ 3/5.67/30+ Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content. © 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Share and Cite MDPI and ACS Style Xu, K.; Zhang, K.; Li, J.; Huang, W.; Wang, Y. CRP-RAG: A Retrieval-Augmented Generation Framework for Supporting Complex Logical Reasoning and Knowledge Planning. Electronics 2025, 14, 47. https://doi.org/10.3390/electronics14010047 AMA Style Xu K, Zhang K, Li J, Huang W, Wang Y. CRP-RAG: A Retrieval-Augmented Generation Framework for Supporting Complex Logical Reasoning and Knowledge Planning. Electronics. 2025; 14(1):47. https://doi.org/10.3390/electronics14010047 Chicago/Turabian Style Xu, Kehan, Kun Zhang, Jingyuan Li, Wei Huang, and Yuanzhuo Wang. 2025. "CRP-RAG: A Retrieval-Augmented Generation Framework for Supporting Complex Logical Reasoning and Knowledge Planning" Electronics 14, no. 1: 47. https://doi.org/10.3390/electronics14010047 APA Style Xu, K., Zhang, K., Li, J., Huang, W., & Wang, Y. (2025). CRP-RAG: A Retrieval-Augmented Generation Framework for Supporting Complex Logical Reasoning and Knowledge Planning. Electronics, 14(1), 47. https://doi.org/10.3390/electronics14010047 Note that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further details here. Article Metrics No No Article Access Statistics For more information on the journal statistics, click here. Multiple requests from the same IP address are counted as one view. Zoom | Orient | As Lines | As Sticks | As Cartoon | As Surface | Previous Scene | Next Scene Cite Export citation file: BibTeX | EndNote | RIS MDPI and ACS Style Xu, K.; Zhang, K.; Li, J.; Huang, W.; Wang, Y. CRP-RAG: A Retrieval-Augmented Generation Framework for Supporting Complex Logical Reasoning and Knowledge Planning. Electronics 2025, 14, 47. https://doi.org/10.3390/electronics14010047 AMA Style Xu K, Zhang K, Li J, Huang W, Wang Y. CRP-RAG: A Retrieval-Augmented Generation Framework for Supporting Complex Logical Reasoning and Knowledge Planning. Electronics. 2025; 14(1):47. https://doi.org/10.3390/electronics14010047 Chicago/Turabian Style Xu, Kehan, Kun Zhang, Jingyuan Li, Wei Huang, and Yuanzhuo Wang. 2025. "CRP-RAG: A Retrieval-Augmented Generation Framework for Supporting Complex Logical Reasoning and Knowledge Planning" Electronics 14, no. 1: 47. https://doi.org/10.3390/electronics14010047 APA Style Xu, K., Zhang, K., Li, J., Huang, W., & Wang, Y. (2025). CRP-RAG: A Retrieval-Augmented Generation Framework for Supporting Complex Logical Reasoning and Knowledge Planning. Electronics, 14(1), 47. https://doi.org/10.3390/electronics14010047 Note that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further details here. clear Electronics, EISSN 2079-9292, Published by MDPI RSS Content Alert Further Information Article Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers For Editors For Librarians For Publishers For Societies For Conference Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook Twitter Subscribe to receive issue release notifications and newsletters from MDPI journals Acoustics Acta Microbiologica Hellenica Actuators Adhesives Administrative Sciences Adolescents Advances in Respiratory Medicine Aerobiology Aerospace Agriculture AgriEngineering Agrochemicals Agronomy AI Air Algorithms Allergies Alloys Analytica Analytics Anatomia Anesthesia Research Animals Antibiotics Antibodies Antioxidants Applied Biosciences Applied Mechanics Applied Microbiology Applied Nano Applied Sciences Applied System Innovation AppliedChem AppliedMath Aquaculture Journal Architecture Arthropoda Arts Astronomy Atmosphere Atoms Audiology Research Automation Axioms Bacteria Batteries Behavioral Sciences Beverages Big Data and Cognitive Computing BioChem Bioengineering Biologics Biology Biology and Life Sciences Forum Biomass Biomechanics BioMed Biomedicines BioMedInformatics Biomimetics Biomolecules Biophysica Biosensors Biosphere BioTech Birds Blockchains Brain Sciences Buildings Businesses C Cancers Cardiogenetics Catalysts Cells Ceramics Challenges ChemEngineering Chemistry Chemistry Proceedings Chemosensors Children Chips CivilEng Clean Technologies Climate Clinical and Translational Neuroscience Clinical Bioenergetics Clinics and Practice Clocks & Sleep Coasts Coatings Colloids and Interfaces Colorants Commodities Complications Compounds Computation Computer Sciences & Mathematics Forum Computers Condensed Matter Conservation Construction Materials Corrosion and Materials Degradation Cosmetics COVID Craniomaxillofacial Trauma & Reconstruction Crops Cryo Cryptography Crystals Current Issues in Molecular Biology Current Oncology Dairy Data Dentistry Journal Dermato Dermatopathology Designs Diabetology Diagnostics Dietetics Digital Disabilities Diseases Diversity DNA Drones Drugs and Drug Candidates Dynamics Earth Ecologies Econometrics Economies Education Sciences Electricity Electrochem Electronic Materials Electronics Emergency Care and Medicine Encyclopedia Endocrines Energies Energy Storage and Applications Eng Engineering Proceedings Entropy Environmental and Earth Sciences Proceedings Environments Epidemiologia Epigenomes European Burn Journal European Journal of Investigation in Health, Psychology and Education Fermentation Fibers FinTech Fire Fishes Fluids Foods Forecasting Forensic Sciences Forests Fossil Studies Foundations Fractal and Fractional Fuels Future Future Internet Future Pharmacology Future Transportation Galaxies Games Gases Gastroenterology Insights Gastrointestinal Disorders Gastronomy Gels Genealogy Genes Geographies GeoHazards Geomatics Geometry Geosciences Geotechnics Geriatrics Glacies Gout, Urate, and Crystal Deposition Disease Grasses Green Health Hardware Healthcare Hearts Hemato Hematology Reports Heritage Histories Horticulturae Hospitals Humanities Humans Hydrobiology Hydrogen Hydrology Hygiene Immuno Infectious Disease Reports Informatics Information Infrastructures Inorganics Insects Instruments Intelligent Infrastructure and Construction International Journal of Environmental Research and Public Health International Journal of Financial Studies International Journal of Molecular Sciences International Journal of Neonatal Screening International Journal of Orofacial Myology and Myofunctional Therapy International Journal of Plant Biology International Journal of Topology International Journal of Translational Medicine International Journal of Turbomachinery, Propulsion and Power International Medical Education Inventions IoT ISPRS International Journal of Geo-Information J Journal of Ageing and Longevity Journal of Cardiovascular Development and Disease Journal of Clinical & Translational Ophthalmology Journal of Clinical Medicine Journal of Composites Science Journal of Cybersecurity and Privacy Journal of Dementia and Alzheimer's Disease Journal of Developmental Biology Journal of Experimental and Theoretical Analyses Journal of Eye Movement Research Journal of Functional Biomaterials Journal of Functional Morphology and Kinesiology Journal of Fungi Journal of Imaging Journal of Intelligence Journal of Low Power Electronics and Applications Journal of Manufacturing and Materials Processing Journal of Marine Science and Engineering Journal of Market Access & Health Policy Journal of Mind and Medical Sciences Journal of Molecular Pathology Journal of Nanotheranostics Journal of Nuclear Engineering Journal of Otorhinolaryngology, Hearing and Balance Medicine Journal of Parks Journal of Personalized Medicine Journal of Pharmaceutical and BioTech Industry Journal of Respiration Journal of Risk and Financial Management Journal of Sensor and Actuator Networks Journal of the Oman Medical Association Journal of Theoretical and Applied Electronic Commerce Research Journal of Vascular Diseases Journal of Xenobiotics Journal of Zoological and Botanical Gardens Journalism and Media Kidney and Dialysis Kinases and Phosphatases Knowledge LabMed Laboratories Land Languages Laws Life Limnological Review Lipidology Liquids Literature Livers Logics Logistics Lubricants Lymphatics Machine Learning and Knowledge Extraction Machines Macromol Magnetism Magnetochemistry Marine Drugs Materials Materials Proceedings Mathematical and Computational Applications Mathematics Medical Sciences Medical Sciences Forum Medicina Medicines Membranes Merits Metabolites Metals Meteorology Methane Methods and Protocols Metrics Metrology Micro Microbiology Research Micromachines Microorganisms Microplastics Microwave Minerals Mining Modelling Modern Mathematical Physics Molbank Molecules Multimodal Technologies and Interaction Muscles Nanoenergy Advances Nanomanufacturing Nanomaterials NDT Network Neuroglia Neurology International NeuroSci Nitrogen Non-Coding RNA Nursing Reports Nutraceuticals Nutrients Obesities Oceans Onco Optics Oral Organics Organoids Osteology Oxygen Parasitologia Particles Pathogens Pathophysiology Pediatric Reports Pets Pharmaceuticals Pharmaceutics Pharmacoepidemiology Pharmacy Philosophies Photochem Photonics Phycology Physchem Physical Sciences Forum Physics Physiologia Plants Plasma Platforms Pollutants Polymers Polysaccharides Populations Poultry Powders Proceedings Processes Prosthesis Proteomes Psychiatry International Psychoactives Psychology International Publications Quantum Beam Science Quantum Reports Quaternary Radiation Reactions Real Estate Receptors Recycling Regional Science and Environmental Economics Religions Remote Sensing Reports Reproductive Medicine Resources Rheumato Risks Robotics Ruminants Safety Sci Scientia Pharmaceutica Sclerosis Seeds Sensors Separations Sexes Signals Sinusitis Smart Cities Social Sciences Société Internationale d’Urologie Journal Societies Software Soil Systems Solar Solids Spectroscopy Journal Sports Standards Stats Stresses Surfaces Surgeries Surgical Techniques Development Sustainability Sustainable Chemistry Symmetry SynBio Systems Targets Taxonomy Technologies Telecom Textiles Thalassemia Reports Therapeutics Thermo Time and Space Tomography Tourism and Hospitality Toxics Toxins Transplantology Trauma Care Trends in Higher Education Tropical Medicine and Infectious Disease Universe Urban Science Uro Vaccines Vehicles Venereology Veterinary Sciences Vibration Virtual Worlds Viruses Vision Waste Water Wild Wind Women World World Electric Vehicle Journal Youth Zoonotic Diseases Subscribe © 1996-2025 MDPI (Basel, Switzerland) unless otherwise stated Disclaimer Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content. Terms and Conditions Privacy Policy We use cookies on our website to ensure you get the best experience. Read more about our cookies here. Accept Share Link Copy clear Share https://www.mdpi.com/3107196 clear Back to TopTop

Source: https://learn.microsoft.com/en-us/azure/developer/ai/advanced-retrieval-augmented-generation
Build Advanced Retrieval-Augmented Generation Systems | Microsoft Learn Skip to main content This browser is no longer supported. Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support. Download Microsoft Edge More info about Internet Explorer and Microsoft Edge Table of contents Exit focus mode Read in English Save Table of contents Read in English Save Add to plan Edit Share via Facebook x.com LinkedIn Email Print Table of contents Build advanced retrieval-augmented generation systems Article 01/15/2025 4 contributors Feedback In this article This article explores retrieval-augmented generation (RAG) in depth. We describe the work and considerations that are required for developers to create a production-ready RAG solution. To learn about two options for building a "chat over your data" application, one of the top use cases for generative AI in businesses, see Augment LLMs with RAG or fine-tuning. The following diagram depicts the steps or phases of RAG: This depiction is called naive RAG. It's a useful way to initially understand the mechanisms, roles, and responsibilities that are required to implement a RAG-based chat system. But a real-world implementation has many more preprocessing and post-processing steps to prepare the articles, queries, and responses for use. The following diagram is a more realistic depiction of a RAG, sometimes called advanced RAG: This article provides a conceptual framework for understanding the preprocessing and post-processing phases in a real-world RAG-based chat system: Ingestion phase Inference pipeline phase Evaluation phase Ingestion Ingestion is primarily about storing your organization's documents so that they can be easily retrieved to answer a user's question. The challenge is ensuring that the portions of the documents that best match the user's query are located and used during inference. Matching is accomplished primarily through vectorized embeddings and a cosine similarity search. However, matching is facilitated by understanding the nature of the content (for example, patterns and form) and the data organization strategy (the structure of the data when it's stored in the vector database). For ingestion, developers need to consider the following steps: Content preprocessing and extraction Chunking strategy Chunking organization Update strategy Content preprocessing and extraction Clean and accurate content is one of the best ways to improve the overall quality of a RAG-based chat system. To get clean, accurate content, start by analyzing the shape and form of the documents to be indexed. Do the documents conform to specified content patterns like documentation? If not, what types of questions might the documents answer? At a minimum, create steps in the ingestion pipeline to: Standardize text formats Handle special characters Remove unrelated, outdated content Account for versioned content Account for content experience (tabs, images, tables) Extract metadata Some of this information (like metadata, for example) might be useful if it's kept with the document in the vector database to use during the retrieval and evaluation process in the inference pipeline. It also can be combined with the text chunk to persuade the chunk's vector embedding. Chunking strategy As a developer, you must decide how to break up a larger document into smaller chunks. Chunking can improve the relevance of the supplemental content that's sent to the LLM to accurately answer user queries. Also consider how to use the chunks after retrieval. System designers should research common industry techniques, and do some experimentation. You can even test your strategy in a limited capacity in your organization. Developers must consider: Chunk size optimization: Determine the ideal chunk size, and how to designate a chunk. By section? By paragraph? By sentence? Overlapping and sliding window chunks: Determine whether to divide the content into discrete chunks, or will the chunks overlap? You can even do both, in a sliding window design. Small2Big: When chunking is done at a granular level like a single sentence, is the content organized so that it's easy to find the neighboring sentences or the paragraph that contains the sentence? Retrieving this information and supplying it to the LLM might provide it with more context to answer user queries. For more information, see the next section. Chunking organization In a RAG system, strategically organizing your data in the vector database is a key to efficient retrieval of relevant information to augment the generation process. Here are the types of indexing and retrieval strategies you might consider: Hierarchical indexes: This approach involves creating multiple layers of indexes. A top-level index (a summary index) quickly narrows down the search space to a subset of potentially relevant chunks. A second-level index (a chunks index) provides more detailed pointers to the actual data. This method can significantly speed up the retrieval process because it reduces the number of entries to scan in the detailed index by first filtering through the summary index. Specialized indexes: Depending on the nature of the data and the relationships between chunks, you might use specialized indexes like graph-based or relational databases: Graph-based indexes are useful when the chunks have interconnected information or relationships that can enhance retrieval, such as citation networks or knowledge graphs. Relational databases can be effective if the chunks are structured in a tabular format. Use SQL queries to filter and retrieve data based on specific attributes or relationships. Hybrid indexes: A hybrid approach combines multiple indexing methods to apply their strengths to your overall strategy. For example, you might use a hierarchical index for initial filtering and a graph-based index to dynamically explore relationships between chunks during retrieval. Alignment optimization To enhance the relevance and accuracy of the retrieved chunks, align them closely with the question or query types they answer. One strategy is to generate and insert a hypothetical question for each chunk that represents the question the chunk is best suited to answer. This helps in several ways: Improved matching: During retrieval, the system can compare the incoming query with these hypothetical questions to find the best match to improve the relevance of chunks that are fetched. Training data for machine learning models: These pairings of questions and chunks can be training data to improve the machine learning models that are the underlying components of the RAG system. The RAG system learns which types of questions are best answered by each chunk. Direct query handling: If a real user query closely matches a hypothetical question, the system can quickly retrieve and use the corresponding chunk and speed up the response time. Each chunk's hypothetical question acts like a label that guides the retrieval algorithm, so it's more focused and contextually aware. This kind of optimization is useful when the chunks cover a wide range of information topics or types. Update strategies If your organization indexes documents that are frequently updated, it's essential to maintain an updated corpus to ensure that the retriever component can access the most current information. The retriever component is the logic in the system that runs the query against the vector database, and then returns results. Here are some strategies for updating the vector database in these types of systems: Incremental updates: Regular intervals: Schedule updates at regular intervals (for example, daily or weekly) depending on the frequency of document changes. This method ensures that the database is periodically refreshed on a known schedule. Trigger-based updates: Implement a system in which an update triggers reindexing. For example, any modification or addition of a document automatically initiates reindexing in the affected sections. Partial updates: Selective reindexing: Instead of reindexing an entire database, update only the changed corpus parts. This approach can be more efficient than full reindexing, especially for large datasets. Delta encoding: Store only the differences between the existing documents and their updated versions. This approach reduces the data processing load by avoiding the need to process unchanged data. Versioning: Snapshotting: Maintain document corpus versions at different points in time. This technique provides a backup mechanism and allows the system to revert to or refer to previous versions. Document version control: Use a version control system to systematically track document changes for maintaining the change history and simplifying the update process. Real-time updates: Stream processing: When information timeliness is critical, use stream processing technologies for real-time vector database updates as changes are made to the document. Live querying: Instead of relying solely on preindexed vectors, use a live data query approach for up-to-date responses, possibly combining live data with cached results for efficiency. Optimization techniques: Batch processing: Batch processing accumulates changes to apply less frequently to optimize resources and reduce overhead. Hybrid approaches: Combine various strategies: Use incremental updates for minor changes. Use full reindexing for major updates. Document structural changes that are made to the corpus. Choosing the right update strategy or the right combination depends on specific requirements, including: Document corpus size Update frequency Real-time data needs Resource availability Evaluate these factors based on the needs of the specific application. Each approach has trade-offs in complexity, cost, and update latency. Inference pipeline Your articles are chunked, vectorized, and stored in a vector database. Now, turn your focus to resolving completion challenges. To get the most accurate and efficient completions, you must account for many factors: Is the user's query written in a way to get the results the user is looking for? Does the user's query violate any of the organization's policies? How do you rewrite the user's query to improve the chances of finding the closest matches in the vector database? How do you evaluate query results to ensure that the article chunks align to the query? How do you evaluate and modify query results before you pass them into the LLM to ensure that the most relevant details are included in the completion? How do you evaluate the LLM's response to ensure that the LLM's completion answers the user's original query? How do you ensure that the LLM's response complies with the organization's policies? The entire inference pipeline runs in real time. There isn't one right way to design your preprocessing and post-processing steps. You likely will choose a combination of programming logic and other LLM calls. One of the most important considerations is the trade-off between building the most accurate and compliant pipeline possible and the cost and latency required to make it happen. Let's identify specific strategies in each stage of the inference pipeline. Query preprocessing steps Query preprocessing occurs immediately after the user submits their query: The goal of these steps is to make sure that the user asks questions that are within the scope of your system and to prepare the user's query to increase the likelihood that it locates the best possible article chunks by using the cosine similarity or "nearest neighbor" search. Policy check: This step involves logic that identifies, removes, flags, or rejects certain content. Some examples include removing personal data, removing expletives, and identifying "jailbreak" attempts. Jailbreaking refers to user attempts to circumvent or manipulate the built-in safety, ethical, or operational guidelines of the model. Query rewriting: This step might be anything from expanding acronyms and removing slang to rephrasing the question to ask it more abstractly to extract high-level concepts and principles (step-back prompting). A variation on step-back prompting is Hypothetical Document Embeddings (HyDE). HyDE uses the LLM to answer the user's question, creates an embedding for that response (the hypothetical document embedding), and then uses the embedding to run a search against the vector database. Subqueries The subqueries processing step is based on the original query. If the original query is long and complex, it can be useful to programmatically break it into several smaller queries, and then combine all the responses. For example, a question about scientific discoveries in physics might be: "Who made more significant contributions to modern physics, Albert Einstein or Niels Bohr?" Breaking down complex queries into subqueries make them more manageable: Subquery 1: "What are the key contributions of Albert Einstein to modern physics?" Subquery 2: "What are the key contributions of Niels Bohr to modern physics?" The results of these subqueries detail the major theories and discoveries by each physicist. For example: For Einstein, contributions might include the theory of relativity, the photoelectric effect, and E=mc^2. For Bohr, contributions might include Bohr's model of the hydrogen atom, Bohr's work on quantum mechanics, and Bohr's principle of complementarity. When these contributions are outlined, they can be assessed to determine more subqueries. For example: Subquery 3: "How have Einstein's theories impacted the development of modern physics?" Subquery 4: "How have Bohr's theories impacted the development of modern physics?" These subqueries explore each scientist's influence on physics, such as: How Einstein's theories led to advancements in cosmology and quantum theory How Bohr's work contributed to understanding atomic structure and quantum mechanics Combining the results of these subqueries can help the language model form a more comprehensive response about who made more significant contributions to modern physics based on their theoretical advancements. This method simplifies the original complex query by accessing more specific, answerable components, and then synthesizing those findings into a coherent answer. Query router Your organization might choose to divide its corpus of content into multiple vector stores or into entire retrieval systems. In that scenario, you can use a query router. A query router selects the most appropriate database or index to provide the best answers to a specific query. A query router typically works at a point after the user formulates the query, but before it sends the query to retrieval systems. Here's a simplified workflow for a query router: Query analysis: The LLM or another component analyzes the incoming query to understand its content, context, and the type of information that is likely needed. Index selection: Based on the analysis, the query router selects one or more indexes from potentially several available indexes. Each index might be optimized for different types of data or queries. For example, some indexes might be more suited to factual queries. Other indexes might excel in providing opinions or subjective content. Query dispatch: The query is dispatched to the selected index. Results aggregation: Responses from the selected indexes are retrieved and possibly aggregated or further processed to form a comprehensive answer. Answer generation: The final step involves generating a coherent response based on the retrieved information, possibly integrating or synthesizing content from multiple sources. Your organization might use multiple retrieval engines or indexes for the following use cases: Data type specialization: Some indexes might specialize in news articles, others in academic papers, and yet others in general web content or specific databases like for medical or legal information. Query type optimization: Certain indexes might be optimized for quick factual lookups (for example, dates or events). Others might be better to use for complex reasoning tasks or for queries that require a deep domain knowledge. Algorithmic differences: Different retrieval algorithms might be used in different engines, such as vector-based similarity searches, traditional keyword-based searches, or more advanced semantic understanding models. Imagine a RAG-based system that's used in a medical advisory context. The system has access to multiple indexes: A medical research paper index optimized for detailed and technical explanations A clinical case study index that provides real-world examples of symptoms and treatments A general health information index for basic queries and public health information If a user asks a technical question about the biochemical effects of a new drug, the query router might prioritize the medical research paper index due to its depth and technical focus. For a question about typical symptoms of a common illness, however, the general health index might be chosen for its broad and easily understandable content. Post-retrieval processing steps Post-retrieval processing occurs after the retriever component retrieves relevant content chunks from the vector database: With candidate content chunks retrieved, the next step is to validate the article chunk usefulness when augmenting the LLM prompt before preparing the prompt to be presented to the LLM. Here are some prompt aspects to consider: Including too much supplement information might result in ignoring the most important information. Including irrelevant information might negatively influence the answer. Another consideration is the needle in a haystack problem, a term that refers to a known quirk of some LLMs in which the content at the beginning and end of a prompt have greater weight to the LLM than the content in the middle. Finally, consider the LLM's maximum context window length and the number of tokens required to complete extraordinarily long prompts (especially for queries at scale). To deal with these issues, a post-retrieval processing pipeline might include the following steps: Filtering results: In this step, ensure that the article chunks that are returned by the vector database are relevant to the query. If they aren't, the result is ignored when the LLM prompt is composed. Re-ranking: Rank the article chunks that are retrieved from the vector store to ensure that relevant details are near the edges (the beginning and the end) of the prompt. Prompt compression: Use a small, inexpensive model to compress and summarize multiple article chunks into a single compressed prompt before sending the prompt to the LLM. Post-completion processing steps Post-completion processing occurs after the user's query and all content chunks are sent to the LLM: Accuracy validation occurs after the LLM's prompt completion. A post-completion processing pipeline might include the following steps: Fact check: The intent is to identify specific claims made in the article that are presented as facts, and then to check those facts for accuracy. If the fact check step fails, it might be appropriate to requery the LLM in hopes of getting a better answer or to return an error message to the user. Policy check: The last line of defense to ensure that answers don't contain harmful content, whether for the user or for the organization. Evaluation Evaluating the results of a nondeterministic system isn't as simple as running the unit tests or integration tests most developers are familiar with. You need to consider several factors: Are users satisfied with the results they're getting? Are users getting accurate responses to their questions? How do you capture user feedback? Do you have any policies in place that limit what data you can collect about user data? For diagnosis of unsatisfactory responses, do you have visibility into all the work that went into answering the question? Do you keep a log of each stage in the inference pipeline of inputs and outputs so that you can perform root cause analysis? How can you make changes to the system without regression or degradation of results? Capturing and acting on feedback from users As described earlier, you might need to work with your organization's privacy team to design feedback capture mechanisms, telemetry, and logging for forensics and root cause analysis of a query session. The next step is to develop an assessment pipeline. An assessment pipeline helps with the complexity and time-intensive nature of analyzing verbatim feedback and the root causes of the responses provided by an AI system. This analysis is crucial because it involves investigating every response to understand how the AI query produced the results, checking the appropriateness of the content chunks that are used from documentation, and the strategies employed in dividing up these documents. It also involves considering any extra preprocessing or post-processing steps that might enhance the results. This detailed examination often uncovers content gaps, particularly when no suitable documentation exists for response to a user's query. Building an assessment pipeline becomes essential to manage the scale of these tasks effectively. An efficient pipeline uses custom tooling to evaluate metrics that approximate the quality of answers provided by AI. This system streamlines the process of determining why a specific answer was given to a user's question, which documents were used to generate that answer, and the effectiveness of the inference pipeline that processes the queries. Golden dataset One strategy to evaluate the results of a nondeterministic system like a RAG chat system is to use a golden dataset. A golden dataset is a curated set of questions and approved answers, metadata (like topic and type of question), references to source documents that can serve as ground truth for answers, and even variations (different phrasings to capture the diversity of how users might ask the same questions). A golden dataset represents the "best case scenario." Developers can evaluate the system to see how well it performs, and then do regression tests when they implement new features or updates. Assessing harm Harms modeling is a methodology aimed at foreseeing potential harms, spotting deficiencies in a product that might pose risks to individuals, and developing proactive strategies to mitigate such risks. A tool designed for assessing the impact of technology, particularly AI systems, would feature several key components based on the principles of harms modeling as outlined in the provided resources. Key features of a harms evaluation tool might include: Stakeholder identification: The tool might help users identify and categorize various stakeholders that are affected by the technology, including direct users, indirectly affected parties, and other entities, like future generations or nonhuman factors, such as environmental concerns. Harm categories and descriptions: The tool might include a comprehensive list of potential harms, such as privacy loss, emotional distress, or economic exploitation. The tool might guide the user through various scenarios, illustrate how the technology might cause these harms, and help evaluate both intended and unintended consequences. Severity and probability assessments: The tool might help users assess the severity and probability of each identified harm. The user can prioritize issues to address first. Examples include qualitative assessments supported by data where available. Mitigation strategies: The tool can suggest potential mitigation strategies after it identifies and evaluates harms. Examples include changes to the system design, adding safeguards, and alternative technological solutions that minimize identified risks. Feedback mechanisms: The tool should incorporate mechanisms for gathering feedback from stakeholders so that the harms evaluation process is dynamic and responsive to new information and perspectives. Documentation and reporting: For transparency and accountability, the tool might facilitate detailed reports that document the harms assessment process, findings, and potential risk mitigation actions taken. These features can help you identify and mitigate risks, but they also help you design more ethical and responsible AI systems by considering a broad spectrum of impacts from the start. For more information, see these articles: Foundations of assessing harm Types of harm Testing and verifying the safeguards This article outlines several processes that are aimed at mitigating the possibility of a RAG-based chat system being exploited or compromised. Red-teaming plays a crucial role in ensuring that the mitigations are effective. Red-teaming involves simulating the actions of a potential adversary to uncover potential weaknesses or vulnerabilities in the application. This approach is especially vital in addressing the significant risk of jailbreaking. Developers need to rigorously assess RAG-based chat system safeguards under various guideline scenarios to effectively test and verify them. This approach not only ensures robustness, but also helps you fine-tune the system’s responses to strictly adhere to defined ethical standards and operational procedures. Final considerations for application design Here's a short list of things to consider and other takeaways from this article that might affect your application design decisions: Acknowledge the nondeterministic nature of generative AI in your design. Plan for variability in outputs and set up mechanisms to ensure consistency and relevance in responses. Assess the benefits of preprocessing user prompts against the potential increase in latency and costs. Simplifying or modifying prompts before submission might improve response quality, but it might add complexity and time to the response cycle. To enhance performance, investigate strategies for parallelizing LLM requests. This approach might reduce latency, but it requires careful management to avoid increased complexity and potential cost implications. If you want to start experimenting with building a generative AI solution immediately, we recommend that you take a look at Get started with chat by using your own data sample for Python. The tutorial is also available for .NET, Java, and JavaScript. Feedback Was this page helpful? Yes No Get help at Microsoft Q&A Additional resources California Consumer Privacy Act (CCPA) Opt-Out Icon Your Privacy Choices Theme Light Dark High contrast Previous Versions Blog Contribute Privacy Terms of Use Trademarks © Microsoft 2025 Additional resources In this article California Consumer Privacy Act (CCPA) Opt-Out Icon Your Privacy Choices Theme Light Dark High contrast Previous Versions Blog Contribute Privacy Terms of Use Trademarks © Microsoft 2025

Source: https://ai.plainenglish.io/the-future-of-retrieval-augmented-generation-evolving-architectures-and-advanced-techniques-2b61c2afce6f
The Future of Retrieval Augmented Generation: Evolving Architectures and Advanced Techniques | by Eric Risco | Artificial Intelligence in Plain English Open in app Sign up Sign in Write Sign up Sign in Member-only story The Future of Retrieval Augmented Generation: Evolving Architectures and Advanced Techniques Eric Risco · Follow Published in Artificial Intelligence in Plain English · 9 min read · Oct 8, 2024 -- Share Retrieval Augmented Generation (RAG) has been a game-changer in how Large Language Models (LLMs) tackle complex, knowledge-driven tasks. By enriching language models with external data, RAG allows them to overcome one of their primary limitations: the inability to access real-time, up-to-date information. However, with the rapid evolution of AI architectures, RAG itself is transforming, and the techniques we use to optimize it are evolving just as quickly. In this post, we’ll explore both the foundational and advanced versions of RAG, while introducing practical prompts that can elevate how RAG interacts with user queries, context extraction, and content generation. Standard RAG: The Foundation The Standard RAG workflow has been instrumental in enhancing LLMs by augmenting them with relevant external knowledge. Here’s how it works: Document Indexing: External documents are loaded from databases (SQL, NoSQL, vector databases) and indexed offline into a vector store. Query Processing: When a user inputs a… -- -- Follow Published in Artificial Intelligence in Plain English 18.4K Followers ·Last published 1 day ago New AI, ML and Data Science articles every day. Follow to join our 3.5M+ monthly readers. Follow Follow Written by Eric Risco 744 Followers ·183 Following Software Engineer Follow No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams

Results for query: Optimizing search performance in large codebases with AST techniques

Search Engine: Bing
Source: https://link.springer.com/chapter/10.1007/978-3-662-70140-9_4
Evaluation of Code Similarity Search Strategies in Large-Scale Codebases | SpringerLink Skip to main content Advertisement Log in Menu Find a journal Publish with us Track your research Search Cart Home Transactions on Large-Scale Data- and Knowledge-Centered Systems LVII Chapter Evaluation of Code Similarity Search Strategies in Large-Scale Codebases Chapter First Online: 25 October 2024 pp 99–113 Cite this chapter Transactions on Large-Scale Data- and Knowledge-Centered Systems LVII Jorge Martinez-Gil 9 & Shaoyi Yin10 Part of the book series: Lecture Notes in Computer Science ((TLDKS,volume 14970)) 64 Accesses Abstract The ability to automatically identify similar code fragments within huge code repositories is crucial for software development and maintenance tasks such as code reuse and debugging. Although several solutions already exist to face this challenge, not many comparisons have yet been established. For this reason, this study presents a comparative analysis of existing and emerging techniques for code similarity search. We benchmark these methods across diverse codebases, examining metrics such as indexing time, search speed, and the semantic relevance of retrieved code fragments. Our research aims to provide software developers with practical information for performing efficient code similarity searches, addressing the challenges associated with the increasing size of codebases. This is a preview of subscription content, log in via an institution to check access. Access this chapter Log in via an institution Subscribe and save Springer+ Basic €32.70 /Month Get 10 units per month Download Article/Chapter or eBook 1 Unit = 1 Article or 1 Chapter Cancel anytime Subscribe now Buy Now Chapter EUR 29.95 Price includes VAT (Belgium) Available as PDF Read on any device Instant download Own it forever Buy Chapter eBook EUR 51.35 Price includes VAT (Belgium) Available as EPUB and PDF Read on any device Instant download Own it forever Buy eBook Softcover Book EUR 63.59 Price includes VAT (Belgium) Compact, lightweight edition Dispatched in 3 to 5 business days Free shipping worldwide - see info Buy Softcover Book Tax calculation will be finalised at checkout Purchases are for personal use only Institutional subscriptions Similar content being viewed by others Source Code Clone Search Chapter © 2021 A comparison of code similarity analysers Article Open access 25 October 2017 Source Code Clone Detection Using Unsupervised Similarity Measures Chapter © 2024 Notes 1. https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Clone-detection-BigCloneBench. References Ain, Q.U., Butt, W.H., Anwar, M.W., Azam, F., Maqbool, B.: A systematic review on code clone detection. IEEE Access 7, 86121–86144 (2019) Article Google Scholar Aumüller, M., Bernhardsson, E., Faithfull, A.: ANN-benchmarks: a benchmarking tool for approximate nearest neighbor algorithms. Inf. Syst. 87, 101374 (2020) Article Google Scholar Douze, M., et al.: The Faiss library (2024) Google Scholar Feng, Z., et al.: CodeBERT: a pre-trained model for programming and natural languages. In: Cohn, T., He, Y., Liu, Y. (eds.) Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16–20 November 2020, volume EMNLP 2020 of Findings of ACL, pp. 1536–1547. Association for Computational Linguistics (2020) Google Scholar Gabel, M., Jiang, L., Su, Z.: Scalable detection of semantic clones. In: Proceedings of the 30th International Conference on Software Engineering, pp. 321–330 (2008) Google Scholar Gormley, C., Tong, Z.: Elasticsearch: The Definitive Guide: A Distributed Real-Time Search and Analytics Engine. O’Reilly Media Inc, Sebastopol (2015) Google Scholar Guo, D., et al.: GraphcodeBERT: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366 (2020) Guo, R., et al.: Accelerating large-scale inference with anisotropic vector quantization. In: International Conference on Machine Learning, pp. 3887–3896. PMLR (2020) Google Scholar Hackeling, G.: Mastering Machine Learning with Scikit-Learn. Packt Publishing Ltd, Birmingham (2017) Google Scholar Haque, S., Eberhart, Z., Bansal, A., McMillan, C.: Semantic similarity metrics for evaluating source code summarization. In: Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension, pp. 36–47 (2022) Google Scholar Higo, Y., Ueda, Y., Kamiya, T., Kusumoto, S., Inoue, K.: On software maintenance process improvement based on code clone analysis. In: Oivo, M., Komi-Sirviö, S. (eds.) PROFES 2002. LNCS, vol. 2559, pp. 185–197. Springer, Heidelberg (2002). https://doi.org/10.1007/3-540-36209-6_17 Chapter Google Scholar Inoue, K., Miyamoto, Y., German, D.M., Ishio, T.: Finding code-clone snippets in large source-code collection by ccgrep. In: Taibi, D., Lenarduzzi, V., Kilamo, T., Zacchiroli, S. (eds.) OSS 2021. IAICT, vol. 624, pp. 28–41. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-75251-4_3 Chapter Google Scholar Juergens, E., Deissenboeck, F., Hummel, B., Wagner, S.: Do code clones matter? In: 2009 IEEE 31st International Conference on Software Engineering, pp. 485–495. IEEE (2009) Google Scholar Karmakar, A., Robbes, R.: What do pre-trained code models know about code? In: 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE), pp. 1332–1336. IEEE (2021) Google Scholar Karnalim, O.: TF-IDF inspired detection for cross-language source code plagiarism and collusion. Comput. Sci. 21 (2020) Google Scholar Karnalim, O., et al.: Explanation in code similarity investigation. IEEE Access 9, 59935–59948 (2021) Article Google Scholar Malkov, Y.A., Yashunin, D.A.: Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Trans. Pattern Anal. Mach. Intell. 42(4), 824–836 (2018) Article Google Scholar Martinez-Gil, J.: A comprehensive review of stacking methods for semantic similarity measurement. Mach. Learn. Appl. 10, 100423 (2022) Google Scholar Martinez-Gil, J.: Source code clone detection using unsupervised similarity measures. In: Bludau, P., Ramler, R., Winkler, D., Bergsmann, J. (eds.) SWQD 2024. LNBIP, vol. 505, pp. 21–37. Springer, Cham (2024). https://doi.org/10.1007/978-3-031-56281-5_2 Chapter Google Scholar Novak, M., Joy, M., Kermek, D.: Source-code similarity detection and detection tools used in academia: a systematic review. ACM Trans. Comput. Educ. (TOCE) 19(3), 1–37 (2019) Article Google Scholar Roy, C.K., Cordy, J.R., Koschke, R.: Comparison and evaluation of code clone detection techniques and tools: a qualitative approach. Sci. Comput. Program. 74(7), 470–495 (2009) Article MathSciNet Google Scholar Saini, N., Singh, S., et al.: Code clones: detection and management. Procedia Comput. Sci. 132, 718–727 (2018) Article Google Scholar Satter, A., Sakib, K.: A similarity-based method retrieval technique to improve effectiveness in code search. In: Companion Proceedings of the 1st International Conference on the Art, Science, and Engineering of Programming, pp. 1–3 (2017) Google Scholar Spotify. Annoy. https://github.com/spotify/annoy, May 2023. Approximate Nearest Neighbors in C++/Python optimized for memory usage and loading/saving to disk Tronícek, Z.: Indexing source code and clone detection. Inf. Softw. Technol. 144, 106805 (2022) Article Google Scholar Download references Acknowledgments The authors thank the anonymous reviewers for their help in improving the work. The research reported in this paper has been funded by the Federal Ministry for Climate Action, Environment, Energy, Mobility, Innovation, and Technology (BMK), the Federal Ministry for Digital and Economic Affairs (BMDW), and the State of Upper Austria in the frame of SCCH, a center in the COMET - Competence Centers for Excellent Technologies Programme managed by Austrian Research Promotion Agency FFG. Author information Authors and Affiliations Software Competence Center Hagenberg GmbH, Softwarepark 32a, 4232, Hagenberg, Austria Jorge Martinez-Gil Paul Sabatier University, IRIT Laboratory, 118 route de Narbonne, Toulouse, France Shaoyi Yin Authors Jorge Martinez-Gil View author publications You can also search for this author in PubMed Google Scholar Shaoyi Yin View author publications You can also search for this author in PubMed Google Scholar Corresponding author Correspondence to Jorge Martinez-Gil . Editor information Editors and Affiliations IRIT, Paul Sabatier University, Toulouse, France Abdelkader Hameurlain IFS, Vienna University of Technology, Vienna, Austria A Min Tjoa Rights and permissions Reprints and permissions Copyright information © 2025 The Author(s), under exclusive license to Springer-Verlag GmbH, DE, part of Springer Nature About this chapter Cite this chapter Martinez-Gil, J., Yin, S. (2025). Evaluation of Code Similarity Search Strategies in Large-Scale Codebases. In: Hameurlain, A., Tjoa, A.M. (eds) Transactions on Large-Scale Data- and Knowledge-Centered Systems LVII. Lecture Notes in Computer Science(), vol 14970. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-662-70140-9_4 Download citation .RIS .ENW .BIB DOI: https://doi.org/10.1007/978-3-662-70140-9_4 Published: 25 October 2024 Publisher Name: Springer, Berlin, Heidelberg Print ISBN: 978-3-662-70142-3 Online ISBN: 978-3-662-70140-9 eBook Packages: Computer ScienceComputer Science (R0) Share this chapter Anyone you share the following link with will be able to read this content: Get shareable link Sorry, a shareable link is not currently available for this article. Copy to clipboard Provided by the Springer Nature SharedIt content-sharing initiative Publish with us Policies and ethics Access this chapter Log in via an institution Subscribe and save Springer+ Basic €32.70 /Month Get 10 units per month Download Article/Chapter or eBook 1 Unit = 1 Article or 1 Chapter Cancel anytime Subscribe now Buy Now Chapter EUR 29.95 Price includes VAT (Belgium) Available as PDF Read on any device Instant download Own it forever Buy Chapter eBook EUR 51.35 Price includes VAT (Belgium) Available as EPUB and PDF Read on any device Instant download Own it forever Buy eBook Softcover Book EUR 63.59 Price includes VAT (Belgium) Compact, lightweight edition Dispatched in 3 to 5 business days Free shipping worldwide - see info Buy Softcover Book Tax calculation will be finalised at checkout Purchases are for personal use only Institutional subscriptions Search Search by keyword or author Search Navigation Find a journal Publish with us Track your research Discover content Journals A-Z Books A-Z Publish with us Journal finder Publish your research Open access publishing Products and services Our products Librarians Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility statement Terms and conditions Privacy policy Help and support Cancel contracts here 178.117.73.144 Not affiliated © 2025 Springer Nature

Source: https://ast-grep.github.io/advanced/how-ast-grep-works.html
How ast-grep Works: A bird's-eye view | ast-grep Skip to content ast-grep SearchK Main Navigation Guide Reference Command Line Interface Rule Config Rule Object Playground Manual Resources FAQ Rule Examples Custom Language Contributing BlogPlayground Appearance Menu Return to top Sidebar Navigation Guide Quick Start Pattern Syntax Rule Essentials Atomic Rule Relational Rule Composite Rule Utility Rule Project Setup Project Configuration Lint Rule Test Your Rule Error Report Rewrite Code Transform Code Rewriter Rule Tooling Overview Editor Integration JSON mode API Usage JavaScript API Python API Performance Tip Examples C C++ Go HTML Java Kotlin Python Ruby Rust TypeScript TSX YAML Reference Command Line Interface ast-grep run ast-grep scan ast-grep test ast-grep new Project Config Rule Config fix transformation rewriter Rule Object API Reference Language List Playground Manual Advanced Topics Frequently Asked Questions How ast-grep Works Core Concepts Pattern Syntax Pattern Match Algorithm How Rewrite Works Custom Language Support Multi-Language Documents Comparison with Other Tools Contributing Guide Development Add New Language Links Playground Blog VSCode Discord StackOverflow Reddit Docs.rs On this page How ast-grep Works: A bird's-eye view In the world of software development, efficiently searching, rewriting, linting, and analyzing code is essential for maintaining high-quality projects. This is where ast-grep comes into play. Designed as a powerful structural search tool, ast-grep simplifies these tasks by leveraging the Abstract Syntax Tree (AST) representation of code. Let's break down how ast-grep works with the help of a diagram. The Workflow of ast-grep Generally speaking, ast-grep takes user queries of various input formats, parses the code into an AST using TreeSitter, and performs search, rewrite, lint, and analysis, utilizing the full power of CPU cores. Query via Various Formats ast-grep can accept queries in multiple formats, making it flexible and user-friendly. Here are some common query formats: Pattern Query: Users can define specific patterns to search for within their codebase. YAML Rule: Structured rules written in YAML format to guide the search and analysis process. API Code: Direct API calls for more programmatic control over the searching and rewriting tasks. ast-grep's Core ast-grep's core functionality is divided into two main components: parsing and matching. 1. Parsing with Tree-Sitter The core of ast-grep's functionality relies on Tree-Sitter Parsers. TreeSitter is a powerful parsing library that converts source code into an Abstract Syntax Tree (AST). This tree structure represents the syntactic structure of the code, making it easier to analyze and manipulate. 2. Tree Matching Once the code is parsed into an AST, the ast-grep core takes over and finds the matching AST nodes based on the input queries. Written in Rust, ast-grep ensures efficient performance by utilizing full CPU cores. This means it can handle large codebases and perform complex searches and transformations quickly. Usage Scenarios ast-grep can be helpful for these scenarios. Search: Find specific patterns or constructs within the code. Rewrite: Automatically refactor or transform code based on predefined rules or patterns. Lint: Identify and report potential issues or code smells. Analyze: Perform in-depth code analysis to gather insights and metrics. Benefits of Using ast-grep Multi-Core Processing: ast-grep can handle multiple files in parallel by taking full advantage of multi-core processors. Typically ast-grep performs tasks faster than many other tools, making it suitable for large projects. Versatility: Whether you need to search for a specific code pattern, rewrite sections of code, lint for potential issues, or perform detailed analysis, ast-grep has you covered. Example in the Real World Pattern + Search: CodeRabbit uses ast-grep patterns to search code repo for code review knowledge. This example is collected from ast-grep's own dogfooding. API + Rewrite: @vue-macros/cli is a CLI for rewriting at Vue Macros powered by ast-grep. YAML + Lint: Vercel turbo is using ast-grep to lint their Rust code with custom rules. Conclusion ast-grep is a versatile and efficient tool for modern software development needs. By parsing code into an Abstract Syntax Tree and leveraging the power of Rust, it provides robust capabilities for searching, rewriting, linting, and analyzing code. With multiple input formats and the ability to utilize full CPU cores, ast-grep is designed to handle the demands of today's complex codebases. Whether you are maintaining a small project or a large enterprise codebase, ast-grep can help streamline your development workflow. Edit this page Last updated: Pager Previous pageFrequently Asked Questions Next pageCore Concepts Made with ❤️ with Rust Copyright © 2022-present Herrington Darkholme

Source: https://dbshisode.medium.com/unlocking-code-improvement-with-abstract-syntax-trees-asts-8b63de0b5d3d
Unlocking Code Improvement with Abstract Syntax Trees (ASTs) | by Deepak Shisode | Medium Open in app Sign up Sign in Write Sign up Sign in Unlocking Code Improvement with Abstract Syntax Trees (ASTs) Deepak Shisode · Follow 4 min read · Oct 10, 2024 -- Listen Share An Abstract Syntax Tree (AST) is a tree representation of the abstract syntactic structure of source code written in a programming language. Each node of the tree denotes a construct occurring in the source code. ASTs are compelling tools for code analysis, transformation, and optimization. By manipulating the AST of code, developers can automate code improvements, enforce coding standards, and even refactor code programmatically. In this answer, we’ll explore practical examples of using ASTs to improve code, focusing on: Code Analysis and Linting Code Refactoring Code Optimization Automated Testing and Debugging We’ll use Python for code examples, leveraging its built-in ast module, but the concepts apply to other programming languages with AST support. 1. Code Analysis and Linting Use Case: Detecting anti-patterns or enforcing code style guidelines. Example: Identifying unused imports or variables in Python code. Solution: Here’s how you can use AST to analyze code and find unused imports. import ast from collections import defaultdict    class ImportUsageVisitor(ast.NodeVisitor):      def __init__(self):          self.imports = {}          self.used_names = set()        def visit_Import(self, node):          for alias in node.names:              self.imports[alias.asname or alias.name] = node.lineno        def visit_ImportFrom(self, node):          module = node.module          for alias in node.names:              name = alias.asname or alias.name              full_name = f"{module}.{name}" if module else name              self.imports[name] = node.lineno        def visit_Name(self, node):          self.used_names.add(node.id)    def find_unused_imports(code):      tree = ast.parse(code)      visitor = ImportUsageVisitor()      visitor.visit(tree)      unused = set(visitor.imports.keys()) - visitor.used_names      for name in unused:          lineno = visitor.imports[name]          print(f"Unused import '{name}' at line {lineno}")    # Example usage:  code = """  import os  import sys  from collections import defaultdict    def foo():      print(os.name)  """    find_unused_imports(code) Output: Unused import 'defaultdict' at line 4 Unused import 'sys' at line 3 Explanation: We parse the code into an AST. ImportUsageVisitor collects all imported names and tracks where they are used. After traversal, we compare the imported names with the used names to find unused imports. 2. Code Refactoring Use Case: Automating repetitive refactoring tasks. Example: Renaming variables across a codebase while ensuring scope correctness. Solution: pip install astor import ast  import astor  # Third-party library to convert AST back to code    class VariableRenamer(ast.NodeTransformer):      def __init__(self, old_name, new_name):          self.old_name = old_name          self.new_name = new_name        def visit_Name(self, node):          if node.id == self.old_name:              return ast.copy_location(ast.Name(id=self.new_name, ctx=node.ctx), node)          return node    code = """  def compute_area(radius):      pi = 3.1415      area = pi * radius ** 2      return area  """    tree = ast.parse(code)  renamer = VariableRenamer('radius', 'r')  modified_tree = renamer.visit(tree)  modified_code = astor.to_source(modified_tree)  print(modified_code) Output: def compute_area(radius):     pi = 3.1415     area = pi * r ** 2     return area Explanation: VariableRenamer walks the AST and replaces instances of old_name with new_name. By using AST, we ensure that only variable names are changed, not strings or comments. 3. Code Optimization Use Case: Simplifying constant expressions to improve performance. Example: Folding constant expressions at compile time. Solution: import ast    class ConstantFolder(ast.NodeTransformer):      def visit_BinOp(self, node):          self.generic_visit(node)          if (isinstance(node.left, ast.Constant) and              isinstance(node.right, ast.Constant)):              result = eval(compile(ast.Expression(node), '', 'eval'))              return ast.copy_location(ast.Constant(value=result), node)          return node    code = """  def calculations():      x = 2 * 3      y = 4 + x      return y  """    tree = ast.parse(code)  folder = ConstantFolder()  optimized_tree = folder.visit(tree)  optimized_code = ast.unparse(optimized_tree)  print(optimized_code) Output: def calculations():     x = 6     y = 4 + x     return y Explanation: ConstantFolder evaluates binary operations with constant operands and replaces them with the result. This can reduce runtime computation, shifting it to compile time. 4. Automated Testing and Debugging Use Case: Instrumenting code to track execution or inject test hooks. Example: Adding logging statements before function calls. Solution: import ast    class FunctionCallLogger(ast.NodeTransformer):      def visit_Call(self, node):          self.generic_visit(node)          log_call = ast.Expr(value=ast.Call(              func=ast.Name(id='print', ctx=ast.Load()),              args=[ast.Constant(value=f"Calling function {ast.unparse(node.func)}")],              keywords=[]          ))          return [log_call, node]    code = """  def add(a, b):      return a + b    def multiply(x, y):      return x * y    result = multiply(add(2, 3), 4)  """    tree = ast.parse(code)  logger = FunctionCallLogger()  instrumented_tree = logger.visit(tree) instrumented_code = ast.unparse(instrumented_tree) print(instrumented_code) Output: def add(a, b):     return a + b def multiply(x, y):     return x * y result = print('Calling function multiply')multiply( print('Calling function add'), add(2, 3), 4) Explanation: FunctionCallLogger inserts a print statement before each function call. This can help in debugging by tracing function calls during execution. Additional Applications Security Analysis: Detecting vulnerable patterns, such as SQL injections or insecure function use. Code Formatters: Tools like Black (for Python) use ASTs to format code consistently. Language Transpilation: Converting code from one programming language to another by translating ASTs. Benefits of Using AST Language-Aware Transformation: AST manipulations understand the syntax and structure, reducing errors that occur with regex or string-based transformations. Safety and Accuracy: By operating on the AST, you avoid altering code in comments, strings, or unintended places. Automation: ASTs enable programmatic code transformations, which are scalable across large codebases. Differences Between AST Manipulation and Reflection Does AST Use Reflection Techniques? No, working with an Abstract Syntax Tree (AST) does not inherently involve using reflection techniques. While both AST manipulation and reflection allow a program to examine aspects of its own code, they operate at different stages and have different purposes in programming. Abstract Syntax Trees provide a powerful way to introspect and manipulate code programmatically. By leveraging ASTs, developers can automate code improvements, enforce standards, and optimize performance. Whether you’re building a linter, an automatic refactoring tool, or performing static analysis, ASTs are an indispensable tool in a developer’s toolkit Refection Abstract Syntax Tree Code Improvement Code Analysis Linting -- -- Follow Written by Deepak Shisode 1 Follower ·1 Following Follow No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams

Source: https://bytegoblin.io/blog/optimize-ast-grep-to-get-10x-faster.mdx
Optimize ast-grep to Get 10X Faster Spee... | ByteGoblin.io ByteGoblin.io RUST Optimize ast-grep to Get 10X Faster Speed Introduction In the world of software development, the tools we use can significantly impact our productivity. ast-grep is one of those tools that can revolutionize how developers analyze and manipulate abstract syntax trees (ASTs) in codebases. However, to harness its full potential, it’s crucial to optimize its performance. In this article, we will discuss various strategies to optimize ast-grep, enabling you to achieve up to 10X faster speed in your code analysis tasks. Understanding ast-grep Before delving into optimization techniques, it’s essential to understand what ast-grep does. ast-grep is a powerful tool for pattern matching within the abstract syntax trees of your code. It allows developers to search for and transform code more efficiently than traditional text-based tools. Key Features of ast-grep Pattern Matching: Find code snippets based on their structure rather than text. Code Transformation: Easily modify code across large codebases. Multi-language Support: Works with various programming languages by leveraging their ASTs. Factors Affecting Performance Several factors can hinder the performance of ast-grep. Understanding these factors is the first step toward optimization. Language Parsing Overhead: Parsing source code into ASTs can be resource-intensive. Large Codebase Size: Searching through extensive repositories increases processing time. Inefficient Patterns: Complex and poorly structured queries can slow down search times. Strategies to Optimize ast-grep 1. Efficient Query Design Efficient query design is crucial to speed up ast-grep. Here are some tips for creating optimized queries: Start Simple: Begin with simple patterns and gradually increase complexity only if necessary. Use Specificity: Be as specific as possible with node types to narrow down search results. For example, instead of searching for all function definitions, target a particular function type.   {
    "type": "FunctionDeclaration",
    "name": {
      "type": "Identifier",
      "name": "myFunction"
    }
  }
 Avoid Wildcards: Excessive use of wildcards can slow down queries. Instead, define clear criteria. 2. Leverage Caching Mechanisms Implementing caching can drastically reduce processing times. Here’s how: Cache Results: Store results from previous queries to avoid repeated computations. Cache ASTs: Cache parsed ASTs to prevent rediscovery of the same structure across multiple runs. 3. Optimize Code Parsing Reducing the parsing overhead can lead to significant speed improvements: Incremental Parsing: Instead of parsing the entire codebase repeatedly, parse only the changes made since the last analysis. Parallel Parsing: Utilize multi-threading to parse different files simultaneously. This can leverage multi-core processors efficiently. 4. Profile and Benchmark To understand where the bottlenecks are, profiling your tool is paramount: Use Profiling Tools: Use tools like gprof, valgrind, or py-spy to identify slow functions in your execution flow. Set Performance Benchmarks: Establish benchmarks based on codebase size and complexity to measure improvements over time. 5. Optimize Resource Usage Efficient resource management can contribute to speed improvements: Memory Management: Ensure you’re not holding on to large structures longer than needed. Release memory after processing files to keep memory consumption low. Optimize Disk I/O: Read files in larger chunks rather than line by line to minimize system calls. 6. Explore Concurrency Taking advantage of concurrency can massively boost performance: Use Async Processing: Implement asynchronous operations for I/O-bound tasks to prevent blocking. Task Queues: Create a task queue for processing files and distribute the workload over multiple threads or processes. Real-World Example Let’s look at a practical example. Imagine you need to refactor or analyze a large JavaScript codebase. Following the above optimization strategies, you create a specific query targeting a limited set of function definitions and employ caching for previous results. After these optimizations: Initial Search Time: Was 30 seconds. After Optimization: Reduced to about 3 seconds. By fine-tuning your queries and caching results, you achieved a 10X increase in speed! Conclusion Optimizing ast-grep not only enhances the speed of analysis but also improves overall productivity in managing and understanding complex codebases. By implementing efficient query designs, leveraging caching, optimizing parsing techniques, and utilizing concurrent processing, you can make significant performance gains. Explore these strategies, apply them in your development workflow, and enjoy faster code analyses and transformations. With these optimizations, you’ll be well on your way to mastering ast-grep. Keywords: ast-grep optimization speed up ast-grep improve ast-grep performance abstract syntax tree analysis efficient code queries caching in ast-grep concurrency in code analysis By staying proactive with optimizations, you can ensure that your journey through code analysis remains swift and efficient. Happy coding! Suggested Articles RUST Learning Rust - Part 5— Traits, Generics and Closures Learning Rust - Part 5— Traits, Generics and Closures RUST Learning Rust - Polling Learning Rust - Polling RUST An Introduction to COW in Rust An Introduction to COW in Rust RUST Zero-to-Hero with Solana Token 2022 — Transfer Hook 🪝 Zero-to-Hero with Solana Token 2022 — Transfer Hook 🪝 RUST An Almost Religious Case for Rust An Almost Religious Case for Rust RUST Optimizing Blockchain with Merkelized Abstract Syntax Trees in Rust Optimizing Blockchain with Merkelized Abstract Syntax Trees in Rust

Source: https://ast-grep.github.io/blog/code-search-design-space.html
Design Space for Code Search Query | ast-grep Skip to content ast-grep SearchK Main Navigation Guide Reference Command Line Interface Rule Config Rule Object Playground Manual Resources FAQ Rule Examples Custom Language Contributing BlogPlayground Appearance Menu Return to top Sidebar Navigation Blog List Homepage VSCode Discord StackOverflow Reddit Docs.rs On this page Design Space for Code Search Query Code search is a critical tool for modern software development. It enables developers to quickly locate, understand, and reuse existing code, boosting productivity and ensuring code consistency across projects. At its core, ast-grep is a code search tool. Its other features, such as linting and code rewriting, are built upon the foundation of its code search capabilities. This blog post delves into the design space of code search, with a particular focus on how queries are designed and used. We'll be drawing inspiration from the excellent paper, "Code Search: A Survey of Techniques for Finding Code". But we won't be covering every single detail from that paper. Instead, our focus will be on the diverse ways that code search tools allow users to express their search intent. Query Design and Query Types Every code search begins with a query, which is simply a way to tell the search engine what kind of code we're looking for. The way these queries are designed is crucial. Code search tool designers aim to achieve several key goals: Easy A query should be easy to write, allowing users to quickly search without needing extensive learning. If it's too difficult to write a query, people might get discouraged from using the tool altogether. Expressive Users should be able to express whatever they're looking for. If the query language is too limited, you simply cannot find some results. Precise The query should be specific enough to yield relevant results, avoiding irrelevant findings. An imprecise query will lead to a lot of noise. Achieving all three of these goals simultaneously is challenging, as they often pull in opposing directions. For example, a very simple and easy query language might be expressive enough, or a very precise query language might be too complex for the average user. How do code search tools balance these goals? The blog categorizes code search queries into a few main types, each with its own characteristics: informal queries, formal queries, and hybrid queries. Informal Queries These queries are closest to how we naturally express ourselves, and can be further divided into: Free-Form Queries These are often free-form, using natural language to describe the desired code functionality, like web search. For example, "read file line by line" or "FileReader close." Pros: Easy for users to formulate, similar to using a web search engine, and highly expressive. Cons: Can be ambiguous and less precise due to the nature of natural language and potential vocabulary mismatches between the query and the code base. Tools like GitHub Copilot use this approach. Input-Output Examples These queries specify the desired behavior of the code by providing input-output pairs. You specify the desired behavior using pairs of inputs and their corresponding outputs. For example, the input "susie@mail.com" should result in the output "susie". Pros: Allows to precisely specify desired behavior Cons: May require some effort to provide sufficient examples This approach is more common in academic research than practical tools. This blog has not been aware of open source tools that use this approach. We will not discuss informal queries in detail, as it is not precise. Formal Queries Based on Existing Programming Languages Formal queries use a structured approach, making them more precise. They can be further divided into several subcategories. Plain Code The simplest version involves providing an exact code snippet that needs to be matched in the codebase. For instance, a user might search for instances of the following Java snippet: java try {
  File file = File.createTempFile("foo", "bar");
} catch (IOException e) {
} Not many tools directly support plain code search. They usually break search queries into smaller parts through the tokenization process, like traditional search engines. A notable example may be grep.app. Code with Holes This approach involves providing code snippets with placeholders to search for code fragments. For example, a user might search for the following pattern in Java: java public void actionClose (JButton a, JFrame f) {
 $$$BODY
} Here, $$$BODY is a placeholder, and the code search engine will try to locate all matching code. ast-grep falls into this category, treating the query as an Abstract Syntax Tree (AST) with holes. The holes in ast-grep are called metavariables. Other tools like gritql and the structural search feature in IntelliJ IDEA also use this technique. Code with Pattern Matching Symbols These queries make use of special symbols to represent and match code structures. For example, the following query in Comby attempts to find all if statements where the condition is a comparison. comby if (:[var] <= :[rest]) In Comby, the :[var] and :[rest] are special markers that match strings of code. java if (width <= 1280 && height <= 800) {
  return 1;
} The :[var] matches any string until a <= character is found and in this case is width. :[rest] matches everything that follows, 1280 && height <= 800. Unlike ast-grep, Comby is not AST-aware, as the :[rest] in the example spans across multiple AST nodes. Tools like Comby and Shisho use this approach. Pros and Cons Pros: Easy to formulate for developers familiar with programming languages. Cons: Parsing incomplete code snippets can be a challenge. The downside of using existing languages is also emphasized in the IntelliJ IDEA documentation: Any (SSR) template entered should be a well formed Java construction ... An off-the-shelf grammar of the programming language may not be able to parse a query because the query is incomplete or ambiguous. For example, "key": "value" is not a valid JSON object, a JSON parser will reject and will fail to create a query. Maybe it is clear to a human that it is a key-value pair, but the parser does not know that. Other examples will be like distinguishing function calls and macro invocation in C/C++. TIP ast-grep takes a unique approach to this problem. It uses a pattern object to represent and disambiguate a complete and valid code snippet, and then leverages a selector to extract the part that matches the query. Formal Queries using Custom Languages Significant Extensions of Existing Programming Languages These languages extend existing programming languages with features like wildcard tokens or regular expression operators. For example, the pattern $(if $$ else $) $+ might be used to find all nested if-else statements in a codebase. Coccinelle and Semgrep are tools that take this approach. Semgrep's pattern-syntax, for example, has extensive features such as ellipsis metavariables, typed metavariables, and deep expression operators, that cannot be parsed by a standard programming language' implementation. Ellipsis MetavariablesTyped MetavariablesDeep Expression operators yaml # combine ellipses and metavariables to match a sequence of ASTs
# note the ellipsis is not valid programming language syntax
pattern: foo($...ARGS, 3, $...ARGS)
# this pattern will match foo(1, 2, 3, 4, 5) yaml #  look for calls to the log method on Logger objects.
# A simple pattern like this will match `Math.log()` as well
pattern: $LOGGER.log(...)
# typed metavariable can put a type constraint on the metavariable
# but it is no longer valid Java code
pattern: (java.util.logging.Logger $LOGGER).log(...) yaml # Use the deep expression operator <... [your_pattern] ...>
# to match an expression that
# could be deeply nested within another expression
pattern: |
  if <... $USER.is_admin() ...>:
    ... Pros: These languages can be more expressive than plain programming languages. Cons: Users need to learn new syntax and semantics and tool developers to support the extension Difference from ast-grep Note ast-grep also supports multi meta variables in the form of $$$VARS. Compared to Semgrep, ast-grep's metavariables still produce valid code snippets. We can represent also search query using Domain Specific Language Logic-based Querying Languages These languages utilize first-order logic or languages like Datalog to express code properties. For example, a user can find all classes with the name "HelloWorld". Some of these languages also resemble SQL. CodeQL and Glean are two notable examples. Here is an example from CodeQL: sql from If ifstmt, Stmt pass
where pass = ifstmt.getStmt(0) and
  pass instanceof Pass
select ifstmt, "This 'if' statement is redundant." This CodeQL query will identify redundant if statements in Python, where the first statement within the if block is a pass statement. Explaination of the query from If ifstmt, Stmt pass: This part of the query defines two variables, ifstmt and pass, which will be used in the query. where pass = ifstmt.getStmt(0) and pass instanceof Pass: This part of the query filters the results. It checks if the first statement in the ifstmt is a Pass statement. select ifstmt, "This 'if' statement is redundant.": This part of the query selects the results. It returns the ifstmt and a message. Pros: These languages can precisely express complex code properties beyond syntax. Cons: Learning curve is steep. Embedded Domain Specific Language Embedded DSLs are using the host language to express the query. The query is embedded in the host language, and the host language provides the necessary constructs to express the query. The query is then parsed and interpreted by the tool. There are further two flavors of embedded DSLs: configuration-based and program-based. Configuration-based eDSL Configuration-based eDSLs allow user to provide configuration objects that describes the query. The tool then interprets this configuration object to perform the search. ast-grep CLI and semgrep CLI both adopt this approach using YAML files. ast-grep YAML ruleSemgrep YAML rule yaml id: match-function-call
language: c
rule:
  pattern:
    context: $M($$$);
    selector: call_expression yaml rules:
  - id: my-pattern-name
    pattern: |
      TODO
    message: "Some message to display to the user"
    languages: [python]
    severity: ERROR Configuration files are more expressive than patterns and still relatively easy to write. Users usually already know the host language (YAML) and can leverage its constructs to express the query. Program-based eDSL Program-based eDSLs provide direct access to the AST through AST node objects. Examples of programmatic APIs include JSCodeshift, the Code Property Graph from Joern, and ast-grep's NAPI. @ast-grep/napiJSCodeshiftCode Property Graph typescript import { parse, Lang } from '@ast-grep/napi'

let source = `console.log("hello world")`
const ast = parse(Lang.JavaScript, source)  // 1. parse the source
const root = ast.root()                     // 2. get the root
const node = root.find('console.log($A)')   // 3. find the node
node.getMatch('A').text()                   // 4. collect the info
// "hello world" javascript const j = require('jscodeshift');

const root = j(`const a = 1; const b = 2;`);

const types = root.find(j.VariableDeclarator).getTypes();
console.log(types); // Set { 'VariableDeclarator' } scala import io.shiftleft.codepropertygraph.Cpg
import io.shiftleft.semanticcpg.language._

object FindExecCalls {
  def main(args: Array[String]): Unit = {
    // Load the C codebase
    val cpg: Cpg = Cpg.apply("path/to/your/codebase")

    // Find all `exec` function calls and print their locations
    cpg.call("exec").location.l.foreach(println)
  }
} Pros: Offer more precision and expressiveness and are relatively easy to write. Cons: The overhead to communicate between the host language and the search tool can be high. General Purpose Like Programming Language Finally, tools can also design their own general purpose programming languages. These languages provide a full programming language to describe code properties. GritQL is an example of this approach. For example, this GritQL query rewrites all console.log calls to winston.debug and all console.error calls to winston.warn: gritql `console.$method($msg)` => `winston.$method($msg)` where {
  $method <: or {
    `log` => `debug`,
    `error` => `warn`
  }
} Explaination of the Query Pattern Matching: The pattern console.$method($msg) is used to match code where there is a console object with a method ($method) and an argument ($msg). Here, $method and $msg are placeholders for any method and argument, respectively. Rewrite: The rewrite symbole => specifies that the matched console code should be transformed to use winston, followed by the method ($method) and the argument ($msg). Method Mapping: The where clause specifies additional constraints on the rewrite. Specifically, $method <: or { 'log' => 'debug', 'error' => 'warn' } means: If $method is log, it should be transformed to debug. If $method is error, it should be transformed to warn. In sum, this rule replaces console logging methods with their corresponding Winston logging methods: console.log('message') becomes winston.debug('message') console.error('message') becomes winston.warn('message') Pros: It offers more precision and expressiveness compared to simple patterns and configuration-based embedded DSLs. But it may not be as flexible as program-based eDSL nor as powerful as logic-based languages. Cons: Have the drawback of requiring users to learn the custom language first. It is easier to learn than logic-based languages, but still requires some learning compared to using embedded DSL. Hybrid Queries Hybrid queries combine multiple query types. For example, you can combine free-form queries with input-output examples, or combine natural language queries with program element references. ast-grep is a great example of a tool that uses hybrid queries. You can define patterns directly in a YAML rule or use a programmatic API. First, you can embed the pattern in the YAML rule, like this: yaml rule:
  pattern: console.log($A)
  inside:
    kind: function_declaration You can also use the similar concept in the programmatic API typescript import { Lang, parse } from '@ast-grep/napi'

const sg = parse(Lang.JavaScript, code)
sg.root().find({
  rule: {
    pattern: 'console.log($A)',
    inside: {
      kind: 'function_declaration'
    }
  }
}) This flexible design allows you to combine basic queries into larger, more complex ones, and you can always use a general-purpose language for very complex and specific searches. ast-grep favors existing programming languages We don't want the user to learn a new language, but rather use the existing language constructs to describe the query. We also think TypeScript is a great language with great type system. There is no need to reinvent a new language to express code search logic. ast-grep's Design Choices Designing a code search tool involves a delicate balancing act. It's challenging to simultaneously achieve ease of use, expressiveness, and precision, as these goals often conflict. Code search tools must carefully navigate these trade-offs to meet the diverse needs of their users. ast-grep makes specific choices to address this challenge: Prioritizing Familiarity: It uses pattern matching based on existing programming language syntax, making it easy for developers to start using the tool with familiar coding structures. Extending with Flexibility: It incorporates configuration-based (YAML) and program-based (NAPI) embedded DSLs, providing additional expressiveness for complex searches. Hybrid, and Progressive, Design: Its pattern matching, YAML rules, and NAPI are designed for hybrid use, allowing users to start simple and gradually add complexity. The concepts in each API are also transferable, enabling users to progressively learn more advanced techniques. AST-Based Precision: It emphasizes precision by requiring all queries to be AST-based, ensuring accurate results. Though it comes with the trade-off that queries should be carefully crafted. Multi-language Support: Instead of creating a new query language for all programming languages or significantly extending existing ones for code search purposes, which would be an enormous undertaking, ast-grep reuses the familiar syntax of the existing programming languages in its patterns. This makes the tool more approachable for developers working across multiple languages. Additional Considerations While we've focused on query design, there are other factors that influence the effectiveness of code search tools. These include: Offline Indexing: This is crucial for rapid offline searching. Currently, ast-grep always builds an AST in memory for each query, meaning it doesn't support offline indexing. Tools like grep.app, which do use indexing, is faster for searching across millions of repositories. Information Indexing: Code search can index various kinds of information besides just code elements. Variable scopes, type information, definitions, and control and data flow are all valuable data for code search. Currently, ast-grep only indexes the AST itself. Retrieval Techniques: How a tool finds matching code given a query is a critical aspect. Various algorithmic and machine learning approaches exist for this. ast-grep uses a manual implementation that compares the query's AST with the code's AST. Ranking and Pruning: How search results are ordered is also a critical factor in providing good search results. Edit this page Last updated: Pager Next pageBlog List Made with ❤️ with Rust Copyright © 2022-present Herrington Darkholme

Source: https://dev.to/shafayeat/building-a-self-evolving-codebase-javascripts-ast-manipulation-and-dynamic-compilation-hb3
Building a Self-Evolving Codebase: JavaScript’s AST Manipulation and Dynamic Compilation - DEV Community Skip to content Navigation menu Search Powered by Search Algolia Log in Create account DEV Community Close Add reaction Like Unicorn Exploding Head Raised Hands Fire Jump to Comments Save Boost More... Moderate Copy link Copy link Copied to Clipboard Share to X Share to LinkedIn Share to Facebook Share to Mastodon Share Post via... Report Abuse Shafayet Hossain Posted on Nov 5, 2024 Building a Self-Evolving Codebase: JavaScript’s AST Manipulation and Dynamic Compilation #javascript #webdev #node #programming Learning JavaScript & Typescripts (42 Part Series) 1 Essential Concepts for JavaScript Developers 2 5 JavaScript Patterns Every Developer Should Know in 2024 ... 38 more parts... 3 Promises in JavaScript, A Guide for 2024 4 Understanding Closures in JavaScript: A Key Concept for 2024 5 JavaScript Memory Management and Garbage Collection 6 TypeScript: Because JavaScript Could Use a Little More Discipline 7 Optimizing JavaScript with Maps, Sets, and Weak References 8 JavaScript Event Loop: How It Works and Why It Matters for Performance 9 Understanding JavaScript Proxy: Power of Intercepting and Customizing Operations 10 Advanced Type System and Generics for Pro TypeScript Developers 11 Why You Should (Probably) Use TypeScript? 12 TypeScript Utility Types: Enhancing Code Reusability 13 Why Web Workers Depend on JavaScript ?? 14 Async Tricks in JavaScript for Smoother Code 15 The Callback Hell, Writing Cleaner Asynchronous JavaScript 16 Top 30 JavaScript Tricks Every Developer Should Know 17 Let's do a Battle Between JavaScript and TypeScript!! 18 Advanced Event Loop Techniques for Peak JS Performance 19 JavaScript Proxy and Reflect for Dynamic Object Control 20 Dominate Object-Oriented Programming (OOP) in JavaScript. 21 Building a Self-Evolving Codebase: JavaScript’s AST Manipulation and Dynamic Compilation 22 JavaScript Memory Management and Optimization Techniques for Large-Scale Applications 23 JavaScript Meta Programming: In-Depth Guide on Proxy, Reflect, and Object Property Control 24 Advanced JavaScript Game Development Techniques for Modern Game Devs 25 The Bad Sides of JavaScript 26 The Evolution of JavaScript 27 What's JavaScript’s Prototype Chain?? 28 Understanding WeakRefs and FinalizationRegistry in JavaScript 29 Mastering JavaScript Event Delegation 30 A Comprehensive Look at Custom JavaScript Compilers 31 Reactive Programming with JavaScript but Beyond the Basics 32 The Evolution of State Management in JavaScript 33 Performance Optimization with TypeScript 34 Mastering Type Inference in Complex Scenarios for TypeScript 35 Mastering Advanced TypeScript Concepts 36 A Comprehensive Exploration of JavaScript Decorators 37 Advanced and Creative TypeScript Techniques for Professionals 38 Mastering Concurrency and Parallelism in TypeScript 39 TypeScript for Domain-Driven Design (DDD) 40 Advanced Design Patterns in TypeScript 41 How to Secure TypeScript Applications...?? 42 Harnessing Functional Programming with JavaScript JavaScript is no longer just a language to handle client-side interactions—it’s now the foundation of rich, complex web and server applications. Imagine taking JavaScript further, building a codebase that doesn’t just function but optimizes itself, adapts to changing conditions, and even rewrites portions to improve performance. Here’s an all-encompassing guide on how you can do just that using Abstract Syntax Trees (ASTs) and dynamic compilation. 1. Understanding AST (Abstract Syntax Trees) An AST breaks down your JavaScript code into structured syntax trees, representing every function, variable, or loop as nodes. Tools like Babel, Acorn, Esprima, and Recast parse JavaScript into ASTs, providing a framework for analyzing or modifying your code. For example, with Babel, you can parse a simple function and examine its AST structure: const babelParser = require("@babel/parser");
const code = `function greet() { return "Hello!"; }`;
const ast = babelParser.parse(code);
console.log(ast);
 Enter fullscreen mode Exit fullscreen mode The AST reveals syntax elements like FunctionDeclaration, Identifier, and ReturnStatement, giving you programmatic access to modify or expand functionality. 2. Why Use AST Manipulation? With ASTs, you can automate code transformations without manually refactoring your code. This ability is critical for creating “self-evolving” codebases that adapt by rewriting portions to enhance performance or readability. Benefits of AST Manipulation: Dynamic Refactoring: Automatically improve code style, structure, or efficiency without manual intervention. Performance Optimizations: Rewrite slow functions or patterns in real-time. Advanced Linting and Error Detection: Correct or flag inefficiencies or stylistic issues directly in the code. 3. Implementing AST Transformations for Self-Evolving Code Creating self-evolving code requires setting up rules that allow code transformations under specific conditions. Let’s implement a dynamic memoization technique, where functions that perform heavy calculations are optimized with caching automatically. const babel = require("@babel/core");

const code = `
function fib(n) {
  return n <= 1 ? n : fib(n - 1) + fib(n - 2);
}
`;

const memoizeTransform = ({ types: t }) => ({
  visitor: {
    FunctionDeclaration(path) {
      path.node.body.body.unshift(t.expressionStatement(
        t.callExpression(t.identifier("memoize"), [t.identifier(path.node.id.name)])
      ));
    }
  }
});

const transformedCode = babel.transformSync(code, { plugins: [memoizeTransform] }).code;
console.log(transformedCode);
 Enter fullscreen mode Exit fullscreen mode In this example, the fib() function is transformed to use memoize automatically, helping improve performance without rewriting the original code manually. 4. Dynamic Compilation in JavaScript Dynamic compilation involves running or testing the modified code in real-time to choose the optimized version. JavaScript enables dynamic code execution via eval() and Node’s vm module, which allows you to test, compile, and apply changes at runtime. const vm = require("vm");

const script = new vm.Script(`function optimizedFunction() { /* optimized code */ }`);
const result = script.runInThisContext();
console.log(result);
 Enter fullscreen mode Exit fullscreen mode This approach lets you evaluate new code on-the-fly, improving the flexibility of your application by making runtime adjustments. 5. Combining AST Manipulation with Machine Learning for Code Optimization To take things further, you could integrate machine learning models that analyze performance or patterns in your code usage and automatically adjust code structures or functions based on real-time data. For instance: Usage Pattern Analysis: Identify which functions are used most frequently and refactor them to avoid memory leaks or optimize processing speed. Predictive Optimization: Pre-emptively restructure code to handle anticipated loads, based on previous patterns. You could track the performance of each code path and feed this data into a model to make predictions about future optimizations. 6. Building an Adaptive Codebase for Real-World Applications Creating a self-evolving codebase offers incredible power but also presents unique challenges: Managing Complexity: Dynamic transformations increase code complexity, which can lead to hard-to-debug issues if not managed carefully. Security: Runtime code execution (especially with eval) poses security risks; ensure transformations are validated to avoid vulnerabilities. Testing & Validation: Automatically transformed code needs rigorous testing to ensure it meets performance and correctness standards. Here’s an outline for creating a self-evolving feature in your JavaScript application: 1. Identify Candidates for Optimization: Look for functions or areas that benefit from performance improvements. 2. Define Transformation Rules: Specify the conditions that trigger AST-based transformations, like memoization for heavy computations or refactoring for more readable code. 3. Implement Dynamic Compilation: Set up evaluation scripts that measure performance changes in real-time. 4. Analyze & Refine: Track the changes over time, tweaking rules and transformations as needed. 7. Use Cases and Future Directions 1. Automated Code Optimization Libraries: Develop libraries that monitor code usage and restructure frequently accessed portions dynamically. 2. Code Evolution in Large-Scale Systems: Use AST manipulation in large projects to maintain efficiency across sprawling codebases by gradually optimizing code in the background. 3. Error Management Systems: Auto-correct frequently encountered issues or add error-checking to improve reliability and maintainability. Conclusion: Building a Truly Adaptive JavaScript Codebase Self-evolving code isn’t just a theoretical concept—it’s a powerful strategy for building flexible, scalable JavaScript applications. By mastering AST manipulation and dynamic compilation, you can create an adaptive codebase that learns, optimizes, and continuously evolves. My Personal Website: https://shafayet.zya.me A meme for you😉😉😉 Learning JavaScript & Typescripts (42 Part Series) 1 Essential Concepts for JavaScript Developers 2 5 JavaScript Patterns Every Developer Should Know in 2024 ... 38 more parts... 3 Promises in JavaScript, A Guide for 2024 4 Understanding Closures in JavaScript: A Key Concept for 2024 5 JavaScript Memory Management and Garbage Collection 6 TypeScript: Because JavaScript Could Use a Little More Discipline 7 Optimizing JavaScript with Maps, Sets, and Weak References 8 JavaScript Event Loop: How It Works and Why It Matters for Performance 9 Understanding JavaScript Proxy: Power of Intercepting and Customizing Operations 10 Advanced Type System and Generics for Pro TypeScript Developers 11 Why You Should (Probably) Use TypeScript? 12 TypeScript Utility Types: Enhancing Code Reusability 13 Why Web Workers Depend on JavaScript ?? 14 Async Tricks in JavaScript for Smoother Code 15 The Callback Hell, Writing Cleaner Asynchronous JavaScript 16 Top 30 JavaScript Tricks Every Developer Should Know 17 Let's do a Battle Between JavaScript and TypeScript!! 18 Advanced Event Loop Techniques for Peak JS Performance 19 JavaScript Proxy and Reflect for Dynamic Object Control 20 Dominate Object-Oriented Programming (OOP) in JavaScript. 21 Building a Self-Evolving Codebase: JavaScript’s AST Manipulation and Dynamic Compilation 22 JavaScript Memory Management and Optimization Techniques for Large-Scale Applications 23 JavaScript Meta Programming: In-Depth Guide on Proxy, Reflect, and Object Property Control 24 Advanced JavaScript Game Development Techniques for Modern Game Devs 25 The Bad Sides of JavaScript 26 The Evolution of JavaScript 27 What's JavaScript’s Prototype Chain?? 28 Understanding WeakRefs and FinalizationRegistry in JavaScript 29 Mastering JavaScript Event Delegation 30 A Comprehensive Look at Custom JavaScript Compilers 31 Reactive Programming with JavaScript but Beyond the Basics 32 The Evolution of State Management in JavaScript 33 Performance Optimization with TypeScript 34 Mastering Type Inference in Complex Scenarios for TypeScript 35 Mastering Advanced TypeScript Concepts 36 A Comprehensive Exploration of JavaScript Decorators 37 Advanced and Creative TypeScript Techniques for Professionals 38 Mastering Concurrency and Parallelism in TypeScript 39 TypeScript for Domain-Driven Design (DDD) 40 Advanced Design Patterns in TypeScript 41 How to Secure TypeScript Applications...?? 42 Harnessing Functional Programming with JavaScript Top comments (4) Subscribe 
 Personal Trusted User Create template Templates let you quickly answer FAQs or store snippets for re-use. Submit Preview Dismiss Collapse Expand Peter Vivo Peter Vivo Peter Vivo Follow Pipeline operator and touch bar fanatic from Hungary. God speed you! Location Pomaz Education streetwise Work full stack developer at TCS Joined Jul 24, 2020 • Nov 5 '24 Dropdown menu Copy link Hide Good to know about this possibillities. Like comment: Like comment: 2 likes Like Comment button Reply Collapse Expand Shafayet Hossain Shafayet Hossain Shafayet Hossain Follow The world would be a better place without me, Which is why I keep living Email shafayeat.me@outlook.com Location Dhaka,Bangladesh Education Shere Bangla School & College Pronouns He/him Joined May 31, 2024 • Nov 5 '24 Dropdown menu Copy link Hide Glad you found it helpful! So much is possible with JavaScript..!!😅😅 Like comment: Like comment: 1 like Like Comment button Reply Collapse Expand Call Loop Call Loop Call Loop Follow Call Loop is the go-to platform for SMS marketing, ringless voicemail, and voice broadcasting, enabling businesses to reach and engage customers effortlessly. Location Delray Beach, FL Joined Nov 6, 2024 • Nov 6 '24 Dropdown menu Copy link Hide Awesome post! Like comment: Like comment: 2 likes Like Comment button Reply Collapse Expand Shafayet Hossain Shafayet Hossain Shafayet Hossain Follow The world would be a better place without me, Which is why I keep living Email shafayeat.me@outlook.com Location Dhaka,Bangladesh Education Shere Bangla School & College Pronouns He/him Joined May 31, 2024 • Nov 12 '24 Dropdown menu Copy link Hide Thank you😊😊😊 Like comment: Like comment: 1 like Like Comment button Reply Code of Conduct • Report abuse Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's permalink. Hide child comments as well Confirm For further actions, you may consider blocking this person and/or reporting abuse Read next A beginner's guide to the Remove-Bg model by Lucataco on Replicate Mike Young - Dec 20 '24 Javascript great again! - The Voice of Void Peter Vivo - Jan 18 Simple SGD Method Matches Adam's Performance While Using Half the Memory Mike Young - Dec 19 '24 JavaScript Interview Questions Prajesh Gawhale - Jan 17 Shafayet Hossain Follow The world would be a better place without me, Which is why I keep living Location Dhaka,Bangladesh Education Shere Bangla School & College Pronouns He/him Joined May 31, 2024 More from Shafayet Hossain Harnessing Functional Programming with JavaScript #javascript #typescript #webdev #programming How to Secure TypeScript Applications...?? #typescript #javascript #webdev #programming The Viral Moments and Trends That Rocked IT Sector in 2024 #discuss #webdev Thank you to our Diamond Sponsor Neon for supporting our community. DEV Community — A constructive and inclusive social network for software developers. With you every step of your journey. Home DEV++ Podcasts Videos Tags DEV Help Forem Shop Advertise on DEV DEV Challenges DEV Showcase About Contact Free Postgres Database Software comparisons Code of Conduct Privacy Policy Terms of use Built on Forem — the open source software that powers DEV and other inclusive communities. Made with love and Ruby on Rails. DEV Community © 2016 - 2025. We're a place where coders share, stay up-to-date and grow their careers. Log in Create account

Source: https://moldstud.com/articles/p-top-strategies-for-enhancing-javascript-performance-through-effective-babel-ast-manipulation-techniques
Best Practices for Optimizing JavaScript Code with Babel AST Manipulation | MoldStud Home Articles Developers FAQ Babel developers questions Top Strategies for Enhancing JavaScript Performance through Effective Babel AST Manipulation Techniques Published on 23 February 2025 by Cătălina Mărcuță & MoldStud Research Team Top Strategies for Enhancing JavaScript Performance through Effective Babel AST Manipulation Techniques Discover best practices for optimizing JavaScript code using Babel AST manipulation to enhance performance and maintainability. The realm of web development is constantly evolving, and developers are always on the lookout for ways to elevate their applications. In this quest, innovative techniques play a crucial role. By examining how abstract syntax trees, or ASTs, can reshape the way we interact with scripts, we open a door to incredible possibilities. Indeed, the ability to manipulate code structure can lead to reduced load times and improved execution efficiency. As users demand more responsive interfaces, the need for efficient transformations becomes paramount. Traditional methods often fall short in addressing the complexities of modern applications. Each optimization can potentially yield significant performance improvements. For example, optimizing scripts can reduce parsing time by up to 40%. Alternatively, eliminating unnecessary code leads to cleaner and more maintainable projects. Such shifts not only enhance user experience but also improve overall application performance. Understanding the underlying principles of AST manipulation allows developers to harness powerful tools that promote smarter coding practices. Enhanced readability and streamlined processes can result from well-executed modifications. The ability to analyze and transform code opens new avenues for innovation, pushing the boundaries of what developers can achieve. By leveraging techniques rooted in ASTs, the landscape of web development becomes richer and more efficient. As we dive deeper into these transformative approaches, it becomes evident that embracing these methodologies is not just beneficial but essential. The ever-increasing demands of users necessitate a reevaluation of our current strategies. The tools at our disposal, when used judiciously, empower us to create applications that not only meet but exceed expectations. In this section, we will explore key strategies that can revolutionize how we approach the coding experience. Understanding the Importance of Babel AST The world of software development is constantly evolving, and understanding the tools available is crucial. One such tool is an intermediary representation that plays a pivotal role in transforming and analyzing source code. This representation helps developers enhance their applications and streamline processes effectively. Grasping its significance can lead to improved performance and maintainability. Many developers often overlook the intricacies behind this concept. However, it is essential to realize how crucial it is in the development lifecycle. It facilitates various transformations that can optimize applications in multiple ways. From error detection to code analysis, its benefits are vast. Facilitates better understanding of the code structure. Allows for advanced error checking during the development stage. Enhances the ability to implement language features. Supports custom transformations for tailored solutions. In fact, recent studies indicate that teams utilizing effective tools can reduce bugs by up to 70%, demonstrating a clear link between proper tool usage and software quality. Understanding this intermediary layer not only aids developers in identifying potential pitfalls early but also empowers them to leverage modern programming techniques. Another compelling reason to appreciate this representation is its compatibility with various programming languages. It acts as a bridge, allowing developers to work seamlessly across different ecosystems. This flexibility fosters innovation and creativity, as developers can experiment with unique approaches to solve complex problems. Improved collaboration among teams. Increased efficiency in the development process. Greater adaptability to industry changes. Furthermore, as the demand for high-quality software continues to rise, understanding this intermediary representation becomes even more critical. Developers are better equipped to meet user expectations and industry standards, ultimately leading to a superior product that stands out in a competitive landscape. What is Babel AST and Its Role? Understanding the mechanism behind code transformation can significantly enhance your development experience. The abstract syntax tree (AST) serves as a structural representation of your source code. This allows developers to manipulate and analyze code effectively. In essence, it decouples syntax from semantics, providing a foundation for further processing. When working with development tools, the AST plays a crucial part in how code is interpreted. It enables transformations, optimizations, and other modifications in a structured way, making it easier to apply changes across the codebase. By utilizing this structure, one can automate countless tasks, streamlining workflows and reducing human error. It's fascinating to note that according to recent industry reports, approximately 70% of developers often deal with JavaScript transpilation, showcasing the need for robust tools. The structure represented by the AST acts as a bridge between human-readable code and machine-executable instructions. This allows for various analyses and transformations that are essential for effective coding practices. Feature Description Tree Structure Represents code in a hierarchical format for easier manipulation. Transformations Allows for code modifications without losing context or structure. Error Detection Aids in identifying syntax errors through a structured approach. By leveraging the power of the AST, developers can unlock a myriad of possibilities that enhance both productivity and code quality. Its role transcends mere code representation; it enables higher-level operations that can streamline entire workflows, aligning with the demands of modern software development. Advantages of Using Babel for Code Optimization Employing a powerful tool can significantly enhance development efficiency. It streamlines workflows by enabling developers to write cleaner, more maintainable scripts. This leads to increased productivity and faster turnaround times for projects. Innovations in modern environments demand high adaptability, and a versatile solution is key. One of the most compelling reasons to consider this tool is its compatibility with various features. By allowing developers to utilize the latest features without worrying about browser support, it alleviates many common headaches. For example, almost 60% of developers reported increased satisfaction in their workflow when using this technology. Moreover, the ability to modify and transform code structures can lead to performance improvements. When developers can access a rich set of plugins, they can tailor their setup to meet specific project needs. This customization often results in optimized performance levels that would be challenging to achieve otherwise. Transitioning older codebases becomes less daunting when using this tool, as it provides a bridge between past and present standards. Statistics indicate that nearly 70% of legacy projects benefit from enhanced readability and maintainability through such transformations, ensuring that they stay relevant in an ever-evolving tech landscape. Finally, the supportive community surrounding this technology ensures continuous growth and improvement. Engaging with fellow developers and sharing insights can uncover new methods to refine performance. As a result, those involved often experience significant advancements in their programming capabilities and project outcomes. Statistics on Performance Improvements with Babel Analyzing the impact of using advanced tools can yield surprising insights. Numerous developers report enhanced efficiency in their applications when employing transformation tools. This holds especially true for web applications that demand high performance. Performance metrics are critical in establishing the true value of such tools. Understanding these figures can guide more informed decisions. A survey indicated that 68% of developers noticed significant speed enhancements. Reports show that load times decreased by an average of 30% across various projects. Increased code efficiency often results in a reduced bundle size. Applications that adopted these transformations observed up to 50% fewer rendering issues. In a recent case study, one large-scale application experienced a reduction in initial load time from 3 seconds to just 1.5 seconds, demonstrating how pivotal these methods can be for user retention and satisfaction. When evaluating the performance metrics, it becomes evident that optimizing the underlying structure not only enhances execution speed but also improves the overall user experience. Developers are continuously striving to implement techniques that yield better outcomes, and these statistics serve as powerful motivators. 30% faster rendering times lead to improved user engagement. 40% increase in processing speed translates to enhanced interaction. 75% of teams reported lesser memory consumption, which is crucial in mobile development. Data reveals that teams adopting these methodologies can expect a notable elevation in performance and user satisfaction, while simultaneously reducing the technical debt associated with legacy systems. As the industry evolves, staying informed about these advancements ensures that developers remain competitive in a fast-paced environment. Effective Techniques for AST Manipulation Understanding the nuances of manipulating abstract syntax trees can significantly enhance a developer's toolkit. This approach allows for deeper insights into code structure, enabling targeted transformations. The potential for optimization and customization is vast. By employing various methods, developers can streamline their workflows and achieve remarkable results. Each technique brings unique advantages that can cater to specific project needs. One effective strategy involves using traversal methods to inspect and modify tree nodes. This allows precise adjustments, ensuring that any changes align seamlessly with the original intent. Another technique revolves around node replacement, which can drastically alter functionality while retaining the original code style. Developers can also utilize plugins that automate such modifications, significantly reducing manual effort. Furthermore, leveraging existing libraries like Babel's tools can simplify many complex operations. Statistics indicate that approximately 65% of developers rely on such frameworks to enhance their productivity. This reliance showcases the importance of automated solutions in modern development practices. As projects become increasingly sophisticated, the need for efficient tree manipulation techniques continues to grow. For detailed insights into related software solutions, you can explore this supply chain software development services. In addition, integrating source maps can provide valuable context for debugging modified code. This practice allows developers to trace back changes to their original positions, maintaining clarity throughout the development process. Moreover, optimizing tree-shaking capabilities can lead to leaner builds, enhancing performance without sacrificing functionality. As the landscape of programming evolves, mastering these skills will be essential for future success. Traversing the AST for Efficient Transformations Navigating the structure of a program's syntax allows developers to make meaningful alterations. Effective traversal of this framework can lead to powerful modifications. It involves exploring nodes within the hierarchy to identify opportunities for enhancement. Transformations become streamlined as programmers gain insights into the underlying architecture. Consider that every codebase contains intricate relationships and dependencies. Analyzing these connections through a systematic approach can reveal patterns that facilitate improvements. A well-structured navigation strategy can significantly reduce the time needed for each modification. Furthermore, this meticulous exploration ensures that unintended consequences are avoided. By employing thoughtful traversal techniques, one can uncover various optimization opportunities. These may include code simplifications, performance boosts, or even increased readability. Each node holds potential for transformation, but it requires careful attention to detail and logic. With approximately 68% of developers reporting code maintainability issues, addressing these concerns effectively becomes crucial. Diving deep into the syntax tree allows for comprehensive understanding and creative solutions. For instance, when reshaping certain structures, programmers can leverage specific design patterns. An example might be the implementation of a component based architecture, which promotes reusability and coherence throughout the codebase. By examining relationships within the tree, transformations can be executed with precision, resulting in cleaner and more efficient final outcomes. Common Patterns in AST Modification In the realm of abstract syntax tree transformations, specific strategies emerge frequently. Developers utilize certain techniques to enhance and reshape the structure of their programs effectively. These transformations can lead to improved performance, maintainability, and readability. Embracing these methods not only streamlines the coding process but also elevates the overall quality of the output. While the variety of approaches is vast, some patterns stand out due to their widespread applicability. One prevalent method involves the manipulation of nodes within the tree. This allows for the addition or removal of elements, which can significantly alter how a program functions. Another common approach is the traversal of the tree, enabling developers to inspect and modify various components systematically. Such rigorous inspection often leads to discovering redundancies or optimizing specific sections of the code. Furthermore, replacements of certain node types can yield significant benefits. By substituting outdated constructs with newer, more efficient ones, developers can enhance performance without extensive rewrites. According to recent studies, adopting newer syntax can improve execution times by up to 30%. This not only makes applications faster but also aligns them better with current standards. Lastly, combining these techniques can lead to powerful transformations. For instance, traversing a tree to identify redundant nodes before replacing them with optimized versions creates a thorough refinement process. Such an approach not only improves the program’s efficiency but also ensures it adheres to modern development practices. In an era where remote collaboration is increasingly essential, leveraging tools that enable secure remote collaboration has become crucial for teams working on complex projects. Consider exploring resources that facilitate this transition, such as secure remote collaboration. Real-World Examples of AST Manipulation Success In today's development landscape, various organizations harness the power of syntax trees to achieve remarkable outcomes. The ability to rework and refine code structures can significantly enhance performance, maintainability, and even developer productivity. Teams have reported improved efficiency by utilizing tree transformations to streamline their workflows. This approach leads to cleaner and more efficient implementations. One prominent example comes from a popular library in the React ecosystem. By applying tree-sitting techniques, developers were able to cut down bundle sizes by nearly 30%. This reduction not only improved load times but also enhanced user experience. Such improvements are vital, especially when statistics indicate that users abandon a site if it takes more than three seconds to load. The impact of these changes reverberates through overall application performance. Another compelling case involves a major tech company that revamped its tooling using custom parser adjustments. The result? A striking 50% increase in code readability after transforming redundant patterns into more streamlined constructs. Developers found that the new format led to fewer bugs and a faster onboarding process for new team members. In fact, internal surveys showed that team morale improved significantly due to the newfound clarity. Moreover, consider a startup that managed to boost the efficiency of their CI/CD pipeline. By integrating static analysis tools combined with syntactic modifications, they not only reduced build times by 40% but also enhanced code quality metrics. These optimizations resulted in a quicker turnaround for feature releases, allowing the team to remain competitive in a fast-paced market. Overall, the transformative potential of tree manipulation in software development is vast and varied, as evidenced by these successful implementations. As the tech landscape continues to evolve, adopting such innovative techniques will remain crucial for teams striving to stay ahead. Embracing these strategies ensures not only improved performance but also a significant leap in overall project effectiveness. Add new comment Full Name Email Message Add a comment Related articles Related Reads on Babel developers questions Dive into our selected range of articles and case studies, emphasizing our dedication to fostering inclusivity within software development. Crafted by seasoned professionals, each publication explores groundbreaking approaches and innovations in creating more accessible software solutions. Perfect for both industry veterans and those passionate about making a difference through technology, our collection provides essential insights and knowledge. Embark with us on a mission to shape a more inclusive future in the realm of software development. 2 February 2025 Unlocking the Full Potential of Babel CLI for Effortless Dependency Management and Streamlined Workflows Master Babel CLI to simplify your dependency management process, streamline your development workflow, and boost productivity with ease. Read Article 2 February 2025 Creating Personalized Babel Presets for Specific Needs A Comprehensive Guide for Developers Discover how to create custom Babel presets tailored for your unique use cases with our comprehensive step-by-step guide. Read Article 2 February 2025 A Comprehensive Guide to JSX Syntax and Its Integration with Babel Explore JSX syntax and learn how to seamlessly integrate it with Babel for building dynamic React applications. Read Article You will enjoy it Recommended Articles 2 August 2024 How to hire remote Laravel developers? When it comes to building a successful software project, having the right team of developers is crucial. Laravel is a popular PHP framework known for its elegant syntax and powerful features. If you're looking to hire remote Laravel developers for your project, there are a few key steps you should follow to ensure you find the best talent for the job. Read Article 27 March 2024 Software Development Services for Startups As technology continues to advance at a rapid pace, businesses are faced with the challenge of navigating the complex tech landscape to ensure they stay competitive and innovative. One key aspect of this is choosing the right software development services to help create custom solutions that meet specific business needs. Read Article 25 March 2024 Team Extension Services The Key to Building Scalable Development Teams In today's fast-paced tech industry, companies are constantly under pressure to deliver cutting-edge solutions quickly and efficiently. One of the key challenges that many businesses face is finding and hiring skilled software developers to meet their development needs. Read Article MoldStud ServicesCareersAbout UsContact UsHire Us MoldStud Services Careers About Us Contact Us Hire Us Industries Solutions Technologies Our Articles Privacy Policy Together, we can redefine technology through accessible design and development © 2018 - 2025 MoldStud . All rights reserved.

Results for query: Best practices for implementing RAG in large-scale software development projects

Search Engine: Google
Source: https://www.google.com/search?q=Best+practices+for+implementing+RAG+in+large-scale+software+development+projects+site:www.reddit.com&sca_esv=392315896b6345ed&sa=X&ved=2ahUKEwjsuoSztK2LAxVJTKQEHYsbIGMQrQIoBHoECBsQBQ
Voordat je verdergaat naar Google Zoeken Inloggen Inloggen Voordat je verdergaat naar Google We gebruiken cookies en gegevens voor het volgende: Google-services leveren en onderhouden Uitval bijhouden en bescherming bieden tegen spam, fraude en misbruik Doelgroepbetrokkenheid en sitestatistieken meten om inzicht te krijgen in hoe onze services worden gebruikt en de kwaliteit van die services te verbeteren Als je Alles accepteren kiest, gebruiken we cookies en gegevens ook voor het volgende: Nieuwe services ontwikkelen en verbeteren Advertenties laten zien en de effectiviteit ervan meten Gepersonaliseerde content laten zien (afhankelijk van je instellingen) Gepersonaliseerde advertenties laten zien (afhankelijk van je instellingen) Als je Alles afwijzen kiest, gebruiken we cookies niet voor deze aanvullende doeleinden. Niet-gepersonaliseerde content wordt beïnvloed door factoren zoals de content die je op dat moment bekijkt, activiteit in je actieve zoeksessie en je locatie. Niet-gepersonaliseerde advertenties worden beïnvloed door de content die je op dat moment bekijkt en je algemene locatie. Gepersonaliseerde content en advertenties kunnen ook relevantere resultaten, aanbevelingen en op jou toegespitste advertenties omvatten die zijn gebaseerd op eerdere activiteit van deze browser, zoals uitgevoerde Google-zoekopdrachten. We gebruiken cookies en gegevens ook om te zorgen dat de functionaliteit geschikt is voor je leeftijd, als dit relevant is. Selecteer Meer opties om meer informatie te bekijken, waaronder informatie over hoe je je privacyinstellingen beheert. Je kunt ook altijd naar g.co/privacytools gaan. Meer opties AfrikaansazərbaycanbosanskicatalàČeštinaCymraegDanskDeutscheestiEnglish (United Kingdom)English (United States)Español (España)Español (Latinoamérica)euskaraFilipinoFrançais (Canada)Français (France)GaeilgegalegoHrvatskiIndonesiaisiZuluíslenskaItalianoKiswahililatviešulietuviųmagyarMelayuNederlandsnorsko‘zbekpolskiPortuguês (Brasil)Português (Portugal)românăshqipSlovenčinaslovenščinasrpski (latinica)SuomiSvenskaTiếng ViệtTürkçeΕλληνικάбеларускаябългарскикыргызчақазақ тілімакедонскимонголРусскийсрпскиУкраїнськаქართულიհայերենעבריתاردوالعربيةفارسیአማርኛनेपालीमराठीहिन्दीঅসমীয়াবাংলাਪੰਜਾਬੀગુજરાતીଓଡ଼ିଆதமிழ்తెలుగుಕನ್ನಡമലയാളംසිංහලไทยລາວမြန်မာខ្មែរ한국어日本語简体中文繁體中文繁體中文 (香港) Privacybeleid Servicevoorwaarden

Source: https://intelliarts.com/blog/enterprise-rag-system-best-practices/
Enterprise RAG System: Best Practices Strategies Services Technology Consulting Custom AI Solutions Data Engineering Software Engineering Expertise Big Data Data Science Custom ML Solutions IoT Solutions Industries Insurance Manufacturing Digital Marketing Renewable Energy Success Stories Insights Blog White Papers Company Company Overview Careers Contact us Contact us Services Technology Consulting Custom AI Solutions Data Engineering Software Engineering Expertise Big Data Data Science Custom ML Solutions IoT Solutions Industries Insurance Manufacturing Digital Marketing Renewable Energy Success Stories Insights Blog White Papers Company Company Overview Careers Contact us Best Practices for Enterprise RAG System Implementation Volodymyr Mudryi 18 November 2024 10 min read IntelliartsBlogMachine LearningBest Practices for Enterprise RAG System Implementation Structure Share article Developing an advanced AI product requires Retrieval Augmented Generation technology. Explore how to implement an enterprise RAG system. To show the growing importance of RAG solutions, Databricks, a global data and AI analytics company, reports that 60% of their LLMs use RAG. The high potential of this technology, given the high popularity of AI in general, is what makes it worthy of good consideration on a business level. Currently, AI-driven solutions are of huge popularity across businesses and industries. Retrieval Augmented Generation (RAG) is exactly the technology that offers better AI performance, whether it’s a common chatbot or an enterprise-level solution for data analytics. In this post, you’ll discover the common enterprise RAG system implementation strategy, its architecture pattern, key concepts, and related considerations. Besides, you’ll obtain an overview of the 10 best practices for utilizing and integrating RAG systems in enterprises. Common architecture pattern of RAG systems for enterprise RAG systems are AI models that combine information retrieval and text generation to provide more accurate and contextually relevant responses. RAG systems are often employed in Large Language Model (LLM) solutions, like ones based on ChatGPT or Claude AI where they aid in retrieving relevant information from large datasets or documents needed for handling complex queries. In tasks requiring up-to-date or domain-specific knowledge, such as customer support, research, and enterprise applications, RAG systems offer unmatched response accuracy improvement. The RAG technique is well-suited for Natural Language Processing (NLP) applications, as it combines retrieval-based methods with generation-based models. It allows for effectively harnessing the strengths of both approaches. Different embedding models can yield varying results. Some models might capture the overall style or intent of a question, while others may focus on individual words rather than the actual meaning. Experimenting with multiple embedding models is beneficial to identify which best aligns with your intended results. — Volodymyr Mudryi, AI, ML, and data science expert at Intelliarts. The infographics depicting a common pattern for enterprise architecture for RAG is shown below: Basically, what the demonstrated architecture of the advanced Retrieval Augmented Generation model reveals is that RAG handles user queries with enhanced security, retrieval, and response generation processes. Starting from user input, it follows steps including authentication, input guardrails, and query rewriting. It utilizes components like encoders, retrieval systems, and ranking to refine information from stored data (embedding, document, history, and feedback storage). This enterprise RAG architecture pattern can be broken down into these RAG workflow steps: User input and authentication Input validation with guardrails Query rewriting and encoding Information retrieval from storage Ranking and response generation Output validation with guardrails Final response delivery to the user You can explore additionally what RAG models are and how they are used in another blog post by Intelliarts. The list of benefits of RAG system in large-scale organizations SLAs (service-level agreements) in enterprises is shown in the image below: 10 best practices for using Retrieval Augmented Generation (RAG) Let’s have an in-depth insight into the best practices for RAG system implementation, based on real practical experience of engineers from Intelliarts: #1 Provide context on how the output was generated In enterprise RAG systems, providing context around generated outputs is essential for building user trust and transparency. By offering users insight into the sources of information, they can verify data accuracy, which is especially valuable in sensitive areas like compliance or customer support. Intelliarts often integrates source attribution in RAG solutions, allowing users to track data origins in real time, enhancing credibility and confidence in the system. #2 Feed the LLM product integration data Integrating data directly from enterprise tools like CRM or ERP systems is crucial for RAG systems to provide contextually relevant and accurate responses. This integration allows the RAG system to deliver responses tailored to the business’s unique needs. Intelliarts regularly incorporates data from internal systems, ensuring real-time insights and improved response relevancy, which is especially beneficial in customer support and operations management. #3 Continuously evaluate the outputs to spot issues and areas of improvement To maintain optimal performance, RAG systems should undergo regular evaluations, focusing on consistency, load handling, and edge cases. This approach ensures that potential issues are quickly identified and addressed. At Intelliarts, we apply rigorous testing methods in all custom RAG implementations, allowing us to refine systems continuously and uphold performance standards, even under varying operational conditions. #4 Utilize dynamic data loading/updating Dynamic data loading ensures that RAG systems operate with the latest information, preventing outdated data from affecting response accuracy. This capability is crucial in fast-paced sectors where information constantly changes. Intelliarts integrates real-time data loading into RAG systems, allowing users to interact with the most up-to-date insights, which enhances response quality in areas like finance and customer support. #5 Maintain hierarchical document structure Keeping a hierarchical structure in documents enhances RAG systems’ ability to retrieve relevant information accurately. Organizing content with clear headings and subheadings ensures that retrieval focuses on contextually appropriate sections. This structure not only boosts response accuracy but also enables more meaningful, layered responses, especially important for industries dealing with complex data. White Paper Driving Business Efficiency with RAG Systems and LLM Integration Download #6 Implement a thoughtful chunking strategy Chunking documents into smaller parts improves retrieval efficiency but can create challenges if relevant information is split across chunks. To optimize chunking, use document structures like sections or paragraphs and experiment with an embedding-based strategy, like chunk size or overlapping, to balance information granularity and processing efficiency, thus enhancing RAG performance. #7 Optimize retrieval quality with relevance scoring Relevance scoring prioritizes the most applicable data during retrieval, improving the precision of responses. By aligning retrieved data closely with user queries, RAG systems deliver more contextually accurate answers. Intelliarts employs relevance scoring in enterprise RAG solutions like retrieval-based chatbots, especially for applications requiring precise data interpretation, such as regulatory compliance and financial analysis. #8 Establish feedback loops for continuous learning RAG system integration with feedback loops adapts over time, learning from user interactions to enhance response quality and accuracy. This continuous learning approach is key to meeting evolving business needs. Intelliarts integrates user feedback into RAG systems, refining data retrieval accuracy based on real-world usage, which is particularly beneficial in applications with dynamic requirements like customer service. #9 Use version control for data sources and models Version control is critical in RAG systems to manage changes to data sources and model configurations, especially in industries requiring strict audit trails. Intelliarts incorporates version control into enterprise RAG solutions, allowing for consistent tracking and updates, which supports data integrity and compliance in sectors like finance and legal. #10 Use ensemble techniques to reduce bias Combining outputs from multiple models through ensemble techniques helps reduce variance, making the system more robust. Intelliarts employs this approach in enterprise RAG systems, ensuring balanced insights that are especially valuable in decision-making contexts. This technique supports unbiased results, crucial for sectors like human resources and customer support, where fair responses are essential. Need help with ML development? Let Intelliarts assist you. Contact us RAG strategy for enterprises and an overview of main concepts and considerations As mentioned previously, RAG is an extension of an AI-driven system. So, should you ask yourself how to how to implement RAG in enterprise, then here’s a best practice for enterprise RAG system implementation step-by-step: Select the AI model that’s suitable for an intended purpose Integrate RAG mechanism Define data sources and structure Establish query-response flow Implement feedback and monitoring mechanisms Ensure data security and compliance Scale and maintain the system Since the steps to implement RAG systems in enterprises are now understood and AI insights obtained, we may proceed with reviewing key concepts and considerations related to this process. They are provided in the table below: The step-by-step process of installing RAG in your business and a consideration of enterprise RAG system integration key concepts should give you valuable insights into RAG and help you form your RAG development needs. Intelliarts experience with RAG Here are several case studies by Intelliarts revealing RAG business development cases: Analysis report generation with RAG-based system success story The Intelliarts team is working on an ongoing project for a customer who handles massive amounts of data to provide actionable insights into China’s political economy. They have an extremely large data pool in PDF format. Currently, analysis is provided by domain experts, and a single request may take as long as an hour. So, there’s a crucial need for automation and process improvement. Here’s how this case unfolds: Challenge: To create a RAG-enhanced AI solution that will be able to execute all sorts of summarization and data retrieval procedures. A development difficulty, in this case, is an inappropriate structuring and storage of the original data pool and lots of unfitted data. Solution: A custom RAG system that contains guardrails and a query refiner, retrieval functionality, and a vector database. It can generate responses to users’ queries, return sources, and tailor responses based on images contained in PDF files from the storage. Result: Currently, the time it takes to respond to user’s request is shortened from hours to minutes. Still, since the project is ongoing, we look forward to refining the RAG system and bringing even more value. Looking for a trusted tech partner to provide you with AI development services? Let Intelliarts address your business needs. RAG-enhanced AI recommendation system development success story The Intelliarts team partnered on a project to develop an AI-powered recommendation system, the “step suggester,” to assist students in cybersecurity coursework. This tool offers real-time guidance to students facing challenges, improving course completion rates. Challenge: Create a RAG-enhanced AI system that analyzes student actions in a virtual environment, delivering tailored recommendations to reduce dropout rates. The main challenge was structuring and analyzing large volumes of unorganized data while incorporating cybersecurity insights. Solution: Intelliarts built a PoC for an AI assistant combining machine learning, LLMs, and programmatic algorithms. The system identifies points where students get stuck and provides actionable steps, leveraging custom algorithms and LLM responses for each task. Result: The AI agent reduced support time and improved course completion rates. The PoC confirmed the solution’s feasibility, setting the stage for full-scale deployment across 700 course tasks. RAG-enhanced ChatGPT chatbot creation success story The Intelliarts team developed an AI chatbot data extraction solution using ChatGPT to help an NGO quickly retrieve information on gun safety from their extensive knowledge base. Challenge: The NGO’s growing data made it challenging for employees to find relevant information efficiently. They needed a solution to extract data quickly from their knowledge base. Solution: Intelliarts created a ChatGPT-powered chatbot that performs data extraction and analysis on user queries, leveraging RAG methods. The chatbot was trained to focus on gun safety topics and provide references to support its responses. This system integrates both textual and tabular data handling, using advanced ML features for accurate and contextually relevant answers. Results: The chatbot reduced information retrieval time from hours to minutes, saving significant operational time. It also aligned with the NGO’s tone, allowing employees to streamline their workflow, improve productivity, and improve decision-making with the RAG system. The AI assistant is seamlessly integrated into the NGO’s system, providing a user-friendly, personalized solution. Final Take RAG systems offer enterprises advanced capabilities in data retrieval and context-aware response generation, making them invaluable across applications like customer service, content generation, and compliance. By following best practices, businesses can unlock the full potential of RAG systems. Understanding what is the best available RAG implementation for your specific needs ensures that you harness the technology’s full potential. For enterprises looking to leverage RAG, partnering with experienced AI developers can streamline implementation and drive impactful results aligned with business goals. Here at Intelliarts, we have substantial in-house AI development expertise. Over the span of 24 years on the market, we have developed a number of fully-fledged AI solutions, including ones reinforced with RAG systems. Build Your AI System with Confidence. Implementing AI-driven systems with RAG requires trusted expertise. Request assistance FAQ 1. How can a RAG system improve our data retrieval processes? An enterprise RAG system enhances data retrieval by combining information retrieval with text generation, delivering more accurate, context-aware responses. This system efficiently processes large datasets, providing relevant results quickly, which can significantly boost decision-making and streamline information access across various enterprise applications. 2. What are the costs involved in implementing a RAG system? Enterprise RAG system implementation costs depend on factors such as data complexity, integration with existing infrastructure, and customization needs. Costs typically cover RAG system software for enterprises, hardware, data storage, and maintenance. We provide a tailored assessment to determine precise costs for your specific use case. 3. How does your company ensure the robustness of RAG systems over time? We ensure the robustness of RAG systems for enterprises by implementing continuous monitoring, regular updates, and retraining models to adapt to evolving data. We have the best tools for enterprise RAG system implementation including performance tracking and error handling, ensuring reliable long-term operation and adaptability to your business needs. 4. Can RAG systems be integrated with our existing IT infrastructure? Yes, enterprise RAG system integration is possible with most IT infrastructures. We assess your current systems to ensure seamless integration, utilizing APIs and other tools to align the RAG system with your data sources and workflows, maximizing its effectiveness within your existing setup. See all questions By Volodymyr Mudryi DS/ML Engineer Rate this article 4.7/5 3 ratings Structure White paper Driving Business Efficiency with RAG Systems and LLM Integration Download Related Posts AI, Machine Learning How AI is Changing Agriculture: 10 Real-World Use Cases LLM, AI, Machine Learning The Evolution of ChatGPT: Vital Statistics and Trends for 2025 Machine Learning Intelliarts Wins with Clutch Fall Global Award 2024 Predictive Maintenance, Machine Learning Predictive Maintenance for Renewable Energy Assets AI, LLM, Machine Learning Claude vs. ChatGPT: Which AI Model is Best for Your Business? Software Engineering, Machine Learning Intelliarts to Attend ONS 2024 Machine Learning, Big Data, Data Engineering Preparing Data for ML Development in 6 Steps 7, Kotyka Str, 700 Room, 79014, Lviv, Ukraine 43, Tomasa Zana Str, 20-601, Lublin, Poland info@intelliarts.com Services Technology Consulting Custom AI Solutions Data Engineering Consulting Software Engineering Expertise Custom ML Solutions Data Science Consulting Big Data Consulting IoT Solutions Industries Insurance Manufacturing Digital Marketing Renewable Energy Company Company Overview Careers Hiring Success Stories Insights Blog White Papers Intelliarts © 1999-2025 Privacy Policy Terms of Use Cookies Policy Intelliarts © 1999-2025 Privacy Policy Terms of Use Cookies Policy

Source: https://www.google.com/url?url=https://support.google.com/websearch?p%3Dfeatured_snippets%26hl%3Den-BE&rct=j&q=&esrc=s&opi=89978449&usg=AOvVaw0-xXX9L80985UB-G9Jvm5i&hl=en-BE&sa=X&ved=2ahUKEwjsuoSztK2LAxVJTKQEHYsbIGMQrpwBegQIIxAB


Source: https://nexla.com/ai-infrastructure/retrieval-augmented-generation/
Retrieval-Augmented Generation (RAG) Tutorial, Examples & Best Practices | Nexla <img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=967623856751580&amp;ev=PageView&amp;noscript=1"/> 🚀 The Future of Retrieval-Augmented AI: Get expert insights on RAG optimization, data integration, and driving strategic innovation. View Webinar Now Menu Product Capabilities Advanced RAG Support for agentic, multi-LLM models GenAI ETL Integration Multi-modal, structured and unstructured data integration Data Integration ETL/ELT/R-ETL integrations Streaming Real-time data and updates Change Data Capture API Integration iPaaS and data API Platform How It Works Architecture Data fabric architecture Nexsets Virtual Data Products Enterprise Security Compliance & governance DataOps Enteprise Scale Reliability | State of Data and AI Integration Solutions By Application AI & GenAI Enterprise Grade AI Inter Company Integrations Customer and Partner onboarding Data Engineering Data Analytics Ready-to-use Data Data Fabric Enterprise-wide Data Platform By industry Asset Management Financial Services Retail & Ecommerce Healthcare & Life Sciences | Advanced Buyers Guide Customers Partners Resources Resources Blogs Customer Stories Case Studies Ebooks Events Webinars Whitepapers Documentation Support All Resources Recent Assets Guide to Enterprise AI Data Integration 101 Data Engineering Best Practices AI Infrastructure Best Practices Guide to Data Fabric Architecture Data Integration Techniques | The Practical Guide to Data Integration - Free Download Company | About Careers Contact Us Login SCHEDULE DEMO Multi-chapter guide | Your Guide to Generative AI Infrastructure Retrieval-Augmented Generation (RAG) Tutorial & Best Practices Chapters Chapter 1: AI Infrastructure Chapter 2: Large Language Model (LLMs) Chapter 3: Vector Embedding Chapter 4: Vector Databases Chapter 5: Retrieval-Augmented Generation (RAG) Chapter 6: LLM Hallucination Chapter 7: Prompt Engineering vs. Fine-Tuning Chapter 8: Model Tuning—Key Techniques and Alternatives Chapter 9: Prompt Tuning vs. Fine-Tuning Chapter 10: Data Drift Chapter 11: LLM Security Chapter 12: LLMOps Chapter 5: Retrieval-Augmented Generation (RAG) Tutorial & Best Practices Prev Next Table of Contents Unlock up to 10x greater productivity Explore the full power of our data integration platform for free. Get started with your GenAI, analytics, and operational initiatives today. Try for Free Like this article? Subscribe to our LinkedIn Newsletter to receive more educational content Subscribe now Retrieval-augmented generation (RAG) represents an innovative approach to artificial intelligence (AI) that significantly improves how machines understand and respond to information. It combines traditional AI language models with the ability to dynamically introduce relevant external data and provide more accurate responses. In this article, we explore how RAG works, its key differences from traditional AI models, and how it improves AI development. We discuss applications, benefits, and the future potential of RAG in various industries. Summary of key retrieval augmented generation concepts The article aims to provide an understanding of RAG and its role in advancing the field of AI through the following sections. Concept Description Understanding retrieval-augmented generation RAG represents a significant advancement in AI language models by integrating static pre-trained knowledge with dynamic external data retrieval. It involves two key processes: retrieving information by conducting targeted searches in a large database and generating responses by integrating the retrieved information with pre-existing knowledge. The benefits of retrieval-augmented generation RAG enhances the system’s ability to provide coherent, context-relevant, and up-to-date answers for various tasks. These include question-answering, article writing, and chatbot dialogue. RAG can help make systems adaptable to a variety of different applications. Comparing RAG to model fine-tuning and building your own model These processes involve evaluating factors like complexity, resource requirements, and adaptability. RAG excels at real-time data integration, using fine-tuning to strike a balance between adaptability and ease of use. Building custom models is optimal for highly specialized tasks and groundbreaking research. The causes of model hallucination Model hallucination in language models occurs when AI generates inaccurate or fabricated information. This can occur due to biased or incomplete training data, a lack of real-world understanding, over-specialization in training data, challenges in comprehending complex language structures, or a lack of ability to fact-check against external data sources in real-time. Use cases for retrieval-augmented generation RAG is transforming industries by improving chatbot accuracy in customer service; aiding content creation; enhancing decision-making in healthcare, education, and legal research; personalizing e-commerce experiences; and improving financial analysis. Challenges RAG encounters technical challenges in managing complex datasets and integrating retrieval and generation components, operational challenges in scalability and system maintenance, and ethical challenges related to biases and data privacy. Addressing these challenges is crucial for successful RAG implementation, and tools like Nexla can assist by facilitating improved data integration, parameter tuning, collaborative teamwork, and production-grade AI pipeline management. Best practices Successful and sustainable implementation of RAG requires regular updates and diversification of data sources, continuous training and performance monitoring, robust infrastructure for scalability, ethical considerations regarding data privacy and regulations, user-friendly design for enhanced interaction, and collaboration with experts and user feedback for ongoing improvement and effectiveness. Understanding retrieval-augmented generation in the context of LLMs There are two main parts to RAG: retrieval and generation. Retrieval is when the machine searches for extra information that is related to what’s being asked about from various data sources. The information is transformed into vector embeddings and stored in a vector database, as illustrated in the diagram below. The high-level process of ingesting data into an LLM model (source: Nexla) During retrieval, RAG uses a part of its system to find information that is relevant to the question or prompt it’s been given. This is like a targeted search, finding text or data that match or relate to the input it received. Once RAG has found this information, the next step is to integrate it into its response. This means the system doesn’t just repeat what it finds—it combines the new information with what it already knows to create a more informed, accurate, and context-relevant answer. Generation occurs when the machine uses what it knows about language to form answers. It takes the information it found and combines it with what it already knows to give a more complete response, as visually summarized below. The high-level process of retrieval-augmented generation (source: Nexla) The generative part of RAG is about creating a response. This isn’t just about using the right words—it’s about making sure the response is coherent and makes sense in the context of the question or topic. RAG can provide answers that are not just based on previously learned information but also improved with the latest and most relevant external data. RAG can be used for various tasks, like answering questions, writing articles, or even creating dialogue for chatbots. Its ability to pull in current information makes it particularly useful for topics that are constantly changing or require detailed, specific knowledge. The benefits of retrieval-augmented generation Normally, AI systems rely on the information they were trained on, which might be outdated or limited. RAG bridges the gap between static, pre-trained knowledge and dynamic and fresh external data. They offer a more advanced, adaptable, and accurate approach to generating contextual content. For example, you can use RAG to Look up and use the most relevant and current information to answer questions or generate content Answer questions where the answer keeps changing based on the latest information. Answer questions about recent events or specific topics more accurately. RAG can also be used to solve problems in the AI model training data set. AI models learn from the data they are trained on. If this training data is biased, incomplete, or contains errors, the model may replicate these flaws in its outputs. You can use RAG to improve output quality and reduce bias. Model hallucination is another challenge where AI generates inaccurate, misleading, or completely fabricated information. Sometimes, models are over-fitted on their training data, which causes them to become too specialized in the data they’ve seen and unable to generalize well to new, unseen data. This can lead to hallucinations when faced with unfamiliar queries as the model tries to make sense of them based on its overly narrow training. RAG can expand the training set and reduce hallucinations. Powering data engineering automation for AI and ML applications Learn how Nexla helps enhance LLM models Enhance LLM models like GPT and LaMDA with your own data Connect to any vector database like Pinecone Build retrieval-augmented generation (RAG) pipelines with no code Comparing RAG to model fine-tuning and building your own model RAG represents the ideal approach for applications where real-time data integration is key, such as dynamic content generation and improved decision-making systems. Developing new models for unexplored or highly specialized tasks allows exploration beyond the current boundaries of AI technology and is thus necessary for cutting-edge AI research. In contrast, a fine-tuned model is well suited for applications where the general capabilities of an AI model need to be directed towards a specific type of data or task, like specialized language understanding, targeted content generation, or domain-specific data analysis Purpose The purpose of RAG is to improve the contextual accuracy and relevance of AI-generated content. It is primarily used in scenarios where the integration of relevant external information is the focus, like real-time data analysis, question-answering systems, or context-aware chatbots. Model fine-tuning may be employed to adapt a general-purpose pretrained model to specific tasks or domains. It’s ideal for tasks where a base model’s broad knowledge needs to be honed for specialized topics, such as legal document analysis or medical diagnosis. Building your own model is often chosen when addressing a highly specialized or novel task that existing models do not address effectively. It’s used for groundbreaking research, dealing with unique business needs, or experimenting with new AI methodologies. Resource requirements In terms of resource requirements, implementing RAG requires setting up a system that not only generates content but also retrieves relevant data in real-time. It demands expertise in both natural language processing and information retrieval systems. In contrast, model fine-tuning is generally more complex than RAG. It requires a dataset for fine-tuning and computational resources for additional training even though it utilizes the existing base model’s capabilities. Building your own model is the most resource-intensive and complex approach. This process involves designing a model architecture, collecting and processing training data, training the model, and then rigorously testing it. It requires significant AI expertise, data, and computational power. Customization RAG is highly adaptable when it comes to customization. However, its adaptability is connected to the quality and scope of the external data sources it can access. Fine-tuning the model allows for customization to specific tasks while building on the foundation of a pre-trained model. However, it is more complex for several use cases Building your own model offers the most customization and adaptability. The model can be tailored to very specific needs and can implement new approaches, though this comes with the challenge of developing everything from the ground up. Use cases for retrieval-augmented generation RAG has a wide range of applications across various industries: Customer service: RAG is transforming chatbots by enabling them to provide more accurate and relevant responses. A RAG-improved chatbot can use real-time information to answer customer queries, making interactions more helpful and efficient. Content creation and journalism: RAG can aid in generating articles and reports that are rich in context and facts. It can apply the latest data and references, ensuring that the content is not only well-written but also factually accurate and current. Healthcare: RAG improves decision-making by providing relevant and accurate medical information. For instance, it can assist doctors and researchers by quickly retrieving the latest research findings or clinical data relevant to a patient’s case or medical condition. Education and research: RAG can improve learning tools and research. Students and researchers can be equipped with the most current information on various topics, making discovery processes more efficient and comprehensive. Legal research and compliance analysis: RAG can retrieve and incorporate the latest legal precedents, regulations, and case studies to assist in legal decision-making and to ensure compliance. E-commerce: RAG helps with personalizing customer experiences. By retrieving and processing customer data and current market trends, it can offer customized product recommendations and improve customer engagement. Financial analysis: RAG can improve forecasting and analysis by integrating the most recent market data, financial reports, and economic indicators, leading to more informed and timely investment decisions. Example of retrieval augmented generation One convincing real-world example of RAG can be found in the field of healthcare, particularly in terms of aiding medical diagnosis and treatment planning. In the medical field, accurate diagnosis and effective treatment planning are very important but often challenging due to the continuously evolving nature of medical knowledge. Doctors need to stay up to date with the latest research findings, treatment methods, and clinical trials to provide the best care to patients. This is where RAG comes into play. When healthcare staff input symptoms or medical queries into an AI system equipped with RAG, the system searches through medical databases and research papers to retrieve the most current and relevant information. This could include recent studies on similar symptoms, the latest treatment protocols, or new drug efficacy reports. The retrieved information is then integrated by the RAG system to provide suggestions or insights that may assist in diagnosing the patient’s condition more accurately. For instance, if a patient presents with a set of symptoms that are rare or atypical, RAG can help by pulling in information from recent case studies or medical research that the doctor might not be immediately aware of. Similarly, for treatment planning, RAG can provide the latest information on drug interactions, side effects, and the success rates of various treatment options. This is particularly valuable in the case of diseases like cancer, where treatment advancements are frequent. RAG can also help with personalizing patient care. By accessing current and actual research and data customized to the specifics of a patient’s condition, the system can aid in preparing a treatment plan that is best suited to the individual patient’s needs. The practical benefits are seen in time efficiency: RAG saves time for medical professionals, allowing them to focus more on patient care rather than spending extensive time on research. By facilitating more accurate diagnoses and effective treatment plans, RAG can directly contribute to better patient outcomes. However, implementing such an application in practice requires some ethical considerations —primarily around data used in the retrieval process. The data for search must be carefully selected to meet privacy and security regulations. A human expert is essential to validate all RAG output, given the sensitive nature of healthcare applications. Challenges While RAG offers significant benefits, it also faces several technical, operational, and ethical challenges during its development and implementation: Technical challenges: One technical challenge is seen in data complexity, where RAG systems deal with complex datasets. Managing and processing this data efficiently is a significant technical issue. The system needs to understand and categorize diverse types of data, which requires advanced algorithms and computing power. Another technical challenge is the integration of non-textual modalities, where seamless integration of the retrieval and generation components can be challenging. The system must not only fetch relevant data but also understand and use it correctly in generating responses, which demands a high level of synchronization between different AI components. Operational challenges: These include scalability and maintaining the system. As RAG systems are scaled up to handle more queries or larger datasets, maintaining performance and speed becomes a challenge. It is very important to ensure that these systems can operate efficiently at a larger scale without a delay in response quality or speed. Furthermore, regularly updating the system to incorporate the latest algorithms and data sources is an important step for keeping RAG systems effective. This ongoing maintenance requires resources and continuous technical oversight. Ethical challenges: The two most important here are biases and data privacy. There’s a risk of bias in the responses generated by RAG systems because these biases can be present in the external data sources from which the system retrieves information. Ensuring neutrality in responses is a complex ethical challenge. RAG systems also often access and process large amounts of data, some of which might be sensitive. Ensuring data privacy and complying with data protection regulations is a significant ethical and legal concern. A critical aspect of implementing RAG is scalability, which is one of the operational challenges mentioned above. The failure of many AI projects is often attributed to scaling challenges. These challenges are various, encompassing not just the expansion of the user base but also extending to integrating more data sources, adjusting parameters, improving generative models with retrieved data, and coordinating efforts across larger data engineering teams. Unlock the Power of Data Integration. Nexla's Interactive Demo. No Email Required! Tour the Product In addressing these scaling challenges, tools like Nexla can be very beneficial. Nexla’s platform offers solutions that can simplify the scaling process by providing: Improved data integration: Since RAG systems require access to diverse data sources, Nexla can facilitate the efficient integration of these sources, ensuring that the AI system has a continuous stream of relevant and current data to draw from. Parameter tuning and model augmentation: Scaling an AI system like RAG involves adjusting various parameters to maintain performance. Nexla’s tools can aid in this process, helping optimize the balance between retrieved data and generative model outputs. Collaborative environment for larger teams: With the expansion of an AI project, the team working on it also grows. Nexla can provide a collaborative framework that enables larger data engineering teams to work together effectively, ensuring that the scaling up of the AI system is smooth and coordinated. Production-grade AI pipeline management: This involves ensuring that the AI system remains efficient, reliable, and accurate as it scales, a critical requirement for the successful implementation of RAG systems. Nexla’s no-code and low-code support for RAG By integrating tools like Nexla, which are designed to address the specific challenges of scaling AI projects, businesses can improve the capabilities of their RAG systems. Best practices Implementing RAG effectively requires a wise approach to ensure its success and sustainability. Here are some actions to consider in a variety of areas: Ensuring data quality: Continuously update the data sources used by the RAG model by setting up a schedule for regular data refreshes to keep the information current. Integrate various data sources to reduce bias by sourcing data from varied and credible databases, journals, and other relevant platforms. Model training and maintenance: Periodically retrain the RAG model with new datasets to adapt the model to evolving language use and information. Implement tools and procedures for constantly monitoring the model’s output, including setting up metrics to track accuracy, relevance, and any biases in the responses. Planning for scalability: From the outset, design the RAG system architecture to handle scaling up, considering factors like increased data volume and user load. Allocate appropriate resources for computational infrastructure, including cloud-based solutions or in-house servers, to handle intensive data processing. Ethical considerations: Establish strict protocols for data privacy, security, and compliance with data protection laws. Stay informed and compliant with AI ethics and regulations, which might involve conducting regular audits and adjusting the system. User experience optimization: Develop user interfaces that are easy and intuitive to navigate, making the system accessible to all users. Ensure that the AI’s responses are clear, concise, and understandable. Feedback integration and testing: Before deployment, thoroughly test the RAG system in various real scenarios to ensure its reliability. Establish mechanisms for receiving and integrating user feedback into ongoing system improvements. Expert collaboration: Work closely with AI researchers and data scientists to ensure the system is built on cutting-edge knowledge. Encourage collaboration between technical and non-technical teams for a holistic approach, blending AI expertise with domain-specific knowledge. Discover the Transformative Impact of Data Integration on GenAI Watch Expert Panel Conclusion RAG stands out as a significant advancement in AI, improving existing traditional language models by integrating real-time external data for more accurate and context-correct responses. This technology has proven its applicability across various industries, from healthcare to customer service, revolutionizing information processing and decision-making. However, its implementation comes with challenges, including technical complexity, scalability, and ethical considerations, necessitating best practices for effective and responsible use. The future of RAG is promising, with the potential for further advancements in AI accuracy, efficiency, and adaptability. As RAG continues to evolve, it will continue to transform AI into an even more powerful tool for various applications, driving innovation and improvement in numerous fields. Navigate Chapters: Previous Chapter Next Chapter Continue reading this series Chapter 1 AI Infrastructure: Tutorial & Best Practices Learn about the key concepts and best practices for data storage, processing, training, inference hardware, and model deployment and hosting in the field of AI infrastructure. Read More Chapter 2 Large Language Models (LLMs) Tutorial Learn how Large Language Models revolutionized Natural Language Processing and their best practices, use cases, and challenges. Read More Chapter 3 Vector Embedding Tutorial & Example Learn how vector embeddings are used to convert non-numeric data into vectors for machine learning. Read More Chapter 4 Vector Databases: Tutorial, Best Practices & Examples Learn about the significance, types, use cases, challenges, and best practices of vector databases, with an exploration of popular solutions like Pinecone, Milvus, Redis, and MongoDB. Read More Chapter 5 Retrieval-Augmented Generation (RAG) Tutorial & Best Practices Learn how retrieval-augmented generation (RAG) combines traditional AI language models with dynamic external data to improve machine understanding and responses. Read More Chapter 6 LLM Hallucination—Types, Causes, and Solution Learn about LLM hallucination, why it happens and how you can use data to improve LLM reliability and ethical use. Read More Chapter 7 Prompt Engineering vs. Fine-Tuning—Key Considerations and Best Practices Learn about how fine-tuning and prompt engineering work, their impact on customization and accuracy in specialized tasks, and how to choose between the two. Read More Chapter 8 Model Tuning—Key Techniques and Alternatives Learn how to improve the performance of your machine learning or large language model through hyperparameter tuning techniques. Open AI tutorial included. Read More Chapter 9 Prompt Tuning vs. Fine-Tuning—Differences, Best Practices and Use Cases Learn prompt tuning vs. fine-tuning in customizing large language models. Explore parameter adjustments, input format, challenges, real-world examples and more. Read More Chapter 10 Data Drift in LLMs—Causes, Challenges, and Strategies Learn about how data drift impacts LLM output quality over time and the need for continuous data integration and re-training to minimize the impact. Read More Chapter 11 LLM Security—Vulnerabilities, User Risks, and Mitigation Measures Learn about all aspects of LLM security—from model design to prompt-based and user-based risks. Implement best practices to protect users and your organization. Read More Chapter 12 LLMOps—Benefits, Implementation, and Best Practices Learn what is LLMOps and why it is different from MLOps. Learn how it works in the LLM lifecycle, implementation details, and best practices for LLM developers. Read More Getting Started Data Integration 101 What is a Data Product? What is Data Fabric? Getting Started with GenAI Customer Stories Nexla Blog Product Tour Free Trial Product Platform Overview Data Fabric Architecture Data Integration Nexsets (Data Products) Data Monitoring Advanced RAG GenAI Ready Data Pricing Schedule Demo Solutions AI & GenAI Inter Company Integrations Data Engineering Data Analytics Asset Management Financial Services Retail & Ecommerce Health Care & Life Sciences Resources Universal Connectors Blog Case Studies Ebooks Guides Webinars Documentation All Resources Company About Nexla Careers Partners Events Press Kit Contact Sales Customer Support © 2025 Nexla, Inc. All rights reserved. Privacy Policy Terms of Use

Source: https://medium.com/towards-data-science/ragops-guide-building-and-scaling-retrieval-augmented-generation-systems-3d26b3ebd627
RAGOps Guide: Building and Scaling Retrieval Augmented Generation Systems | by Abhinav Kimothi | Towards Data Science | Medium Open in app Sign up Sign in Write Sign up Sign in Member-only story Learning Retrieval Augmented Generation RAGOps Guide: Building and Scaling Retrieval Augmented Generation Systems The Architecture, Operational Layers, and Best Practices for Effective RAG Implementation Abhinav Kimothi · Follow Published in Towards Data Science · 22 min read · Nov 26, 2024 -- 2 Share RAG Operations (Source: Image Generated by Author using Dall-E 3) It may not come as a surprise that retrieval augmented generation (RAG) is among the most applied techniques in the world of generative AI and large language model-powered applications. In fact, according to a Databricks report, more than 60% of LLM-powered applications use RAG in some form. Therefore, in the global LLM market, which is currently valued at around $6 Billion and growing at almost 40% YoY, RAG undoubtedly becomes one of those crucial techniques to master. Building a PoC RAG pipeline is not too challenging today. There are readily available examples of code leveraging frameworks like LangChain or LlamaIndex and no-code/low-code platforms like RAGArch, HelloRAG, etc. Conversing with Documents: Unleashing the Power of LLMs and LangChain Using LangChain to build a Streamlit app for a QnA with documents and links. medium.com A production-grade RAG system, on the other hand, is composed of several specialised layers… -- -- 2 Published in Towards Data Science 801K Followers ·Last published 20 hours ago Your home for data science and AI. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals. Follow Written by Abhinav Kimothi 1.4K Followers ·350 Following Co-founder and Head of AI @ Yarnit.app || Author : A Simple Guide to RAG (https://mng.bz/jXJ9) || AIML since 2007 || BITS-Pilani, ISB-Hyderabad Follow Responses (2) See all responses Help Status About Careers Press Blog Privacy Terms Text to speech Teams

Source: https://www.google.com/url?url=https://support.google.com/websearch?p%3Dfeatured_snippets%26hl%3Den-BE&rct=j&q=&esrc=s&opi=89978449&usg=AOvVaw0-xXX9L80985UB-G9Jvm5i&hl=en-BE&sa=X&ved=2ahUKEwjsuoSztK2LAxVJTKQEHYsbIGMQrpwBegQIIxAC


Source: https://www.kapa.ai/blog/rag-best-practices
RAG Best Practices: Lessons from 100+ Technical Teams - kapa.ai - Instant AI answers to technical questions Product Solutions Customers Security Resources Pricing Docs Sign In Request Demo Book Demo Sign In Request Demo Back Nov 11, 2024 RAG Best Practices: Lessons from 100+ Technical Teams RAG Best Practices: Lessons from 100+ Technical Teams Production insights from implementing retrieval augmented generation at OpenAI, Docker, Langchain, CircleCI, and more. by by by Emil Sorensen Emil Sorensen Emil Sorensen Overview 1. Carefully Curate Your Data Sources 2. Implement a Robust Refresh Pipeline 3. Build Comprehensive Evaluations 4. Optimize Prompting for Your Use Case 5. Implement Security Best Practices 6. Conclusion: Making it Work Let's start with a hard truth: Most retrieval augmented generation (RAG) implementations fail to make it out of the proof-of-concept stage. A rencet global survey of 500 technology leaders shows that more than 80% of in-house generative AI projects fall short. But it doesn't have to be this way. At kapa.ai, we've worked with over 100 technical teams like Docker, CircleCI, Reddit and Monday.com to implement RAG-based systems in production. Below is what we've learned about making it past the proof-of-concept stage. Before diving what we've learned, here's a quick primer on RAG. Think of it as giving an AI a carefully curated reference library before asking it questions. Instead of relying solely on its training data (which can lead to hallucinations), RAG-based systems first retrieve relevant information from your knowledge base, then use that to generate accurate answers. It's like the difference between asking someone to recall a conversation from memory versus having them reference the actual transcript. In practice, this means indexing your knowledge in a vector database—think super-powered search engine—and connecting it to large language models that can use this information to answer questions naturally and accurately. This approach has become the go-to method for building reliable AI systems that can discuss your specific product or domain. 1. Carefully Curate Your Data Sources The old programming adage "garbage in, garbage out" holds especially true for RAG systems. The quality of your RAG system is directly proportional to the quality of your knowledge base. Here's what many teams get wrong: They dump their entire knowledge base—every Slack message, support ticket, and documentation page from the last decade—into their RAG system. They assume more data equals better results. Instead, we recommend starting with your core content sources. For technical AI assistants, that often includes primary sources like: Technical documentation and API references Product updates and release notes Verified support solutions Knowledge base articles Once you have your primary sources covered, you can thoughtfully expand to secondary sources like Slack channels, forum discussions, and support tickets. But be selective—apply filters like recency (only posts from last year) and authority (only replies from verified community members). When it comes to implementation, you have options. You can use open-source tools like LangChain, which provides information retrieval connectors to various data sources including Slack. Or if you prefer a no-code approach with pre-built filters and data connectors, that's exactly what we've built at kapa.ai (that's us 👋). Either way, the key is to be intentional about what data you include and how you filter it. A final important consideration is separating your public knowledge sources from private data. We recommend maintaining distinct vector stores: one for external data like public documentation, and another for sensitive enterprise data and relevant documents. This separation ensures better security and makes it easier to manage access controls. 2. Implement a Robust Refresh Pipeline Your data sources for an AI agent are not static—they are constantly evolving. Just look at companies like Stripe, whose documentation repositories sees dozens of updates daily. Your RAG system needs to keep up with this pace to stay up to date with changes in the underlying knowledge base and latest information. Here's what happens if you don't get this right: Your AI starts giving outdated answers, missing critical updates, or worse, mixing old and new information in confusing ways. Yet we've seen teams treat their RAG knowledge base like a one-time setup. A production-ready system needs automated refresh pipelines. But here's the trick: you don't want to reindex everything every time. That's expensive and unnecessary. Instead, build a delta processing system similar to a Git diff that only updates what's changed. Think of it like continuous deployment for your AI's knowledge. Key pipeline components to consider: Change detection system to monitor documentation updates Content validation to catch breaking layout changes Incremental updating for efficiency Version control to track changes Quality monitoring to prevent degradation For technical teams building this in-house, here's a practical approach: Set up cron jobs to regularly check for content changes Use a message queue (like RabbitMQ) to handle update processing Implement validation checks before indexing Deploy monitoring to track refresh performance This can work well, but it requires significant engineering effort to build and maintain. That's actually why we built kapa.ai with automatic content refreshes out of the box. Our platform handles all the complexity of keeping your knowledge base current—from detecting changes to validating content before it goes live. Whether you build it yourself or use a platform like ours, the key is making sure your RAG system stays as current as your knowledge sources. After all, your AI assistant is only as good as the information it has access to. As a sidenote: this ability to update your knowledge store without retraining the core model is one of the key benefits of RAG versus fine-tuning. It's what makes RAG particularly powerful for teams with frequently changing documentation. 3. Build Comprehensive Evaluations Here's where most teams drop the ball: they lack rigorous evaluation frameworks. When you're building RAG applications, you're juggling countless parameters—chunk sizes, embedding models, context windows, citation strategies, and more. Each choice cascades into the next, creating a maze of optimization possibilities. Modern RAG architectures have evolved far beyond simple embeddings and retrieval. Companies like Perplexity have pioneered techniques like query decomposition, while others push the boundaries with cross-encoder reranking and hybrid search approaches. But here's the catch: you can't optimize what you can't measure. Vibe checks ("does this answer look right?") might work for proof-of-concepts. You know the drill—throw a few test questions at your system, eyeball the responses, and call it a day. While that's a fine way to start, it won't get you to production. You have two options for evaluation frameworks: Open-source tools like Ragas provide out-of-the-box metrics for answer correctness, context relevance, and hallucination detection Custom evaluation frameworks built specifically for your use case While tools like Ragas offer a great starting point, they often need significant extension to match real-world needs. Your evaluation framework should cover: Query understanding accuracy Citation and source tracking Response completeness Hallucination detection The key is building evaluations that match your specific use case. If you're building a product AI copilot for sales, your evaluation criteria will be very different from a system designed for customer support or legal document analysis. At kapa.ai, we've taken a focused approach: optimizing specifically for accurately answering product questions. Rather than trying to build a one-size-fits-all solution, we've spent years developing evaluation frameworks that reflect real-world usage patterns and customer feedback in this specific domain. We've learned that academic benchmarks and generic evaluation tools only take you so far—you need evaluations that truly reflect your users' needs. Whether you build your own evaluation framework, extend open-source tools, or use a specialized platform like ours, remember: every improvement to your RAG system should be validated through rigorous testing. It's the only way to ensure your optimizations actually improve the end-user experience rather than just looking good on paper. 4. Optimize Prompting for Your Use Case Getting your prompting strategy right is crucial for a production RAG system. It's not just about crafting clever prompts—it's about building a comprehensive strategy that aligns with your specific use case. Here are the key principles we've learned from working with technical teams: Ground Your Answers The first rule of RAG systems is: never let your AI make things up. Your prompting strategy should explicitly instruct the model to (1) only use provided context, and (2) include clear citations for claims. B. Know When to Say "I Don't Know" This might sound obvious, but it's crucial: your AI should confidently acknowledge its limitations. A good RAG system should (1) recognize when it lacks sufficient information, (2) suggest alternative resources when possible, and (3) never guess or hallucinate answers. C. Stay on Topic A production RAG system needs clear boundaries. Your prompts should ensure the AI (1) stays within its knowledge domain, (2) refuses to answer questions about unrelated products, and (3) maintains consistent tone and formatting. D. Handle Multiple Sources Your prompting strategy needs to elegantly handle information from different sources. This includes (1) synthesizing information from multiple documents, (2) handling version-specific information, (3) managing conflicting information, and (4) providing relevant context. These principles work together. For instance, when handling multiple sources, your system might need to say "I don't know" about newer product versions, or when grounding answers, it might need to acknowledge conflicting information between sources. Implementation Options When it comes to implementing these principles, you have several approaches: DIY Approach: Tools like Anthropic's Workbench let you iterate on prompts rapidly, testing different approaches against various scenarios. It's particularly useful for finding the right balance between informative and cautious responses for your specific use case. Managed Solutions: At kapa.ai, we've built our Answer Engine to handle these challenges out of the box, continuously balancing these principles to provide reliable, accurate responses to technical questions. The key is to test your prompting strategy extensively with real-world scenarios before deploying to production. Pay particular attention to edge cases and potentially problematic queries that might tempt your system to guess or hallucinate. 5. Implement Security Best Practices Security can't be an afterthought for production RAG systems. Two major risk factors make RAG systems particularly vulnerable: prompt hijacking (where users craft inputs to manipulate the system's behavior) and hallucinations (where systems generate false or sensitive information as discussed above). If you're handling customer data or internal documentation, these risks become even more critical. However there's a long tail of additional risks that are often not covered for production systems, which we will cover here. PII Detection and Masking Your RAG system needs to handle sensitive information carefully. Users often accidentally share sensitive data in their questions—API keys in error messages, email addresses in examples, or customer information in support tickets. Once this information enters your system, it's hard to guarantee it's completely removed. B. Bot Protection and Rate Limiting The moment you deploy a public-facing RAG system, it becomes a target. We've seen cases where unprotected endpoints got hammered with thousands of requests per minute, not just driving up costs but potentially extracting sensitive information. Essential protections include rate limiting, reCAPTCHA integration, and request validation. Modern solutions are emerging to address these challenges. Cloudflare recently launched their Firewall for AI, showing how the industry is evolving to protect AI systems at scale. C. Access Controls Not everyone should see everything. Without proper access controls, internal documentation or customer data can leak across team boundaries. Role-based access control ensures your knowledge base stays secure while remaining accessible to the right users. More importantly, it lets you track who's accessing what. You can build these protections yourself using established security libraries and services like Cloudflare's AI Firewall. Alternatively, managed solutions like kapa.ai include these protections out of the box in a SOC II Type II certified environment. The key is implementing these protections before you deploy—not after an incident. 6. Conclusion: Making it Work After working with hundreds of teams, here's what successful RAG implementations have in common: Start Small, Start Strong Begin with: A focused set of high-quality documentation One or two well-defined use cases Clear evaluation metrics Basic security measures B. Common Pitfalls to Avoid Including too much data too quickly Neglecting refresh pipelines Relying on manual testing Treating security as an afterthought Whether you build it yourself with open-source tools or use a managed solution like kapa.ai, these principles remain the same. The key is treating your RAG system as core infrastructure rather than an experimental add-on. Want to learn more about implementing RAG for your technical product questions? If you are interested in seeing how kapa.ai can transform your knowledge base into an intelligent assistant, you can test it out by requesting a demo here. Trusted by hundreds of COMPANIES to power production-ready AI assistants Turn your knowledge base into a production-ready AI assistant Request a demo to try kapa.ai on your data sources today. Request Demo Request Demo Request Demo Instant Al answers for your techincal product. Resources Documentation Data Sources Deploying Kapa Analytics API Reference Security PII Anonymisation Role Based Access SOC 2 Type II Company About Careers Podcast Blog Terms of Service Privacy Policy Socials Twitter Linkedin YCombinator Profile © 2024 Kapa.ai Inc. All rights reserved. Instant Al answers for your techincal product. Resources Documentation Data Sources Deploying Kapa Analytics API Reference Security PII Anonymisation Role Based Access SOC 2 Type II Company About Careers Podcast Blog Terms of Service Privacy Policy Socials Twitter Linkedin YCombinator Profile © 2024 Kapa.ai Inc. All rights reserved. Instant Al answers for your techincal product. Resources Documentation Data Sources Deploying Kapa Analytics API Reference Security PII Anonymisation Role Based Access SOC 2 Type II Company About Careers Podcast Blog Terms of Service Privacy Policy Socials Twitter Linkedin YCombinator Profile © 2024 Kapa.ai Inc. All rights reserved.

Source: https://github.com/resources/articles/ai/software-development-with-retrieval-augmentation-generation-rag
Software Development with Augmented Retrieval · GitHub Skip to content GitHub Copilot is now available for free. Learn more Navigation Menu Toggle navigation Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} Articles AI Enhancing software development with retrieval-augmented generation Table of contents Open Table of contents What exactly is retrieval-augmented generation (RAG)? How does RAG work with traditional LLMs? The current software development landscape Real-world RAG development applications Implementing RAG in developer workflows Equipping your RAG toolkit How to maximize your team’s RAG potential The future of RAG in software development Featured GitHub’s DevOps Solution Learn why 90% of the Fortune 100 use GitHub to build, scale, and deliver secure software. Start your journey with GitHub You're a software developer with a vast library of code snippets, documentation, and best practices at your fingertips. Now, imagine using a tool that instantly retrieves the most relevant information from that library and uses it to generate more accurate, context-aware code suggestions. That's the potential of retrieval-augmented generation (RAG) for software developers. Retrieval-augmented generation isn’t just another buzzword in the ever-expanding universe of AI. It's a game-changing approach that combines the power of large language models (LLMs) with the precision of information retrieval systems. Whether you're wrestling with complex algorithms, trying to optimize your code, or simply looking for the right design pattern, RAG-powered tools can provide suggestions that aren’t just syntactically correct, but also aligned with your project's specific context and requirements. But RAG isn't just about making developers' lives easier (although that's a pretty sweet benefit). It's about elevating the entire software development process. By bridging the gap between general coding knowledge and project-specific information, RAG has the potential to: Accelerate development cycles. Improve code quality and consistency. Enhance documentation and knowledge sharing. Facilitate faster onboarding for new team members. What exactly is retrieval-augmented generation (RAG)? Think of RAG as a supercharged memory bank with a pair of very smart glasses. It combines the creative power of large language models (LLMs) with the precision of information retrieval systems. Here's how it works: Retrieval: RAG uses advanced search algorithms to find relevant information from external sources—think documentation, code repositories, or even Stack Overflow discussions. Augmentation: This is where RAG adds the retrieved information to the input before passing it to the LLM. Generation: Finally, the LLM takes this augmented input and generates a response. But now, instead of just relying on its training data, it has access to specific, relevant information. How does RAG work with traditional LLMs? RAG isn’t just another LLM—it’s a tool that helps LLMs provide better results. It can quickly look up accurate, up-to-date, relevant information and pass it to the rest of the model. For users, this leads to: Increased accuracy: By grounding responses in retrieved information, RAG reduces the likelihood of AI "hallucinations"—which is when AI generates plausible-sounding but incorrect information. Enhanced contextual relevance: RAG can pull in project-specific documentation or coding standards, helping you make sure that the generated code aligns with your team's practices. Up-to-date knowledge: Unlike static LLMs, RAG can access the latest information, keeping pace with rapidly evolving technologies and best practices. More transparency: RAG can cite sources for its suggestions, helping developers verify and understand the reasoning behind the generated code. The magic of RAG lies in its ability to pair the creative, pattern-matching strengths of LLMs with the increased accuracy and specificity of information retrieval. This powerful combination can significantly enhance your development workflow. The current software development landscape Software development has seen some exciting changes in recent decades, but it’s also an increasingly complex world. Developers have seen many “emerging technologies” become commonplace, such as: Cloud-native everything: Microservices, containerization, and serverless architectures are no longer the new kids on the block. AI and ML integration: From chatbots to predictive analytics, AI is quickly becoming a standard part of the developer's toolkit. DevOps and CI/CD: The line between development and operations continues to blur, with automation taking center stage. Cross-platform development: The idea of “write once, run anywhere” is no longer a pipe dream but an expectation. Open-source dominance: The collaborative spirit of open-source has won the day, with even the biggest tech giants embracing and contributing to open-source projects. Of course, new challenges always accompany big changes. Today’s developers face: Complexity overload: With the proliferation of frameworks, libraries, and tools, choosing the right tech stack is overwhelming (to say the least). Security nightmares: In a world of increasing cyber threats, writing secure code is no longer optional – it's a full-time job in itself. Technical debt: Legacy systems don't magically disappear, and developers are struggling to maintain these systems while building new features. Rapid change: Keeping up with new technologies, languages, and best practices is like trying to drink from a firehose. Documentation woes: Writing and maintaining good documentation often feels like a chore, but its absence can lead to chaos. Scalability struggles: Building systems that can handle explosive growth without exploding themselves is a constant challenge. As developers stand at this crossroads of challenges and opportunities, one thing is clear: the tools and practices that got them here won't be enough to take them where they need to go. They need smarter, more context-aware assistance to navigate this complex landscape. Real-world RAG development applications As RAG technology evolves, we're likely to see even more innovative uses that push the boundaries of what's possible in software development. Examples include: Real-time code assistance Use case: You're working on a new feature in your team's massive codebase. As you type, RAG-powered tools analyze your code, project history, and documentation to offer intelligent suggestions. def process_user_data(user_id): # RAG suggests: "Consider using the get_user_by_id function from user_utils.py" user = get_user_by_id(user_id) # RAG warns: "This operation may be slow for large datasets. Consider using bulk_update from data_ops.py for better performance" update_user_preferences(user) # RAG suggests: "Don't forget to log this operation as per our auditing guidelines" log_user_activity(user_id, "process_user_data") RAG doesn't just complete your code; it takes into account your project's context and helps you write more efficient code that’s aligned with your team's practices. Automated documentation generation Use case: You've just finished refactoring a critical module. Instead of spending hours updating the docs, you let RAG do the heavy lifting. RAG analyzes your code changes and existing documentation. It generates updated documentation, including: Function descriptions Parameter explanations Usage examples Changes from the previous version RAG even suggests where to update related documentation in other parts of your project. The result? Documentation that's in sync with your code, saving you time and reducing confusion for your team. Debugging and troubleshooting Use case: You're faced with a cryptic error message in a part of the codebase you're not familiar with. RAG springs into action and: Analyzes the error message and surrounding code context. Retrieves relevant information from your project's issue tracker, Stack Overflow, and internal knowledge base. Presents you with: Potential causes of the error Similar issues that have been resolved in the past Suggested fixes with code snippets # Original error: TypeError: cannot unpack non-iterable int object for x, y in some_function(): # RAG suggests: "It seems some_function() is returning an int instead of an iterable. # Consider wrapping the return value in a tuple or list, or check if the function is # meant to return multiple values." process_data(x, y) With RAG, you're not just fixing errors; you're learning and improving your codebase along the way. Code review assistance Use case: You're reviewing a pull request from a junior developer. RAG assists you by: Automatically checking the code against your team's style guide and best practices. Highlighting potential performance issues or security vulnerabilities. Suggesting improvements based on patterns in your team's high-quality code. Providing context and explanations for each suggestion, helping the junior developer learn. def get_user_data(user_id): # RAG comment: "Consider using a context manager for database connections # to ensure proper resource handling." db = connect_to_database() data = db.query(f"SELECT * FROM users WHERE id = {user_id}") # RAG warning: "Potential SQL injection vulnerability. Use parameterized queries instead." db.close() return data This helps take code reviews from a bottleneck to a smooth, educational process that catches issues early and spreads knowledge effectively. Implementing RAG in developer workflows RAG is poised to be one of the most useful tools in your dev toolkit—but it's not a magic wand. To get the best results, it must be adopted and used thoughtfully. Assess your needs: Before diving in, take a moment to identify where RAG could have the biggest impact in your development process. Is it in code assistance, documentation, debugging, or code reviews? Start small: Pick a single use case to start with, like improving code completion in your integrated development environment (IDE). Prepare your knowledge base: RAG is only as good as the information it can retrieve. Start organizing your codebase, documentation, and other relevant resources in a way that's easily searchable. Choose your tools: Look for IDEs or plugins that offer RAG-like features. Many modern development environments are starting to incorporate these capabilities. Experiment and iterate: Start using RAG-powered tools in your daily coding. Pay attention to what works well, what doesn't, and be prepared to adjust your approach. Equipping your RAG toolkit Look for these types of tools and resources when getting started: RAG-enabled IDEs and plugins: Many popular IDEs are incorporating RAG-like features. Look for capabilities like context-aware code completion and documentation generation. Vector databases: These are crucial for efficient information retrieval in RAG systems. Explore open-source options that can help you store and query your code and documentation effectively. Embedding models: These convert your code and documentation into a format that RAG systems can easily search. Look for pre-trained models suitable for code and technical documentation. LLM integration tools: These help you connect large language models with your retrieval system. Explore open-source libraries that facilitate this integration. Documentation tools: Look for documentation generators that can tap into your codebase and existing docs to create and maintain up-to-date documentation. How to maximize your team’s RAG potential Keep your knowledge base fresh: Regularly update the information sources that your RAG system draws from. This includes code comments, documentation, and any external resources you're using. Balance automation and human oversight: While RAG can automate many tasks, human judgment is still crucial. Use RAG as a powerful assistant, not a replacement for critical thinking. Customize for your context: Tune your RAG system to understand your project's specific terminology, coding style, and best practices. Encourage team adoption: RAG becomes more powerful when everyone's on board. Provide training and support to help your team embrace RAG-powered tools. Monitor and evaluate: Keep track of how RAG impacts your development process. Are you seeing improvements in code quality or reduced debugging time? Use these metrics to refine your approach. Respect privacy and security: Be mindful of what data you're feeding into the system to ensure that your RAG implementation doesn't accidentally expose sensitive information. Contribute to the community: As you gain experience with RAG, consider sharing your insights and even contributing to open-source RAG tools As you dive deeper into RAG, you'll discover more sophisticated ways to integrate it into your development workflow. Just remember that implementing RAG is a journey, not a destination. Start small, experiment, and gradually expand your use as you become more comfortable with it. Soon, you'll wonder how you ever coded without it. The future of RAG in software development RAG will, of course, bring challenges. But it also promises to make development more efficient, accessible, and innovative. As with any powerful tool, the key will be learning to use it wisely. When you do, it can free you from the mundane so you can focus on the big picture. It’s impossible to know how exactly AI and RAG will change software development, but there are already some emerging trends to keep an eye on, such as: Hyper-personalized coding assistants: Imagine an AI that doesn't just assist with code, but analyzes and takes into account the context of your work. Future RAG systems will learn your coding style, preferences, and even your common mistakes, offering tailored suggestions for whatever you’re working on. Cross-language and cross-paradigm understanding: RAG applications of the future won't be limited to a single programming language or paradigm. They'll be able to assist you in translating concepts from one language to another, or even suggest how to implement object-oriented patterns in a functional programming style. Ethical and bias-aware suggestions: As we become more aware of biases in AI systems, future RAG models will likely include features to detect and mitigate bias in code suggestions, promoting more inclusive and fair software development practices. Natural language code generation: While we're already seeing early versions of this, future RAG systems will get much better at turning natural language descriptions into complex, efficient code structures. This helps to make programming more accessible to non-developers and speed up prototyping for experienced devs. For the software industry at large, AI and RAG are likely to have a long-lasting and far-reaching impact, including: Democratization of coding: As RAG makes coding more accessible, we might see a surge in "citizen developers"—non-traditional programmers who can create sophisticated applications with the help of AI. Shift in developer skills: The focus may shift from memorizing syntax and algorithms to higher-level problem-solving and system design skills, as RAG takes care of more low-level coding tasks. Rapid prototyping and innovation: With RAG accelerating the coding process, we could see a boom in software innovation. Evolution of programming education: Coding bootcamps and CS programs may need to adapt, focusing more on the skills that AI can't easily replicate and how to effectively work with AI coding tools. New roles in software development: We might see the emergence of new job titles like "AI-assisted development specialist" or "RAG system trainer", focused on optimizing the use of these tools in development workflows. Changes in code ownership and licensing: As AI becomes more involved in code generation, interesting legal and ethical questions may arise about code ownership and licensing with AI-assisted development. Frequently asked questions Why is RAG important for software development? RAG essentially gives your AI assistant a memory upgrade and a direct line to your team's collective knowledge. It helps developers write more accurate, context-aware code faster by pulling in relevant information from your project's docs, codebase, and even external sources. How does a RAG pipeline work? When you ask an AI tool a question, RAG quickly finds relevant info in your project's "library" (codebase, docs, etc.). It then passes this info to the LLM, which uses this extra context to deliver a more informed, project-specific answer. Is RAG suitable for open-source software development? Absolutely! RAG can help maintain consistent coding styles across contributors, assist with documentation, and even help new contributors get up to speed faster. Plus, since RAG systems can learn from the entire project history, they get smarter as more people contribute. How do RAG models help developers? RAG models can suggest code snippets that fit your project's style, help you navigate complex APIs, and even predict potential bugs before they happen. It's like pair programming, but your partner has instant access to best practices, lines of documentation, and the clever solutions that your team has already come up with. Can RAG be integrated with existing development tools? Yes, it can. Many popular IDEs and development tools are starting to incorporate RAG-like features, and there are more plugins being introduced every day. What are the benefits of using open-source tools for RAG? Open-source RAG tools are like a community garden for AI – everyone contributes, and everyone benefits. They offer transparency (you can see how the AI is making decisions), customizability (tweak it to fit your needs), and rapid innovation (the whole community can contribute improvements). Plus, you're not locked into any one vendor's ecosystem. How does RAG enhance code reviews? RAG can automatically check code against your team's style guide, flag potential issues, and even suggest improvements based on your project's best practices. It catches the obvious stuff so human reviewers can focus on the big-picture issues. Plus, it can provide context and explanations, turning every review into a learning opportunity for the whole team. Site-wide Links Subscribe to our developer newsletter Get tips, technical guides, and best practices. Twice a month. Right in your inbox. Subscribe Product Features Enterprise Copilot Security Pricing Team Resources Roadmap Compare GitHub Platform Developer API Partners Education GitHub CLI GitHub Desktop GitHub Mobile Support Docs Community Forum Professional Services Premium Support Skills Status Contact GitHub Company About Customer stories Blog The ReadME Project Careers Newsroom Inclusion Social Impact Shop © 2025 GitHub, Inc. Terms Privacy (Updated 02/2024) Sitemap What is Git? Manage cookies Do not share my personal information GitHub on LinkedIn Instagram GitHub on Instagram GitHub on YouTube GitHub on X TikTok GitHub on TikTok Twitch GitHub on Twitch GitHub’s organization on GitHub You can’t perform that action at this time.

Source: https://www.reddit.com/r/learnmachinelearning/comments/1e9vn18/rag_best_practices/
Reddit - Dive into anything Skip to main content We value your privacy Reddit and its partners use cookies and similar technologies to provide you with a better experience. By accepting all cookies, you agree to our use of cookies to deliver and maintain our services and site, improve the quality of Reddit, personalize Reddit content and advertising, and measure the effectiveness of advertising. By rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of our platform. For more information, please see our Cookie Notice and our Privacy Policy. Open menu Open navigation Go to Reddit Home r/learnmachinelearning A chip A close button Get App Get the Reddit app Log In Log in to Reddit Expand user menu Open settings menu Go to learnmachinelearning r/learnmachinelearning r/learnmachinelearning Welcome to r/learnmachinelearning - a community of learners and educators passionate about machine learning! This is your space to ask questions, share resources, and grow together in understanding ML concepts - from basic principles to advanced techniques. Whether you're writing your first neural network or diving into transformers, you'll find supportive peers here. For ML research, /r/machinelearning For resume review, /r/engineeringresumes For ML engineers, /r/mlengineering Members Online • behitek RAG best practices Tutorial I just published my latest blog post on the Behitek blog: "RAG in Production: Best Practices for Robust and Scalable Systems" 🌟 This article explores how to effectively implement Retrieval-Augmented Generation (RAG) models in production environments. From reducing hallucinations to maintaining document hierarchy and optimizing chunking strategies, this guide covers all you need to know for robust and efficient RAG deployments. Here is the summary content: Try the simple method first. eg. BM25 Maintain the hierarchical structure of your documents. Take a look at the chunking strategy. Consider using text summarization for long documents. Take a look at the format of (search_query, document) pairs. Moving tasks from RAG-frontend to RAG-backend. Read the details here: https://behitek.com/blog/2024/07/18/rag-in-production I'd love to hear your feedback and any additional tips you have. 👇 Read more Top 1% Rank by size Public Anyone can view, post, and comment to this community &nbsp; TOPICS Internet Culture (Viral) Amazing Animals & Pets Cringe & Facepalm Funny Interesting Memes Oddly Satisfying Reddit Meta Wholesome & Heartwarming Games Action Games Adventure Games Esports Gaming Consoles & Gear Gaming News & Discussion Mobile Games Other Games Role-Playing Games Simulation Games Sports & Racing Games Strategy Games Tabletop Games Q&As Q&As Stories & Confessions Technology 3D Printing Artificial Intelligence & Machine Learning Computers & Hardware Consumer Electronics DIY Electronics Programming Software & Apps Streaming Services Tech News & Discussion Virtual & Augmented Reality Pop Culture Celebrities Creators & Influencers Generations & Nostalgia Podcasts Streamers Tarot & Astrology Movies & TV Action Movies & Series Animated Movies & Series Comedy Movies & Series Crime, Mystery, & Thriller Movies & Series Documentary Movies & Series Drama Movies & Series Fantasy Movies & Series Horror Movies & Series Movie News & Discussion Reality TV Romance Movies & Series Sci-Fi Movies & Series Superhero Movies & Series TV News & Discussion RESOURCES About Reddit Advertise Help Blog Careers Press Communities Best of Reddit Topics Reddit Rules Privacy Policy User Agreement Reddit, Inc. © 2025. All rights reserved.

Source: https://www.reddit.com/r/ChatGPTCoding/comments/1e4naf6/applying_rag_to_largescale_code_repositories_guide/
Reddit - Dive into anything Skip to main content We value your privacy Reddit and its partners use cookies and similar technologies to provide you with a better experience. By accepting all cookies, you agree to our use of cookies to deliver and maintain our services and site, improve the quality of Reddit, personalize Reddit content and advertising, and measure the effectiveness of advertising. By rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of our platform. For more information, please see our Cookie Notice and our Privacy Policy. Open menu Open navigation Go to Reddit Home r/ChatGPTCoding A chip A close button Get App Get the Reddit app Log In Log in to Reddit Expand user menu Open settings menu Go to ChatGPTCoding r/ChatGPTCoding r/ChatGPTCoding Welcome to our community! This subreddit focuses on the coding side of ChatGPT - from interactions you've had with it, to tips on using it, to posting full blown creations! Make sure to read our rules before posting! Members Online • thumbsdrivesmecrazy Applying RAG to Large-Scale Code Repositories - Guide Resources And Tips The article discusses various strategies and techniques for implementing RAG to large-scale code repositories, as well as potential benefits and limitations of the approach as well as show how RAG can improve developer productivity and code quality in large software projects: RAG with 10K Code Repos Read more New to Reddit? Create your account and connect with a world of communities. Continue with Email Continue with phone number By continuing, you agree to our User Agreement and acknowledge that you understand the Privacy Policy. Top 1% Rank by size Public Anyone can view, post, and comment to this community Reddit Rules Privacy Policy User Agreement Reddit, Inc. © 2025. All rights reserved. Expand Navigation Collapse Navigation &nbsp; TOPICS Internet Culture (Viral) Amazing Animals & Pets Cringe & Facepalm Funny Interesting Memes Oddly Satisfying Reddit Meta Wholesome & Heartwarming Games Action Games Adventure Games Esports Gaming Consoles & Gear Gaming News & Discussion Mobile Games Other Games Role-Playing Games Simulation Games Sports & Racing Games Strategy Games Tabletop Games Q&As Q&As Stories & Confessions Technology 3D Printing Artificial Intelligence & Machine Learning Computers & Hardware Consumer Electronics DIY Electronics Programming Software & Apps Streaming Services Tech News & Discussion Virtual & Augmented Reality Pop Culture Celebrities Creators & Influencers Generations & Nostalgia Podcasts Streamers Tarot & Astrology Movies & TV Action Movies & Series Animated Movies & Series Comedy Movies & Series Crime, Mystery, & Thriller Movies & Series Documentary Movies & Series Drama Movies & Series Fantasy Movies & Series Horror Movies & Series Movie News & Discussion Reality TV Romance Movies & Series Sci-Fi Movies & Series Superhero Movies & Series TV News & Discussion RESOURCES About Reddit Advertise Help Blog Careers Press Communities Best of Reddit Topics

Source: https://developer.ibm.com/articles/awb-strategies-enhancing-rag-effectiveness/
IBM Developer

Source: https://www.reddit.com/r/LangChain/comments/1e8oct1/rag_in_production_best_practices_for_robust_and/
Reddit - Dive into anything Skip to main content We value your privacy Reddit and its partners use cookies and similar technologies to provide you with a better experience. By accepting all cookies, you agree to our use of cookies to deliver and maintain our services and site, improve the quality of Reddit, personalize Reddit content and advertising, and measure the effectiveness of advertising. By rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of our platform. For more information, please see our Cookie Notice and our Privacy Policy. Open menu Open navigation Go to Reddit Home r/LangChain A chip A close button Get App Get the Reddit app Log In Log in to Reddit Expand user menu Open settings menu Go to LangChain r/LangChain r/LangChain LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/. Members Online • behitek RAG in Production: Best Practices for Robust and Scalable Systems Tutorial 🚀 Exciting News! 🚀 Just published my latest blog post on the Behitek blog: "RAG in Production: Best Practices for Robust and Scalable Systems" 🌟 In this article, I explore how to effectively implement Retrieval-Augmented Generation (RAG) models in production environments. From reducing hallucinations to maintaining document hierarchy and optimizing chunking strategies, this guide covers all you need to know for robust and efficient RAG deployments. Check it out and share your thoughts or experiences! I'd love to hear your feedback and any additional tips you might have. 👇 🔗 https://behitek.com/blog/2024/07/18/rag-in-production Read more Top 3% Rank by size Public Anyone can view, post, and comment to this community Reddit Rules Privacy Policy User Agreement Reddit, Inc. © 2025. All rights reserved. Expand Navigation Collapse Navigation &nbsp; TOPICS Internet Culture (Viral) Amazing Animals & Pets Cringe & Facepalm Funny Interesting Memes Oddly Satisfying Reddit Meta Wholesome & Heartwarming Games Action Games Adventure Games Esports Gaming Consoles & Gear Gaming News & Discussion Mobile Games Other Games Role-Playing Games Simulation Games Sports & Racing Games Strategy Games Tabletop Games Q&As Q&As Stories & Confessions Technology 3D Printing Artificial Intelligence & Machine Learning Computers & Hardware Consumer Electronics DIY Electronics Programming Software & Apps Streaming Services Tech News & Discussion Virtual & Augmented Reality Pop Culture Celebrities Creators & Influencers Generations & Nostalgia Podcasts Streamers Tarot & Astrology Movies & TV Action Movies & Series Animated Movies & Series Comedy Movies & Series Crime, Mystery, & Thriller Movies & Series Documentary Movies & Series Drama Movies & Series Fantasy Movies & Series Horror Movies & Series Movie News & Discussion Reality TV Romance Movies & Series Sci-Fi Movies & Series Superhero Movies & Series TV News & Discussion RESOURCES About Reddit Advertise Help Blog Careers Press Communities Best of Reddit Topics

Source: https://www.reddit.com/r/LocalLLaMA/comments/16cbimi/yet_another_rag_system_implementation_details_and/
Reddit - Dive into anything Skip to main content We value your privacy Reddit and its partners use cookies and similar technologies to provide you with a better experience. By accepting all cookies, you agree to our use of cookies to deliver and maintain our services and site, improve the quality of Reddit, personalize Reddit content and advertising, and measure the effectiveness of advertising. By rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of our platform. For more information, please see our Cookie Notice and our Privacy Policy. Open menu Open navigation Go to Reddit Home r/LocalLLaMA A chip A close button Get App Get the Reddit app Log In Log in to Reddit Expand user menu Open settings menu Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA Subreddit to discuss about Llama, the large language model created by Meta AI. Members Online • snexus_d Yet another RAG system - implementation details and lessons learned Tutorial | Guide Edit: Fixed formatting. Having a large knowledge base in Obsidian and a sizable collection of technical documents, for the last couple of months, I have been trying to build an RAG-based QnA system that would allow effective querying. After the initial implementation using a standard architecture (structure unaware, format agnostic recursive text splitters and cosine similarity for semantic search), the results were a bit underwhelming. Throwing a more powerful LLM at the problem helped, but not by an order of magnitude (the model was able to reason better about the provided context, but if the context wasn't relevant to begin with, obviously it didn't matter). Here are implementation details and tricks that helped me achieve significantly better quality. I hope it will be helpful to people implementing similar systems. Many of them I learned by reading suggestions from this and other communities, while others were discovered through experimentation. Most of the methods described below are implemented ihere - [GitHub - snexus/llm-search: Querying local documents, powered by LLM](https://github.com/snexus/llm-search/tree/main). ## Pre-processing and chunking Document format - the best quality is achieved with a format where the logical structure of the document can be parsed - titles, headers/subheaders, tables, etc. Examples of such formats include markdown, HTML, or .docx. PDFs, in general, are hard to parse due to multiple ways to represent the internal structure - for example, it can be just a bunch of images stacked together. In most cases, expect to be able to split by sentences. Content splitting: Splitting by logical blocks (e.g., headers/subheaders) improved the quality significantly. It comes at the cost of format-dependent logic that needs to be implemented. Another downside is that it is hard to maintain an equal chunk size with this approach. For documents containing source code, it is best to treat the code as a single logical block. If you need to split the code in the middle, make sure to embed metadata providing a hint that different pieces of code are related. Metadata included in the text chunks: Document name. References to higher-level logical blocks (e.g., pointing to the parent header from a subheader in a markdown document). For text chunks containing source code - indicating the start and end of the code block and optionally the name of the programming language. External metadata - added as external metadata in the vector store. These fields will allow dynamic filtering by chunk size and/or label. Chunk size. Document path. Document collection label, if applicable. Chunk sizes - as many people mentioned, there appears to be high sensitivity to the chunk size. There is no universal chunk size that will achieve the best result, as it depends on the type of content, how generic/precise the question asked is, etc. One of the solutions is embedding the documents using multiple chunk sizes and storing them in the same collection. During runtime, querying against these chunk sizes and selecting dynamically the size that achieves the best score according to some metric. Downside - increases the storage and processing time requirements. ## Embeddings There are multiple embedding models achieving the same or better quality as OpenAI's ADA - for example, `e5-large-v2` - it provides a good balance between size and quality. Some embedding models require certain prefixes to be added to the text chunks AND the query - that's the way they were trained and presumably achieve better results compared to not appending these prefixes. ## Retrieval One of the main components that allowed me to improve retrieval is a **re-ranker**. A re-ranker allows scoring the text passages obtained from a similarity (or hybrid) search against the query and obtaining a numerical score indicating how relevant the text passage is to the query. Architecturally, it is different (and much slower) than a similarity search but is supposed to be more accurate. The results can then be sorted by the numerical score from the re-ranker before stuffing into LLM. A re-ranker can be costly (time-consuming and/or require API calls) to implement using LLMs but is efficient using cross-encoders. It is still slower, though, than cosine similarity search and can't replace it. Sparse embeddings - I took the general idea from [Getting Started with Hybrid Search | Pinecone](https://www.pinecone.io/learn/hybrid-search-intro/) and implemented sparse embeddings using SPLADE. This particular method has an advantage that it can minimize the "vocabulary mismatch problem." Despite having large dimensionality (32k for SPLADE), sparse embeddings can be stored and loaded efficiently from disk using Numpy's sparse matrices. With sparse embeddings implemented, the next logical step is to use a **hybrid search** - a combination of sparse and dense embeddings to improve the quality of the search. Instead of following the method suggested in the blog (which is a weighted combination of sparse and dense embeddings), I followed a slightly different approach: Retrieve the **top k** documents using SPLADE (sparse embeddings). Retrieve **top k** documents using similarity search (dense embeddings). Create a union of documents from sparse or dense embeddings. Usually, there is some overlap between them, so the number of documents is almost always smaller than 2*k. Re-rank all the documents (sparse + dense) using the re-ranker mentioned above. Stuff the top documents sorted by the re-ranker score into the LLM as the most relevant documents. The justification behind this approach is that it is hard to compare the scores from sparse and dense embeddings directly (as suggested in the blog - they rely on magical weighting constants) - but the re-ranker should explicitly be able to identify which document is more relevant to the query. Let me know if the approach above makes sense or if you have suggestions for improvement. I would be curious to know what other tricks people used to improve the quality of their RAG systems. Read more Top 1% Rank by size Public Anyone can view, post, and comment to this community Top Posts Reddit reReddit: Top posts of September 7, 2023 Reddit reReddit: Top posts of September 2023 Reddit reReddit: Top posts of 2023 Reddit Rules Privacy Policy User Agreement Reddit, Inc. © 2025. All rights reserved. Expand Navigation Collapse Navigation &nbsp; TOPICS Internet Culture (Viral) Amazing Animals & Pets Cringe & Facepalm Funny Interesting Memes Oddly Satisfying Reddit Meta Wholesome & Heartwarming Games Action Games Adventure Games Esports Gaming Consoles & Gear Gaming News & Discussion Mobile Games Other Games Role-Playing Games Simulation Games Sports & Racing Games Strategy Games Tabletop Games Q&As Q&As Stories & Confessions Technology 3D Printing Artificial Intelligence & Machine Learning Computers & Hardware Consumer Electronics DIY Electronics Programming Software & Apps Streaming Services Tech News & Discussion Virtual & Augmented Reality Pop Culture Celebrities Creators & Influencers Generations & Nostalgia Podcasts Streamers Tarot & Astrology Movies & TV Action Movies & Series Animated Movies & Series Comedy Movies & Series Crime, Mystery, & Thriller Movies & Series Documentary Movies & Series Drama Movies & Series Fantasy Movies & Series Horror Movies & Series Movie News & Discussion Reality TV Romance Movies & Series Sci-Fi Movies & Series Superhero Movies & Series TV News & Discussion RESOURCES About Reddit Advertise Help Blog Careers Press Communities Best of Reddit Topics cera_coop • Promoted Word Cera-vennoot Learn More cera.coop onirisapp • Promoted A new open source testing tool allows testing the efficacy of WAF solutions in real-world conditions Learn More openappsec.io

Source: https://www.reddit.com/r/softwaredevelopment/comments/1e4lxgq/applying_retrieval_augmented_generation_rag_to/
Reddit - Dive into anything Skip to main content We value your privacy Reddit and its partners use cookies and similar technologies to provide you with a better experience. By accepting all cookies, you agree to our use of cookies to deliver and maintain our services and site, improve the quality of Reddit, personalize Reddit content and advertising, and measure the effectiveness of advertising. By rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of our platform. For more information, please see our Cookie Notice and our Privacy Policy. Open menu Open navigation Go to Reddit Home r/softwaredevelopment A chip A close button Get App Get the Reddit app Log In Log in to Reddit Expand user menu Open settings menu Go to softwaredevelopment r/softwaredevelopment r/softwaredevelopment Software development methodologies, techniques, and tools. Not to be confused with programming. Covering Agile, RUP, Waterfall, Crystal, Extreme Programming, Scrum, Lean, Kanban and more. Members Online • thumbsdrivesmecrazy Applying Retrieval Augmented Generation (RAG) to Large-Scale Code Repos - Guide The article discusses various strategies and techniques for implementing RAG to large-scale code repositories, as well as potential benefits and limitations of the approach as well as show how RAG can improve developer productivity and code quality in large software projects: RAG with 10K Code Repos Read more Top 2% Rank by size Public Anyone can view, post, and comment to this community &nbsp; TOPICS Internet Culture (Viral) Amazing Animals & Pets Cringe & Facepalm Funny Interesting Memes Oddly Satisfying Reddit Meta Wholesome & Heartwarming Games Action Games Adventure Games Esports Gaming Consoles & Gear Gaming News & Discussion Mobile Games Other Games Role-Playing Games Simulation Games Sports & Racing Games Strategy Games Tabletop Games Q&As Q&As Stories & Confessions Technology 3D Printing Artificial Intelligence & Machine Learning Computers & Hardware Consumer Electronics DIY Electronics Programming Software & Apps Streaming Services Tech News & Discussion Virtual & Augmented Reality Pop Culture Celebrities Creators & Influencers Generations & Nostalgia Podcasts Streamers Tarot & Astrology Movies & TV Action Movies & Series Animated Movies & Series Comedy Movies & Series Crime, Mystery, & Thriller Movies & Series Documentary Movies & Series Drama Movies & Series Fantasy Movies & Series Horror Movies & Series Movie News & Discussion Reality TV Romance Movies & Series Sci-Fi Movies & Series Superhero Movies & Series TV News & Discussion RESOURCES About Reddit Advertise Help Blog Careers Press Communities Best of Reddit Topics Reddit Rules Privacy Policy User Agreement Reddit, Inc. © 2025. All rights reserved.

Source: https://zilliz.com/blog/best-practice-in-implementing-rag-apps
Best Practices for Building RAG Apps - Zilliz blog Products Zilliz Cloud Fully-managed vector database service designed for speed, scale and high performance.Zilliz Cloud vs. Milvus Milvus Open-source vector database built for billion-scale vector similarity search. BYOC Benchmark Open Source Integrations Support Portal High-Performance Vector Database Made Serverless. Pricing Pricing PlanFlexible pricing options for every team on any budget CalculatorEstimate your cost Free Tier Developers Documentation The Zilliz Cloud Developer Hub where you can find all the information to work with Zilliz Cloud Learn More Learn GenAI Resource Hub Notebooks AI Models Community Download Milvus Join the Milvus Discord Community Resources Blog Guides Research Analyst Reports Webinars Trainings Events Definitive Guide to Choosing a Vector Database Solutions By Use CaseRetrieval Augmented GenerationView all use cases View by industry View all customer stories Ivy.ai Scales GenAI-Powered Communication with Zilliz Cloud English 日本語 Contact usLog inGet Started Free English 日本語 Blog Best Practices in Implementing Retrieval-Augmented Generation (RAG) Applications Best Practices in Implementing Retrieval-Augmented Generation (RAG) Applications Oct 21, 202415 min read By Ruben Winastwan Retrieval-Augmented Generation (RAG) is a method that has proven very effective in improving LLMs' responses and addressing LLM hallucinations. In a nutshell, RAG provides LLMs with context that can help them generate more accurate and contextualized responses. The contexts can come from anywhere: from your internal documents, vector databases, CSV files, JSON files, etc. RAG is a novel approach that consists of many components that work together. These components include query processing, context chunking, context retrieval, context reranking, and the LLM itself to generate the response. Each component influences the quality of the final response generated from a RAG application. The problem is that it's difficult to find the best combination of methods in each component that leads to the most optimal RAG performance. In this article, we will discuss several techniques commonly used in all RAG components, evaluate the best approach for each component, and then find the best combination that leads to the most optimal RAG-generated response, according to this paper. So, without further ado, let's start with an introduction to RAG components. RAG Components As mentioned, RAG is a powerful method to alleviate LLMs' hallucination problems, which commonly occur when we ask queries beyond their training data or when they require specialized knowledge. For example, if we ask an LLM a question about our internal data, we will likely get an inaccurate answer. RAG solves this problem by providing our LLM with context that can help answer our query. RAG consists of a chain of components that form a workflow. The typical RAG components include: Query classification: to determine whether our query needs retrieval of contexts or can be processed directly by the LLM. Context retrieval: to fetch the top k candidates in the most relevant contexts to our query. Context reranking: to sort the top k candidates fetched from the retrieval component, starting with the most similar one. Context repacking: to organize the most relevant contexts into a more structured format for better response generation. Context summarization: to extract key information from relevant contexts for improved response generation. Response generation: to generate a response based on the query and relevant contexts. Figure- RAG Components..png Figure: RAG Components. Source While these RAG components are useful during the response generation process (i.e., when we've already stored all contexts and they're ready to be fetched), several other factors must be considered before implementing a RAG method. We need to transform our context documents into vector embeddings to make our context documents useful in a RAG approach. Therefore, choosing the most appropriate embedding model and strategy to represent our input documents as embeddings is crucial. An embedding contains a semantically rich representation of our input document. However, if the document used as context is too long, it can confuse the LLM when generating an appropriate response. A common approach to solving this problem is to apply a chunking method, where we split our input document into several chunks and then transform each chunk into an embedding. It's crucial to pick the best chunking method and size, as chunks that are too short will likely contain insufficient information. Figure- RAG workflow.png Figure: RAG workflow We must consider appropriate storage for these embeddings once we transform each chunk into an embedding. If you're not dealing with many embeddings, you can store them directly in your device's local memory. However, you'll commonly deal with hundreds or even millions of embeddings in practice. In this case, you need a vector database like Milvus or its managed service, Zilliz Cloud, to store them, and choosing the right vector database is crucial for the success of our RAG application. The final consideration is the LLM itself. If applicable, we can fine-tune the LLM to target our specific needs more precisely. However, fine-tuning is costly and unnecessary in most cases, especially if we're using a performant LLM with many parameters. In the following sections, we'll discuss the best approaches for each RAG component. Next, we'll explore combinations of these best approaches and suggest several strategies for deploying RAG that balance performance and efficiency. Query Classification As mentioned in the previous section, RAG is useful in ensuring that the LLM generates accurate and contextualized responses, especially when specialized knowledge from our internal data is required. However, RAG also increases the runtime of the response generation process. The thing is, not all queries require the retrieval process, and many of them can be processed directly by the LLM. Therefore, skipping the context retrieval process would be more beneficial if a query doesn't need it. We can implement a query classification model to determine whether a query needs context retrieval before the response generation process. Such a classification model usually consists of a supervised model, such as BERT, with the main goal of predicting whether a query needs retrieval or not. However, like other supervised models, we must train it before using it for inference. To train the model, we need to generate a dataset of example prompts and their corresponding binary labels, including whether the prompt needs retrieval or not. Figure- Query classification dataset example..png Figure: Query classification dataset example. Source In the paper, a BERT-base-multilingual model is used for query classification. The training data includes 15 types of prompts in total, such as translation, summarization, rewriting, in-context learning, etc. There are two distinct labels: "sufficient" if the prompt is entirely based on user-given information and doesn't need retrieval, and "insufficient" if the prompt information is incomplete, needs specialized information, and requires a retrieval process. Using this approach, the model achieved 95% in both accuracy and F1 score. This query classification step can significantly improve the efficiency of the RAG process by avoiding unnecessary retrievals for queries that can be handled directly by the LLM. It acts as a filter, ensuring that only queries requiring additional context are sent through the more time-consuming retrieval process. Figure- Query classifier result..png Figure: Query classifier result. Source Chunking Technique Chunking refers to the process of splitting long input documents into smaller segments. This process is highly useful in providing the LLM with a more granular context. There are several methods for chunking, including token-level and sentence-level approaches. Sentence-level chunking often leads to a good balance between simplicity and semantic preservation of the context. When choosing a chunking method, we need to be careful with the chunk size, as chunks that are too short may not provide helpful context for the LLM. Figure- Splitting a long document into smaller chunks.png Figure: Splitting a long document into smaller chunks To find the optimal chunk size, an evaluation was conducted on the Lyft 2021 document. The first 60 pages of the document were chosen as a corpus and chunked into several sizes. An LLM was then used to generate 170 queries based on these 60 pages. The text-embedding-ada-002 model was used for embeddings, while the Zephyr 7B model was used as the LLM to generate responses based on the chosen queries. To evaluate the model's performance on different chunk sizes, GPT-3.5 Turbo was used. Two metrics were employed to evaluate the response quality: faithfulness and relevancy. Faithfulness measures whether the response is hallucinated or matches the retrieved contexts, while relevancy measures whether the retrieved contexts and responses match the queries. Figure- Comparison of different chunk sizes. .png Figure: Comparison of different chunk sizes. Source The results show that a maximum chunk size of 512 tokens is preferred for highly relevant response generation from the LLM. Shorter chunk sizes, such as 256 tokens, also perform well and can improve the overall runtime of the RAG application. Advanced chunking techniques such as small2big and sliding windows can be used to combine the benefits of different chunk sizes. Small2big is a chunking approach that organizes chunk block relationships. Small-sized chunks are used to match queries, and larger chunks containing the information from the smaller ones are used as the final context for the LLM. A sliding window is a chunking method that provides token overlaps between chunks to preserve context information. Figure- Comparison of different chunking techniques..png Figure: Comparison of different chunking techniques. Source Experiments show that with a smaller chunk size of 175 tokens, a larger chunk size of 512 tokens, and a chunk overlap of 20 tokens, both chunking techniques improve the faithfulness and relevancy scores of the LLM responses. Next, finding the best embedding model to represent each chunk as a vector embedding is crucial. A test on namespace-Pt/msmarco was conducted for this purpose. The results show that both LLM Embedder and bge-large-en models perform best. However, since LLM Embedder is three times smaller than bge-large-en, it was chosen as the default embedding for the experiment. Figure- Results for different embedding models on namespace-Pt:msmarco. .png Figure: Results for different embedding models on namespace-Ptmsmarco. Source Vector Databases Vector databases play a crucial role in RAG applications, particularly in storing and retrieving relevant contexts. In common real-world RAG applications, we deal with a huge amount of documents, leading to a vast number of context embeddings that need to be stored. In such cases, storing these embeddings in local memory is insufficient, and computing the retrieval of relevant contexts among large collections of embeddings would take considerable time. Vector databases are designed to solve these problems. With a vector database, we can store millions to even billions of vector embeddings and perform context retrieval in a split second. When choosing the best vector database for your use case, we need to consider several factors, such as index type support, billion-scale vector support, hybrid search support, and cloud-native capabilities. Among these criteria, Milvus stands out as the best open-source vector database compared to its competitors like Weaviate, Chroma, Faiss, Qdrant, etc. Comparison of Various Vector Databases..png Comparison of Various Vector Databases. Source. In terms of index type support, Milvus offers several indexing methods to suit various needs, such as the naive flat index (FLAT) or other indexing types designed to speed up the retrieval process, like the inverted file index (IVF-FLAT) and Hierarchical Navigable Small World (HNSW). To compress the memory needed to store the contexts, you can also implement product quantization (PQ) during the embeddings' indexing process. Milvus also supports a hybrid search approach. This approach enables us to combine two different methods during the context retrieval process. For example, we can combine dense embedding with sparse embedding to retrieve relevant contexts, enhancing the relevancy of the retrieved context with respect to the query. This, in turn, also enhances the response generated by the LLM. Additionally, we can combine dense embedding with metadata filtering if desired. If you want to use Milvus on the cloud, whether on GCP or AWS, to store billions of embeddings, you can opt for its managed service: Zilliz Cloud. With Zilliz Cloud, you can create cluster units (CUs) optimized for both capacity and performance to store large-scale embeddings. For example, you can create 256 performance-optimized CUs serving 1.3 billion 128-dimensional vectors or 128 capacity-optimized CUs serving 3 billion 128-dimensional vectors. Diagram of logical cluster and auto-scaling implemented in Zilliz Cloud Serverless..png Diagram of logical cluster and auto-scaling implemented in Zilliz Cloud Serverless. If you'd like to build a RAG application with Milvus but also want to save on operating costs, you can opt for Zilliz Cloud Serverless. This service provides an auto-scaling feature within Milvus, with costs that increase only as your business grows. The serverless option is also perfect for cost savings because you only pay when you use the service, not when it's idle. Zilliz Cloud has launched multiple exciting updates recently, including a new migration service, multiple replicas, new integration with Fivetran connectors, auto-scaling capability, and many more production readiness features. See more details below: Zilliz Cloud Update: Migration Services, Fivetran Connectors, Multi-replicas, and More Enhanced Monitoring and Observability in Zilliz Cloud Unlock AI-powered search with Fivetran and Milvus Top 5 reasons to migrate from Open Source Milvus to Zilliz Cloud Retrieval Techniques The main goal of the retrieval component is to fetch the top k most relevant contexts for a given query. However, a significant challenge in this component that might affect the overall quality of our RAG comes from the query itself. Original queries are often poorly written or expressed, lacking the semantic information needed for RAG applications to fetch relevant contexts. Several commonly applied techniques to solve this problem include: Query rewriting: Prompts the LLM to rewrite the original query to improve its clarity and semantic information. Query decomposition: Decomposes the original query into subqueries and performs retrieval based on these sub-queries. Pseudo-document generation: Generates hypothetical or synthetic documents based on the original query and then uses these hypothetical documents to retrieve similar documents in the database. The most well-known implementation of this approach is HyDE (Hypothetical Document Embeddings). Experiments show that combining HyDE and hybrid search yields the best results on TREC DL19/20 compared to query rewriting and query decomposition. The hybrid search mentioned in the experiment combines LLM Embedder to obtain dense embeddings and BM25 to obtain sparse embeddings. The workflow of HyDe + hybrid search is as follows: first, we generate a hypothetical document that answers the query with HyDE. Next, this hypothetical document is concatenated with the original query before being transformed into dense and sparse embeddings using LLM Embedder and BM25, respectively. Results for different retrieval methods. .png Results for different retrieval methods. Source Although the combination of HyDE and hybrid search yields the best results, it also comes with higher computational costs. Based on further tests on several NLP datasets, both hybrid search and using only dense embeddings result in comparable performance to HyDE + hybrid search, but with almost 10x lower latency. Therefore using a hybrid search would be more recommended. Since we're using a hybrid search, the retrieved contexts are based on vector search from dense and sparse embeddings. Therefore, it's also interesting to examine the impact of the weighting value between dense and sparse embeddings on the overall relevance score according to this equation: formula.png Figure- Results of hybrid search with different alpha values..png Figure: Results of hybrid search with different alpha values. Source. The experiment shows that a weighting value of 0.3 yields the best overall relevance score on TREC DL19/20. Reranking and Repacking Techniques The main goal of reranking techniques is to reorder the top k most relevant contexts fetched from the retrieval method to ensure that the most similar context is returned at the top of the list. There are two common approaches to rerank the contexts: DLM Reranking: This method uses a deep learning model for reranking. The model is trained with a pair consisting of the original query and a context as input and a binary label "true" (if the pair is relevant to each other) or "false" as output. The contexts are then sorted based on the probability the model returns when it predicts a pair of queries and context as "true." TILDE Reranking: This approach uses the likelihood of each term in the original query to rerank. During inference time, we can use either the query likelihood component (TILDE-QL) alone for faster reranking or the combination of TILDE-QL with its document likelihood component (TILDE-DL) to improve the reranking result at a higher computational cost. Figure- Results of different reranking methods..png Figure: Results of different reranking methods. Source Experiments on the MS MARCO Passage ranking dataset show that the DLM reranking method with the Llama 27B model yields the best reranking performance. However, since it's a large model, using it comes with a significant computational cost. Therefore, using mono T5 is more recommended for DLM reranking, as it provides a balance between performance and computational efficiency. After the reranking phase, we also need to consider how to present the reranked contexts to our LLM: whether in descending ("forward") or ascending ("reverse") order. Based on the experiments conducted in this paper, it can be concluded that the best response quality is generated using the "reverse" configuration. The hypothesis is that positioning more relevant context closer to the query leads to optimal outcomes. Summarization Techniques In cases where we have long contexts retrieved from previous components, we might want to make them more compact and remove redundant information. To achieve this goal, summarization approaches are typically implemented. There are two different context summarization techniques: extractive and abstractive. Extractive summarization splits the input document into smaller segments, then ranked based on importance. Meanwhile, the abstractive method generates a new context summary that contains only relevant information. Figure- Comparison between different summarization methods..png Figure: Comparison between different summarization methods. Source Based on experiments on three different datasets (NQ, TriviaQA, and HotpotQA), abstractive summarization with Recomp yields the best performance compared to other abstractive and extractive methods. The Summary of Best RAG Techniques Now that we know the best approach for each RAG component for specific benchmark datasets, we can further test all of the approaches mentioned in the previous sections on more datasets. The results show that each component contributes to the overall performance of our RAG application. Below is a summary of the results for each approach in each component based on five different datasets: Figure- Results of the search for optimal RAG practices..png Figure: Results of the search for optimal RAG practices. Source The query classification component proves to help improve the accuracy of responses and reduce the overall runtime latency. This initial step helps determine whether a query requires context retrieval or can be processed directly by the LLM, thus optimizing the system's efficiency. The retrieval component is crucial in ensuring that we obtain relevant context candidates with respect to the query. For this component, a more scalable and performant vector database like Milvus or its managed service, Zilliz Cloud, is recommended. In addition, hybrid search or dense embedding search is recommended. These methods strike a balance between comprehensive context matching and computational efficiency. The reranking component ensures we get the most relevant contexts by reordering the top k contexts retrieved from the retrieval component. The monoT5 model is recommended for reranking due to its balance of performance and computational cost. This step refines the selection of contexts, prioritizing the most relevant contexts to the query. The reverse method is recommended to repack the context. This approach positions the most relevant context closest to the query, potentially leading to more accurate and coherent responses from the LLM. Finally, the abstractive method with Recomp has shown the best performance for context summarization. This technique helps condense long contexts while preserving key information, making it easier for the LLM to process and generate relevant responses. LLM Fine-Tuning In most cases, LLM fine-tuning is not necessary, especially if you're using a performant LLM with many parameters. However, if you have hardware constraints and can only use smaller LLMs, you may need to fine-tune them to make them more robust when generating responses related to your use case. Before fine-tuning an LLM, you need to consider the data you will use as training data. During the data preparation, you can collect training data in prompt and context as a pair of inputs, with an example of generated text as output. Experiments show that augmenting your data with a mix of relevant and randomly selected contexts during training will result in the best performance. The intuition behind this is that mixing relevant and random contexts during fine-tuning can improve our LLM's robustness. Conclusion In this article, we explored various RAG components, from query classification to context summarization. We have discussed and highlighted the approaches with optimal performance in each component. These optimized components work together to improve the overall performance of the RAG system. They’re improving the quality and relevance of generated responses while maintaining computational efficiency. By implementing these best practices in each component, we can create a more robust and effective RAG system capable of handling a wide range of queries and tasks. Further Reading Generative AI Resource Hub | Zilliz Top Performing AI Models for Your GenAI Apps | Zilliz Build AI Apps with Milvus: Tutorials & Notebooks How to Build a Multilingual RAG with Milvus, LangChain, and OpenAI Build a Multimodal RAG with Gemini, BGE-M3, Milvus and LangChain What is GraphRAG? Enhancing RAG with Knowledge Graphs How to Evaluate RAG Applications Ruben Winastwan Content RAG Components Query Classification Chunking Technique Vector Databases Retrieval Techniques Reranking and Repacking Techniques Summarization Techniques The Summary of Best RAG Techniques LLM Fine-Tuning Conclusion Further Reading Start Free, Scale Easily Try the fully-managed vector database built for your GenAI applications. Try Zilliz Cloud for Free Share this article Copied Keep Reading Insights into LLM Security from the World’s Largest Red Team We will discuss how the Gandalf project revealed LLMs' vulnerabilities to adversarial attacks. Additionally, we will address the role of vector databases in AI security. Read Now How Inkeep and Milvus Built a RAG-driven AI Assistant for Smarter Interaction Robert Tran, the Co-founder and CTO of Inkeep, shared how Inkeep and Zilliz built an AI-powered assistant for their documentation site. Read Now Deploying a Multimodal RAG System Using vLLM and Milvus This blog will guide you through creating a Multimodal RAG with Milvus and vLLM. Read Now Sign up for the Zilliz newsletter Subscribe 201 Redwood Shores Pkwy, Suite 330 Redwood City, California 94065 Product Zilliz Cloud Zilliz Cloud BYOC Free Tier Milvus GPTCache Attu Milvus CLI Integrations Resources Customers Docs Blog GitHub Benchmark Comparison Resources Glossary What is a Vector Database? What is Retrieval Augmented Generation GenAI Resource Hub Free RAG Cost Estimator Data Connectors Company About Careers News Partners Events Contact Sales Terms of ServicePrivacy PolicySecuritySystem Status Cookie Settings LF AI, LF AI & data, Milvus, and associated open-source project names are trademarks of the the Linux Foundation. © Zilliz 2025 All rights reserved.

Source: https://www.linkedin.com/pulse/best-practices-implementing-retrieval-augmented-rag-bhanu-chaddha-nn5yc
Best Practices for Implementing Retrieval-Augmented Generation (RAG) in Production LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including professional and job ads) on and off LinkedIn. Learn more in our Cookie Policy. Select Accept to consent or Reject to decline non-essential cookies for this use. You can update your choices at any time in your settings. Accept Reject Agree & Join LinkedIn By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. Sign in to view more content Create your free account or sign in to continue your search Sign in Welcome back Email or phone Password Show Forgot password? Sign in or By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. New to LinkedIn? Join now or New to LinkedIn? Join now By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. LinkedIn LinkedIn is better on the app Don’t have the app? Get it in the Microsoft Store. Open the app Skip to main content LinkedIn Articles People Learning Jobs Games Get the app Join now Sign in Best Practices for Implementing Retrieval-Augmented Generation (RAG) in Production Report this article Bhanu Chaddha Bhanu Chaddha Senior AI Consultant | GenAI Specialist | AI Educator & Speaker | Transitioning from Lead Engineer to Generative AI Innovator Published Oct 24, 2024 + Follow As AI continues to revolutionize customer experience, Retrieval-Augmented Generation (RAG) is proving to be a game-changer for applications needing real-time, context-specific responses. Today, I want to share some key best practices to help you implement RAG efficiently in production environments. Introduction: The Production Scenario Imagine you manage a knowledge-intensive customer support center that handles thousands of customer requests every day. Users expect accurate, personalized, and up-to-date answers—something that can be challenging when dealing with static knowledge bases. RAG combines the strengths of both retrieval and generation, allowing customer support systems to provide the latest information directly from company documents, databases, and web pages. But with great power comes the need for careful implementation to ensure performance and reliability. Real-World Scenario Consider a scenario in an e-commerce company where customers frequently inquire about product availability, returns, and shipping policies. Static responses often fail because the answers depend on frequently changing information—like updated inventory, new return policies, or ongoing promotions. This variability can result in outdated responses, leaving customers dissatisfied. RAG enables the retrieval of relevant data at runtime and combines it with an LLM (Large Language Model) to create dynamic, precise responses. Here are some best practices to implement RAG effectively for such scenarios. Best Practices for Implementing RAG 1. Efficient Indexing and Data Management - Use a fast, efficient document indexing mechanism like LamaIndex or Elasticsearch. Make sure the data being retrieved is relevant, up-to-date, and structured to allow fast access. - Index documents by common customer questions or categories to improve the retriever’s accuracy in finding relevant information. 2. Model Selection and Fine-Tuning - Use a language model that suits your business requirements in terms of speed and accuracy. Models like GPT-Neo can be fine-tuned for specific customer service use cases to provide more context-aware responses. - Fine-tune LLMs based on domain-specific data, such as FAQs, product documentation, and previous customer interactions, to enhance their response quality. 3. Caching Frequent Queries - Identify common customer queries and cache the responses. This will reduce retrieval and generation time, leading to a significant performance boost. - Implement a layered cache, where frequently asked questions are served instantly without needing the retriever or LLM to generate a response. Rag with Caching 4. Latency Management - Combining retrieval and generation can lead to latency issues. To counter this, consider using lightweight LLMs like DistilGPT or smaller versions of GPT for rapid response. - Implement an asynchronous request processing system to ensure that users do not experience delays, especially during peak hours. 5. Fallback Mechanisms - Always have a fallback mechanism in place if retrieval or generation fails. This could include static responses or a mechanism to connect the user to a human agent. - Log failures to continuously monitor the health of your RAG setup and iterate on improvements. Example Code: Caching Here’s how you can implement a RAG system for customer support with some of these best practices: Recommended by LinkedIn Intelligent Document Processing: Enhancing Efficiency… Naveen Joshi 1 year ago A Practical Approach to Building & Evaluating Advanced… Pavan Belagatti 3 months ago Business Transformation: AI Innovation SymphonyAI 7 months ago Sample Output Key Takeaways - Reduced Latency: Implement caching for frequent queries to minimize latency. - Domain-Specific Responses: Fine-tuning LLMs on specific data makes them more relevant for the task, leading to higher customer satisfaction. - Reliability: A well-planned fallback mechanism ensures uninterrupted customer service, even if parts of the system fail. Common Pitfalls & Best Practices Common Pitfalls & Best Practices 1. Index Management: Failing to keep indexed documents updated can lead to inaccurate responses. Regularly refresh your index to reflect current policies and information. 2. Latency Trade-offs: Using large LLMs can lead to high latency. Strike a balance between model complexity and response time to ensure a good user experience. 3. Caching Stale Data: Cached responses can become outdated quickly. Use a short TTL (Time-To-Live) for cache items related to dynamic content. Conclusion RAG provides a powerful approach to delivering accurate, real-time customer support. By implementing these best practices, you can ensure that your system is scalable, responsive, and reliable. Have you faced challenges with latency or data retrieval in your AI projects? How do you think RAG could address these issues? Let’s share experiences and learn from each other in the comments! #RAG #AI #LangChain #CustomerSupport #OpenAI #MachineLearning #RealTimeAI #AIProduction #LLM #Caching #AIIntegration #AIinCustomerService #AIChatbots #LLMDeployment #IntelligentRetrieval #AIAutomation #CustomerExperience #AIOptimizations #TechInnovation #DeepLearning #ConversationalAI --- If you're interested in similar topics or want to break into the amazing world of AI, follow me below! I regularly share valuable insights and practical guides to help you navigate the AI landscape. LinkedIn: https://www.linkedin.com/in/bhanu-chaddha/ Twitter/X: https://x.com/bhanuchddha Let’s connect and learn together! 🚀 Like Like Celebrate Support Love Insightful Funny Comment Copy LinkedIn Facebook Twitter Share 2 To view or add a comment, sign in More articles by Bhanu Chaddha Why DeepSeek R1 Is a Game-Changer—And Why Nvidia and Meta Stocks Are Falling Jan 29, 2025 Why DeepSeek R1 Is a Game-Changer—And Why Nvidia and Meta Stocks Are Falling If you thought OpenAI and Meta were untouchable, think again. DeepSeek R1 is here to disrupt the AI landscape—and it’s… 6 Understanding Retrieval-Augmented Generation (RAG) Oct 11, 2024 Understanding Retrieval-Augmented Generation (RAG) In recent years, Retrieval-Augmented Generation (RAG) has emerged as a groundbreaking approach in the AI domain… 9 2 Comments Generative AI Tools: Discover the Best Solutions for Creativity and Productivity in Every Industry Sep 30, 2024 Generative AI Tools: Discover the Best Solutions for Creativity and Productivity in Every Industry In today’s fast-paced business world, Generative AI is revolutionizing the way we approach creativity and productivity.… 3 AI vs Traditional Application Development: What’s the Difference? Sep 26, 2024 AI vs Traditional Application Development: What’s the Difference? The landscape of software development is evolving rapidly, thanks to the rise of artificial intelligence (AI)… 6 Kubernetes Operator vs Helm Chart Oct 6, 2023 Kubernetes Operator vs Helm Chart Introduction Navigating the Kubernetes ecosystem can be complex, with various tools available for deploying and… 6 Kubernetes Operator: A Practical Introduction Oct 3, 2023 Kubernetes Operator: A Practical Introduction Introduction Kubernetes Operators are essential tools for managing complex applications efficiently within the… 7 1 Comment Setting Resource Limits for Pods in Kubernetes Oct 1, 2023 Setting Resource Limits for Pods in Kubernetes In the dynamic environment of Kubernetes (K8s), setting proper resource limits for Pods is pivotal. This practice not… 10 How can you debug a crashing Pod in Kubernetes? Sep 29, 2023 How can you debug a crashing Pod in Kubernetes? Debugging a crashing Pod in Kubernetes can involve several steps as you work to identify the underlying issue. The… 8 2 Comments Ultimate Docker Commands - Cheat Sheet Dec 15, 2022 Ultimate Docker Commands - Cheat Sheet Docker is an important tool in the world of software development and deployment. It allows developers to package their… 10 Use Git command-line like a Pro - Ultimate Git Cheat Sheet Dec 7, 2022 Use Git command-line like a Pro - Ultimate Git Cheat Sheet Version control is the technique of tracking and managing changes to software code. It is also known as source control. 4 Show more See all articles Insights from the community Algorithms Here's how you can utilize feedback to enhance algorithm performance. Problem Solving What are the pros and cons of using backtracking for CSPs? Information Architecture You're navigating the realm of AI integration tools. How can you adapt your IA strategies for success? Accounting How can you use financial technology to analyze your competitors' cost structures? Algorithms You're facing real-time scenarios with adapted algorithms. How do you ensure accuracy and efficiency? Information Technology How do you integrate artificial intelligence with other information technology systems and applications? Artificial Intelligence What are the key practices for testing AI code? Algorithms What is the best way to test and debug algorithms for machine learning and artificial intelligence? Lease Negotiations How do you keep up with the latest trends and developments in lease abstraction technology and innovation? Technical Analysis You're struggling with technical analysis accuracy. Can AI tools help you make better predictions? Show more Show less Others also viewed Goodbye Manual Data Entry: Automating Document Processing with AI Emmanuel Ramos 9mo Deploying GenAI Applications Made Easy! Pallavi Singh 2w 5 Ways AI Engineers Are Transforming Small-Medium Businesses Overture Partners 2mo Possible profit pools in Gen AI Stack Pramod Gosavi 1y AI/RAG Tutorial: Building Enterprise-Grade, Secure, Scalable Data APIs Vincent Granville 3mo Adopting Metadata-Rich Markup Standards for Integration APIs in Specific Domains Leonardo Ruoso 11mo AI-driven Business Transformation: Strategies, Challenges, and Future Trends | Data Analytics and Business Intelligence InbuiltData 1y Integrating AI & ML with .NET Applications: A Complete Guide AddWeb Solution Private Limited 8mo Spotlight on Mosaic Agent Evaluation: the new synthetic data generation API Maria Pere-Perez 1mo Building Effective Multi-Agent Systems - The Role of Tools Ravi Chandu Ummadisetti 2w Show more Show less Explore topics Sales Marketing IT Services Business Administration HR Management Engineering Soft Skills See All LinkedIn © 2025 About Accessibility User Agreement Privacy Policy Cookie Policy Copyright Policy Brand Policy Guest Controls Community Guidelines العربية (Arabic) বাংলা (Bangla) Čeština (Czech) Dansk (Danish) Deutsch (German) Ελληνικά (Greek) English (English) Español (Spanish) فارسی (Persian) Suomi (Finnish) Français (French) हिंदी (Hindi) Magyar (Hungarian) Bahasa Indonesia (Indonesian) Italiano (Italian) עברית (Hebrew) 日本語 (Japanese) 한국어 (Korean) मराठी (Marathi) Bahasa Malaysia (Malay) Nederlands (Dutch) Norsk (Norwegian) ਪੰਜਾਬੀ (Punjabi) Polski (Polish) Português (Portuguese) Română (Romanian) Русский (Russian) Svenska (Swedish) తెలుగు (Telugu) ภาษาไทย (Thai) Tagalog (Tagalog) Türkçe (Turkish) Українська (Ukrainian) Tiếng Việt (Vietnamese) 简体中文 (Chinese (Simplified)) 正體中文 (Chinese (Traditional)) Language

Results for query: Step-by-step guide to building a RAG model for large code repositories

Search Engine: Yahoo
Source: https://github.com/HoDangCao/LLMs-System
GitHub - HoDangCao/LLMs-System: Go through all main components of a LLM system from scratch: RAG, CoT, Llama 3 Skip to content Navigation Menu Toggle navigation Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} HoDangCao / LLMs-System Public Notifications You must be signed in to change notification settings Fork 0 Star 0 Go through all main components of a LLM system from scratch: RAG, CoT, Llama 3 0 stars 0 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 0 Pull requests 0 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights HoDangCao/LLMs-System main Branches Tags Go to file Code Folders and files Name Name Last commit message Last commit date Latest commit History 11 Commits CoT CoT Llama Llama RAG RAG LLM_finetuning.ipynb LLM_finetuning.ipynb LLMs_sys_summary.ipynb LLMs_sys_summary.ipynb README.md README.md requirements.txt requirements.txt View all files Repository files navigation README Large Language Models (LLMs) - A Comprehensive Guide Welcome to the LLMs Project! This repository is a step-by-step guide to understanding, building, and experimenting with Large Language Models (LLMs), covering everything from basic components to advanced topics like Retrieval-Augmented Generation (RAG) and Chain of Thought (CoT) prompting. Explore more my LLM productions: websites that allows users to Upload pdf files and return information corresponding to questions from users - Details. Upload databases, return SQL code and result tables corresponding to user queries within few seconds while ensuring data security without feeding data to LLMs - Details. Table of Contents Main Components Overview What's LLMs? Building a Private LLM Data Curation Data Preprocessing LLM Training Evaluating LLMs RAG (Retrieval-Augmented Generation) Multi-Agent Systems LLMs Quantization Building Llama 3 Model from scratch Input Block with Byte Pair Encoding (BPE) Tokenizer Decoder Block with Sub-Components: RMS Norm (Root Mean Square Normalization) RoPE (Rotary Positional Encoding) KV Cache Group Query Attention FeedForward Network The Output Block Training & Evaluation Inference Example Retrieval-Augmented Generation (RAG) Chunking Data Retrieval Methods: Keyword Based & Vector Embeddings Ranking Retrieved Contexts Evaluation of Retrieved Contexts Augmented Generation Steps Challenges & Solutions in Retrieval Evaluation of RAG Chain of Thought (CoT) Theory of CoT Prompting Properties of CoT Prompting Tree-of-Thought Prompting Reasonings in CoT: Arithmetic, Commonsense, Symbolic Challenges & Limitations Concise CoT Basic Inference Code on CoT Reasoning LLM Fine-Tuning Strategies Fine-Tuning Process PEFT Techniques: Adapters, Prompt Tuning, Prefix Tuning, LoRA family, IA3, BOFT 1. Main Components Overview In this section, you will get a solid foundation of LLMs, their architecture, and the processes involved in building a private LLM: LLMs System Summary: Learn about the general components that make up an LLM system and what is required to build a private LLM, including Data Curation, Data Preprocessing, and LLM Training. RAG (Retrieval-Augmented Generation): Dive deep into how retrieval is integrated with language generation. Multi-Agent Systems: Explore how multiple LLMs can collaborate to solve tasks. LLM Quantization: Understand asymmetric and symmetric linear quantization techniques to optimize LLM models. 2. Building a Llama 3 Model Step by Step In this section, we take you through the complete process of building and training your own Llama 3 Model: Input Block with Byte Pair Encoding (BPE): Learn how to preprocess data using BPE to tokenize text efficiently. Decoder Block: Understand the core components like RMS Norm, RoPE, and Group Query Attention. Training & Evaluation: Walk through the process of training your model for 40 epochs and evaluating its performance. Inference: Get hands-on with an example query and response generation. 3. Retrieval-Augmented Generation (RAG) RAG combines the power of retrieval-based methods with generation-based approaches to improve response quality and accuracy: Chunking Data: Learn how to divide data into manageable pieces for easier retrieval. Retrieval Methods: Explore both Keyword-based Retrieval and Vector Embeddings techniques for retrieving relevant data. Ranking Retrieved Contexts: See how ranking plays a crucial role in selecting the best results. Evaluation of Retrieved Contexts: Understand how to assess whether retrieved content is valuable. Challenges and Solutions in Retrieval: Address common obstacles encountered in the retrieval process and how to overcome them. Evaluation of RAG: Learn how to assess the performance of your RAG system. 4. Chain of Thought (CoT) Chain of Thought (CoT) is a powerful technique that enhances LLM reasoning capabilities. This section covers everything you need to know about CoT: CoT Prompting: Understand how CoT improves reasoning by allowing models to follow a chain of thought. Tree-of-Thought Prompting: Learn about this variation in CoT prompting and how it extends model capabilities. Reasoning in CoT: Explore arithmetic, commonsense, and symbolic reasoning methods used in CoT. Concise CoT: Discover how to streamline CoT to make it more efficient. Inference Code on CoT: Get hands-on with basic inference code that uses CoT for reasoning. 5. LLM Fine-Tuning Strategies Fine-tuning is essential for adapting a pre-trained model to specific tasks. In this section, you will learn the best practices for fine-tuning your LLM: Fine-Tuning Process: Understand the steps involved in fine-tuning an LLM. PEFT (Parameter-Efficient Fine-Tuning): Explore different techniques like Adapters, Prompt Tuning, Prefix Tuning, and LoRA family to fine-tune your model efficiently. How to Contribute We welcome contributions from everyone! If you have improvements, bug fixes, or new features to add, feel free to fork the repository, create a branch, and submit a pull request. Getting Started To get started with this project, follow these steps: Clone the repository: git clone https://github.com/HoDangCao/LLMs-System.git Install dependencies: pip install -r requirements.txt Start exploring the notebooks in the LLMs_sys_summary.ipynb file for the first overview! Happy learning, and we hope you enjoy building LLMs as much as we do! 🚀 About Go through all main components of a LLM system from scratch: RAG, CoT, Llama 3 Resources Readme Activity Stars 0 stars Watchers 1 watching Forks 0 forks Report repository Releases No releases published Packages 0 No packages published Languages Jupyter Notebook 100.0% Footer © 2025 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can’t perform that action at this time.

Source: https://github.com/Analystics-AI/Guideline_RAG
GitHub - Analystics-AI/Guideline_RAG: This repository serves as a comprehensive guide for implementing a Retrieval-Augmented Generation (RAG) model within the context of a master thesis. It includes a step-by-step tutorial, best practices, and code examples to facilitate the development of a RAG model. Skip to content Navigation Menu Toggle navigation Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} Analystics-AI / Guideline_RAG Public template Notifications You must be signed in to change notification settings Fork 0 Star 1 This repository serves as a comprehensive guide for implementing a Retrieval-Augmented Generation (RAG) model within the context of a master thesis. It includes a step-by-step tutorial, best practices, and code examples to facilitate the development of a RAG model. analystics.ai/ License MIT license 1 star 0 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Pull requests 0 Discussions Security Insights Additional navigation options Code Pull requests Discussions Security Insights Analystics-AI/Guideline_RAG main Branches Tags Go to file Code Folders and files Name Name Last commit message Last commit date Latest commit History 71 Commits utils utils LICENSE LICENSE README.md README.md intro_rag.ipynb intro_rag.ipynb View all files Repository files navigation README MIT license Master_Thesis_RAG_Guideline This repository serves as a comprehensive guide for implementing a Retrieval-Augmented Generation (RAG) model within the context of a master thesis. It includes a step-by-step tutorial, best practices, and code examples to facilitate the development of a RAG model. To help you getting along with the Notebook we prepared an introduction video placed on Dropbox: https://www.dropbox.com/scl/fi/o7s6e3y3p5kl1qrtsz3to/Intro_RAG.mp4?rlkey=zxpdguoy85i2h6cfofn654l5r&st=bju4ov8g&dl=0 About This repository serves as a comprehensive guide for implementing a Retrieval-Augmented Generation (RAG) model within the context of a master thesis. It includes a step-by-step tutorial, best practices, and code examples to facilitate the development of a RAG model. analystics.ai/ Resources Readme License MIT license Activity Custom properties Stars 1 star Watchers 0 watching Forks 0 forks Report repository Releases No releases published Packages 0 No packages published Contributors 2 manuyweissel cafawo Fabian Woebbeking Languages Jupyter Notebook 51.8% Python 48.2% Footer © 2025 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can’t perform that action at this time.

Source: https://buildrag.com/tutorials/build-your-first-rag/build-rag-from-scratch/
Build RAG From Scratch: Step-by-Step Tutorial for Beginners | LlamaIndex - BuildRag Skip to content BuildRag Build RAG From Scratch: Step-by-Step Tutorial for Beginners | LlamaIndex Initializing search Home Blog Tutorials BuildRag Home Blog Blog Archive Archive 2024 Categories Categories Fine-Tuning Prompt Engineering RAG Tutorials Tutorials Getting Started Getting Started How does RAG work? Build Your First RAG Build Your First RAG Setting Up Your Development Environment Build a RAG System from Scratch Build a RAG System from Scratch Table of contents Install the required packages Set up the OpenAI API key Define Data Source Define Embedding Function Embed the Data Source Define Search Function Test the Search Function Extract the Retrieved Chunks Define Base Prompt Format the Prompt Generate Response What's Next? Rag Components Rag Components Data Parsing Data Parsing What is Data Parsing Parsing Text Documents Parsing Webpages Parsing PDFs Parsing CSVs Understanding Embeddings Vector Databases Explained Prompt Engineering Production RAG Production RAG Loading Data Chunking Strategies SQL for RAG Advanced Topics Advanced Topics More than vector search Table of contents Install the required packages Set up the OpenAI API key Define Data Source Define Embedding Function Embed the Data Source Define Search Function Test the Search Function Extract the Retrieved Chunks Define Base Prompt Format the Prompt Generate Response What's Next? Build a RAG from scratch¶ This tutorial will walk you through the process of building a RAG (Retrieval Augmented Generation) system from scratch without using libraries. Using High Level Libraries can build a demo faster, but they hide how things work inside. This guide will show you how RAG works step-by-step. This can really help you to understand the fundamental concepts of RAG pipeline. Note I highly recommend you to read How does RAG work? first before implementing this tutorial. In this tutorial, we'll not use vector database or chunk the data. This is to focus on the core concepts of RAG. Install the required packages¶ First, let's install the required packages: %pip install transformers openai torch torchvision --q
 Set up the OpenAI API key¶ Next, we'll set up our OpenAI API key, we will be using OpenAI model for this tutorial. You can use any other LLM that you want. If you havent created an OpenAI account yet, you can do it here. import os

os.environ["OPENAI_API_KEY"] = "sk-proj-..."
 Define Data Source¶ Now, let's define our corpus of documents, we pre-chunked the data as we want to focus on the core concepts of RAG instead. Chunking will be covered in another tutorial with more details. corpus_of_documents = [
    "At a YC event, Brian Chesky gave a memorable talk where he challenged conventional wisdom about running large companies. As Airbnb grew, he received advice to 'hire good people and give them room' which proved disastrous, leading him to develop his own management approach inspired by Steve Jobs.",
    "Many successful founders at the event reported similar experiences - following traditional management advice had damaged their companies instead of helping them grow. This raised questions about why everyone was giving founders the wrong advice.",
    "The answer emerged: founders were being told how to run companies like professional managers, not founders. There are two distinct modes of running a company: founder mode and manager mode. Most assumed scaling meant switching to manager mode.",
    "Founder mode remains largely undocumented - there are no specific books about it, and business schools don't acknowledge its existence. What we know comes from individual founders' experiments and experiences.",
    "The traditional manager approach treats parts of the org chart as black boxes, avoiding 'micromanagement' by delegating completely to direct reports. In practice, this often means hiring professional fakers who can damage the company.",
    "Founders report feeling gaslit both by advisors pushing manager mode and by employees when implementing it. This is a rare case where founders should trust their instincts despite widespread disagreement.",
    "Founder mode breaks the principle that CEOs should only engage via direct reports. Skip-level meetings become normal rather than unusual, opening up many new organizational possibilities.",
    "Steve Jobs' example of running annual retreats for Apple's 100 most important people (not necessarily highest-ranking) demonstrates an unconventional approach that could make big companies feel like startups.",
    "While founders can't run a 2000-person company exactly like a 20-person startup, the extent and nature of delegation in founder mode will vary by company and situation, making it more complex than manager mode.",
    "Early evidence suggests founder mode works better than manager mode, based on examples of founders who've found their way toward it, even when their methods were considered eccentric.",
    "The premise that founders must run their companies as managers has been accepted even in Silicon Valley. The dismay of founders who tried this approach and their success in finding alternatives suggests another way exists.",
    "Business education and literature focus almost exclusively on manager mode, leaving a gap in understanding how founders can effectively run larger companies while maintaining their unique advantages.",
    "Brian Chesky's success at Airbnb, demonstrated by their exceptional free cash flow margin, suggests that founder mode can produce superior results when properly implemented.",
    "The insight about different modes of company operation came from observing patterns in founder experiences, particularly their consistent struggles with conventional management wisdom.",
    "Founder mode may involve more direct engagement across organizational levels, breaking traditional management hierarchies while maintaining necessary delegation structures.",
    "The skills and approaches that work for professional managers may be fundamentally different from what works for founders, suggesting the need for distinct operational frameworks.",
    "The success of founders who rejected traditional management advice indicates that founder mode, while less understood, might be more effective for scaling companies.",
    "The lack of understanding about founder mode represents both a challenge and an opportunity - founders have achieved success despite following suboptimal advice.",
    "The story suggests that many successful founders may have independently discovered aspects of founder mode while being viewed as unconventional or difficult.",
    "The potential for improved company performance once founder mode is better understood and documented could lead to significant changes in how fast-growing companies are managed."
]
 Define Embedding Function¶ Next, we'll set up our embedding function. The embedder will turn our text into a embeddings by using BERT model that is pulled from Hugging Face. BERT is an open source model from Google. from transformers import AutoTokenizer, AutoModel
import torch

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

def embedder(chunk):
    """
    embed corpus of documents
    """
    tokens = tokenizer(chunk, return_tensors="pt", padding=True, truncation=True)

    with torch.no_grad():
        model_output = model(**tokens)

    embeddings = model_output.last_hidden_state[:, 0, :]
    embed = embeddings[0].numpy()
    return embed
 Embed the Data Source¶ Now, let's embed our corpus, this code maps each chunks that we defined earlier into embeddings. Each of the original text corpus is mapped to their corresponding embeddings. embedded_data_source = []

for chunk in corpus_of_documents:
    #embed the chunk
    embedding = embedder(chunk)

    #map the embedding to the chunk
    embedded_data_source.append((embedding, chunk))
 Define Search Function¶ Now, let's define our search function. This function will find the top k chunks that are most similar to the query. The function will embed the query, then compare to the embeddings of the data source. By using cosine similarity, it will find the most similar chunks compare to the query. import numpy as np

def search(query, data_source, k=5):
    """
    Search function to find top k similar chunks to the query
    """
    # Compute embedding for the query
    query_embedding = embedder(query)

    # Normalize the query embedding
    query_norm = np.linalg.norm(query_embedding)

    # Compute cosine similarities between query embedding and data source embeddings
    similarities = []
    for embedding, chunk in data_source:
        # Normalize the data source embedding
        embedding_norm = np.linalg.norm(embedding)

        # Compute cosine similarity
        similarity = np.dot(query_embedding, embedding) / (query_norm * embedding_norm)
        similarities.append((similarity, chunk))

    # Sort the similarities in descending order
    similarities.sort(reverse=True, key=lambda x: x[0])

    # Get the top k chunks
    top_k = similarities[:k]

    return top_k
 Test the Search Function¶ Now, let's test our search function. We'll use the query "explain what is founder mode" and find the top 5 chunks that are most similar to the query. user_query = "explain what is founder mode"
top_k_chunks = search(user_query, embedded_data_source, k=5)

for similarity, chunk in top_k_chunks:
    print(f"Similarity: {similarity:.4f}, Chunk: {chunk}")
 Extract the Retrieved Chunks¶ Now, let's extract the retrieved chunks. After we found the most similar chunks, we'll extract the text from the chunks. We only need the text after we found the most similar chunks. retrieved_chunks = []

#extract the chunks's text only from the top_k_chunks
for similarity, chunk in top_k_chunks:
    retrieved_chunks.append(chunk)

print(retrieved_chunks)
 Define Base Prompt¶ Now, let's define our base prompt. This prompt will be used to generate the response for our LLM. base_prompt = """You are an AI assistant for RAG. Your task is to understand the user question, and provide an answer using the provided contexts.

Your answers are correct, high-quality, and written by an domain expert. If the provided context does not contain the answer, simply state, "The provided context does not have the answer."

User question: {user_query}

Contexts:
{chunks_information}
"""
 Format the Prompt¶ Now, let's format the prompt. We'll replace the {user_query} with the user query, and replace the {chunks_information} with the retrieved chunks that we extracted earlier. #formatting the prompt
prompt = base_prompt.format(user_query=user_query, chunks_information="\n".join([chunk for chunk in retrieved_chunks]))

print(prompt)
 Generate Response¶ Finally, let's use OpenAI's API to generate a response. from openai import OpenAI

client = OpenAI()
response = client.chat.completions.create(
    model="gpt-4o-mini",
    temperature=0,
    messages=[
        {"role": "system", "content": prompt},
    ],
)

print(response.choices[0].message.content)
 This completes the RAG (Retrieval-Augmented Generation) process from scratch! Note This is just a demonstration of RAG to help you understand how RAG works. In the next chapter, we'll cover deeper into each components of RAG. Teaching you how to build a production-ready RAG system. What's Next?¶ Now that you understand the basic building blocks of RAG, you're ready to dive deeper! The next chapters will be covering RAG Components, helping you build even better systems. Back to top Previous Setting Up Your Development Environment Next Rag Components Copyright © 2024 Goh Young Jiet Made with Material for MkDocs

Source: https://medium.com/gjam/building-a-code-retrieval-system-with-rag-and-googles-gemini-api-a-step-by-step-guide-6fd4a072533b
Building a Code Retrieval System with RAG and Google’s Gemini API: A Step-by-Step Guide | by Japan Gor | GJAM | Medium Open in app Sign up Sign in Write Sign up Sign in Mastodon Building a Code Retrieval System with RAG and Google’s Gemini API: A Step-by-Step Guide Japan Gor · Follow Published in GJAM · 4 min read · Oct 17, 2024 -- Listen Share Introduction Retrieval-Augmented Generation (RAG) is a powerful technique that enhances the output of large language models (LLMs) by integrating external knowledge. In this article, I’ll show you how to build a code retrieval system using RAG with Google’s Gemini API. We’ll cover the entire workflow step-by-step, complete with code snippets, to help you easily follow along and create your own working system. If you’re interested in taking your LLMs to the next level by integrating them with existing knowledge repositories, this guide is for you! Be sure to follow me on social media for more content like this: Twitter: Japan Gor web: https://japangor.com LinkedIn: JAPANGOR What is Retrieval-Augmented Generation (RAG)? Retrieval-Augmented Generation (RAG) combines LLMs with an external knowledge retrieval component, allowing for better relevance and accuracy in responses. Instead of relying purely on a model’s pre-trained knowledge, RAG pulls in information from external sources to generate context-aware answers. This approach is particularly useful for specialized domains, such as code generation, where domain-specific knowledge is often required. Step-by-Step Workflow Step 1: Setting Up the Environment The first step is to install the necessary packages, including Google Cloud’s AI Platform SDK, LangChain for building LLM applications, and FAISS for managing the vector store. # Install Required Packages %pip install --upgrade --user -q google-cloud-aiplatform \     langchain langchain_google_vertexai langchain-community \     faiss-cpu nbformat If you’re using Google Colab, you’ll need to restart the runtime to use the newly installed packages. # Restart Runtime (for Colab) import sys if "google.colab" in sys.modules:     import IPython     app = IPython.Application.instance()     app.kernel.do_shutdown(True) Step 2: Authenticating and Importing Libraries To use Google Cloud services, authenticate your environment and import the necessary libraries. # Authenticate with Google Cloud (if needed) from google.colab import auth auth.authenticate_user() # Import Libraries import time from google.cloud import aiplatform from langchain.chains import RetrievalQA from langchain.prompts import PromptTemplate from langchain.vectorstores import FAISS from langchain.text_splitter import Language, RecursiveCharacterTextSplitter from langchain_google_vertexai import VertexAI, VertexAIEmbeddings import nbformat import requests Step 3: Initializing Vertex AI SDK Next, you need to initialize the Vertex AI SDK by specifying your project ID and location. # Initialize Vertex AI SDK PROJECT_ID = "your_project_id" LOCATION = "us-central1" vertexai.init(project=PROJECT_ID, location=LOCATION) Step 4: Setting Up the Gemini Model Now that we have the environment set up, let’s initialize the Gemini API for code generation. This model will be responsible for generating Python code based on user prompts. # Setup Model (Gemini API) code_llm = VertexAI(     model_name="gemini-1.5-pro",     max_output_tokens=2048,     temperature=0.1,     verbose=False, ) Step 5: Crawling GitHub for Jupyter Notebooks and Python Files The next step is to gather code examples. We will use the GitHub API to crawl the repository for .ipynb and .py files. # Define GitHub Token and Repository GITHUB_TOKEN = "your_github_token" GITHUB_REPO = "GoogleCloudPlatform/generative-ai" # Function to Crawl GitHub Repository for Notebooks def crawl_github_repo(url, is_sub_dir=False, access_token=GITHUB_TOKEN):     api_url = f"https://api.github.com/repos/{url}/contents" if not is_sub_dir else url     headers = {"Authorization": f"Bearer {access_token}"}     response = requests.get(api_url, headers=headers)     response.raise_for_status()     files = []     for item in response.json():         if item["type"] == "file" and item["name"].endswith((".py", ".ipynb")):             files.append(item["html_url"])         elif item["type"] == "dir":             files.extend(crawl_github_repo(item["url"], True))     return files# List Files in Repository code_files = crawl_github_repo(GITHUB_REPO) Step 6: Extracting Code from the Jupyter Notebooks After we have the list of .ipynb and .py files, we extract the code from them for indexing. # Extract Python Code from Notebooks def extract_code(github_url):     raw_url = github_url.replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")     response = requests.get(raw_url)     response.raise_for_status()     notebook = nbformat.reads(response.text, as_version=nbformat.NO_CONVERT)     return "\n".join(cell.source for cell in notebook.cells if cell.cell_type == "code") # Extract Code from Listed Files code_content = [extract_code(url) for url in code_files] Step 7: Chunking Code Strings To make it easier for the model to retrieve and understand the code, we split it into smaller chunks. # Chunking Code Strings text_splitter = RecursiveCharacterTextSplitter.from_language(     language=Language.PYTHON, chunk_size=2000, chunk_overlap=200 ) texts = text_splitter.split_documents(code_content) Step 8: Creating a Vector Store with FAISS We need a way to efficiently search through these code chunks. We’ll use FAISS to create a vector store. # Custom Embedding Class with Rate Limiting class CustomVertexAIEmbeddings(VertexAIEmbeddings):     def embed_documents(self, texts, batch_size=None):         return [self.client.get_embeddings([text]).values[0] for text in texts] # Create Vector Store using FAISS embeddings = CustomVertexAIEmbeddings(model_name="textembedding-gecko@latest") db = FAISS.from_documents(texts, embeddings) retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 5}) Step 9: Defining the Prompt Template and Querying the Model Now that everything is set up, we can define a prompt template and use the Gemini API to generate responses based on retrieved code snippets. # Define Prompt Template and Query prompt_template = PromptTemplate(     input_variables=["question"],     template="You are a Python developer. Respond with the correct code for: {question}" ) retrieval_chain = RetrievalQA(llm=code_llm, retriever=retriever, prompt_template=prompt_template) # Example Query to Test RAG question = "Write a Python function to fetch JSON data from a public API." response = retrieval_chain.run(question) print(response) Conclusion In this article, we’ve built a code retrieval system using RAG and the Gemini API. By integrating LLMs with an external knowledge base like GitHub, you can enhance the relevance of the generated output, especially for niche tasks like code generation. I hope this guide helps you get started with building your own RAG-based applications. If you have questions or need further clarification, feel free to reach out on social media. Twitter: Japan Gor web: https://japangor.com LinkedIn: JAPANGOR Happy coding! Llm Programming AI Automation Debugging -- -- Published in GJAM 8 Followers ·Last published Oct 17, 2024 Gjam.in is an innovative Indian tech hub offering cutting-edge software solutions and educational tools. Specializing in e-learning platforms, we empower users with dynamic, accessible digital education resources Follow Written by Japan Gor 13 Followers ·33 Following Founder at GJAM Follow No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams

Source: https://learnbybuilding.ai/tutorials/rag-from-scratch
A beginner's guide to building a Retrieval Augmented Generation (RAG) application from scratch Learn by Building AI Open main menu HomeTutorialsComparisonsNewsletter A beginner's guide to building a Retrieval Augmented Generation (RAG) application from scratch Retrieval Augmented Generation, or RAG, is all the rage these days because it introduces some serious capabilities to large language models like OpenAI's GPT-4 - and that's the ability to use and leverage their own data. This post will teach you the fundamental intuition behind RAG while providing a simple tutorial to help you get started. The problem with learning in a fast moving space There's so much noise in the AI space and in particular about RAG. Vendors are trying to overcomplicate it. They're trying to inject their tools, their ecosystems, their vision. It's making RAG way more complicated than it needs to be. This tutorial is designed to help beginners learn how to build RAG applications from scratch. No fluff, no (ok, minimal) jargon, no libraries, just a simple step by step RAG application. Jerry from LlamaIndex advocates for building things from scratch to really understand the pieces. Once you do, using a library like LlamaIndex makes more sense. Build from scratch to learn, then build with libraries to scale. Let's get started! Introducing our concept: Retrieval Augmented Generation You may or may not have heard of Retrieval Augmented Generation or RAG. Here's the definition from the blog post introducing the concept from Facebook: Building a model that researches and contextualizes is more challenging, but it's essential for future advancements. We recently made substantial progress in this realm with our Retrieval Augmented Generation (RAG) architecture, an end-to-end differentiable model that combines an information retrieval component (Facebook AI’s dense-passage retrieval system) with a seq2seq generator (our Bidirectional and Auto-Regressive Transformers BART model). RAG can be fine-tuned on knowledge-intensive downstream tasks to achieve state-of-the-art results compared with even the largest pretrained seq2seq language models. And unlike these pretrained models, RAG’s internal knowledge can be easily altered or even supplemented on the fly, enabling researchers and engineers to control what RAG knows and doesn’t know without wasting time or compute power retraining the entire model. Wow, that's a mouthful. In simplifying the technique for beginners, we can state that the essence of RAG involves adding your own data (via a retrieval tool) to the prompt that you pass into a large language model. As a result, you get an output. That gives you several benefits: You can include facts in the prompt to help the LLM avoid hallucinations You can (manually) refer to sources of truth when responding to a user query, helping to double check any potential issues. You can leverage data that the LLM might not have been trained on. The High Level Components of our RAG System a collection of documents (formally called a corpus) An input from the user a similarity measure between the collection of documents and the user input Yes, it's that simple. To start learning and understanding RAG based systems, you don't need a vector store, you don't even need an LLM (at least to learn and understand conceptually). While it is often portrayed as complicated, it doesn't have to be. The ordered steps of a querying RAG system We'll perform the following steps in sequence. Receive a user input Perform our similarity measure Post-process the user input and the fetched document(s). The post-processing is done with an LLM. A note from the paper itself The actual RAG paper is obviously the resource. The problem is that it assumes a LOT of context. It's more complicated than we need it to be. For instance, here's the overview of the RAG system as proposed in the paper. That's dense. It's great for researchers but for the rest of us, it's going to be a lot easier to learn step by step by building the system ourselves. Working through an example - the simplest RAG system Let's get back to building RAG from scratch, step by step. Here's the simplified steps that we'll be working through. Getting a collection of documents Below you can see that we've got a simple corpus of 'documents' (please be generous 😉). corpus_of_documents = [
    "Take a leisurely walk in the park and enjoy the fresh air.",
    "Visit a local museum and discover something new.",
    "Attend a live music concert and feel the rhythm.",
    "Go for a hike and admire the natural scenery.",
    "Have a picnic with friends and share some laughs.",
    "Explore a new cuisine by dining at an ethnic restaurant.",
    "Take a yoga class and stretch your body and mind.",
    "Join a local sports league and enjoy some friendly competition.",
    "Attend a workshop or lecture on a topic you're interested in.",
    "Visit an amusement park and ride the roller coasters."
] Defining and performing the similarity measure Now we need a way of measuring the similarity between the user input we're going to receive and the collection of documents that we organized. Arguably the simplest similarity measure is jaccard similarity. I've written about that in the past (see this post but the short answer is that the jaccard similarity is the intersection divided by the union of the "sets" of words. This allows us to compare our user input with the source documents. Side note: preprocessing A challenge is that if we have a plain string like "Take a leisurely walk in the park and enjoy the fresh air.",, we're going to have to pre-process that into a set, so that we can perform these comparisons. We're going to do this in the simplest way possible, lower case and split by " ". def jaccard_similarity(query, document):
    query = query.lower().split(" ")
    document = document.lower().split(" ")
    intersection = set(query).intersection(set(document))
    union = set(query).union(set(document))
    return len(intersection)/len(union) Now we need to define a function that takes in the exact query and our corpus and selects the 'best' document to return to the user. def return_response(query, corpus):
    similarities = []
    for doc in corpus:
        similarity = jaccard_similarity(user_input, doc)
        similarities.append(similarity)
    return corpus_of_documents[similarities.index(max(similarities))] Now we can run it, we'll start with a simple prompt. user_prompt = "What is a leisure activity that you like?" And a simple user input... user_input = "I like to hike" Now we can return our response. return_response(user_input, corpus_of_documents) 'Go for a hike and admire the natural scenery.'
 Congratulations, you've built a basic RAG application. I got 99 problems and bad similarity is one Now we've opted for a simple similarity measure for learning. But this is going to be problematic because it's so simple. It has no notion of semantics. It's just looks at what words are in both documents. That means that if we provide a negative example, we're going to get the same "result" because that's the closest document. user_input = "I don't like to hike" return_response(user_input, corpus_of_documents) 'Go for a hike and admire the natural scenery.'
 This is a topic that's going to come up a lot with "RAG", but for now, rest assured that we'll address this problem later. At this point, we have not done any post-processing of the "document" to which we are responding. So far, we've implemented only the "retrieval" part of "Retrieval-Augmented Generation". The next step is to augment generation by incorporating a large language model (LLM). Adding in a LLM To do this, we're going to use ollama to get up and running with an open source LLM on our local machine. We could just as easily use OpenAI's gpt-4 or Anthropic's Claude but for now, we'll start with the open source llama2 from Meta AI. ollama installation instructions are here This post is going to assume some basic knowledge of large language models, so let's get right to querying this model. import requests
import json First we're going to define the inputs. To work with this model, we're going to take user input, fetch the most similar document (as measured by our similarity measure), pass that into a prompt to the language model, then return the result to the user That introduces a new term, the prompt. In short, it's the instructions that you provide to the LLM. When you run this code, you'll see the streaming result. Streaming is important for user experience. user_input = "I like to hike"
relevant_document = return_response(user_input, corpus_of_documents)
full_response = []
# https://github.com/jmorganca/ollama/blob/main/docs/api.md
prompt = """
You are a bot that makes recommendations for activities. You answer in very short sentences and do not include extra information.
This is the recommended activity: {relevant_document}
The user input is: {user_input}
Compile a recommendation to the user based on the recommended activity and the user input.
""" Having defined that, let's now make the API call to ollama (and llama2). an important step is to make sure that ollama's running already on your local machine by running ollama serve. Note: this might be slow on your machine, it's certainly slow on mine. Be patient, young grasshopper. url = 'http://localhost:11434/api/generate'
data = {
    "model": "llama2",
    "prompt": prompt.format(user_input=user_input, relevant_document=relevant_document)
}
headers = {'Content-Type': 'application/json'}
response = requests.post(url, data=json.dumps(data), headers=headers, stream=True)
try:
    count = 0
    for line in response.iter_lines():
        # filter out keep-alive new lines
        # count += 1
        # if count % 5== 0:
        #     print(decoded_line['response']) # print every fifth token
        if line:
            decoded_line = json.loads(line.decode('utf-8'))
            
            full_response.append(decoded_line['response'])
finally:
    response.close()
print(''.join(full_response))  Great! Based on your interest in hiking, I recommend trying out the nearby trails for a challenging and rewarding experience with breathtaking views Great! Based on your interest in hiking, I recommend checking out the nearby trails for a fun and challenging adventure.
 This gives us a complete RAG Application, from scratch, no providers, no services. You know all of the components in a Retrieval-Augmented Generation application. Visually, here's what we've built. The LLM (if you're lucky) will handle the user input that goes against the recommended document. We can see that below. user_input = "I don't like to hike"
relevant_document = return_response(user_input, corpus_of_documents)
# https://github.com/jmorganca/ollama/blob/main/docs/api.md
full_response = []
prompt = """
You are a bot that makes recommendations for activities. You answer in very short sentences and do not include extra information.
This is the recommended activity: {relevant_document}
The user input is: {user_input}
Compile a recommendation to the user based on the recommended activity and the user input.
"""
url = 'http://localhost:11434/api/generate'
data = {
    "model": "llama2",
    "prompt": prompt.format(user_input=user_input, relevant_document=relevant_document)
}
headers = {'Content-Type': 'application/json'}
response = requests.post(url, data=json.dumps(data), headers=headers, stream=True)
try:
    for line in response.iter_lines():
        # filter out keep-alive new lines
        if line:
            decoded_line = json.loads(line.decode('utf-8'))
            # print(decoded_line['response'])  # uncomment to results, token by token
            full_response.append(decoded_line['response'])
finally:
    response.close()
print(''.join(full_response))  Sure, here is my response:

Try kayaking instead! It's a great way to enjoy nature without having to hike.
 Areas for improvement If we go back to our diagream of the RAG application and think about what we've just built, we'll see various opportunities for improvement. These opportunities are where tools like vector stores, embeddings, and prompt 'engineering' gets involved. Here are ten potential areas where we could improve the current setup: The number of documents 👉 more documents might mean more recommendations. The depth/size of documents 👉 higher quality content and longer documents with more information might be better. The number of documents we give to the LLM 👉 Right now, we're only giving the LLM one document. We could feed in several as 'context' and allow the model to provide a more personalized recommendation based on the user input. The parts of documents that we give to the LLM 👉 If we have bigger or more thorough documents, we might just want to add in parts of those documents, parts of various documents, or some variation there of. In the lexicon, this is called chunking. Our document storage tool 👉 We might store our documents in a different way or different database. In particular, if we have a lot of documents, we might explore storing them in a data lake or a vector store. The similarity measure 👉 How we measure similarity is of consequence, we might need to trade off performance and thoroughness (e.g., looking at every individual document). The pre-processing of the documents & user input 👉 We might perform some extra preprocessing or augmentation of the user input before we pass it into the similarity measure. For instance, we might use an embedding to convert that input to a vector. The similarity measure 👉 We can change the similarity measure to fetch better or more relevant documents. The model 👉 We can change the final model that we use. We're using llama2 above, but we could just as easily use an Anthropic or Claude Model. The prompt 👉 We could use a different prompt into the LLM/Model and tune it according to the output we want to get the output we want. If you're worried about harmful or toxic output 👉 We could implement a "circuit breaker" of sorts that runs the user input to see if there's toxic, harmful, or dangerous discussions. For instance, in a healthcare context you could see if the information contained unsafe languages and respond accordingly - outside of the typical flow. The scope for improvements isn't limited to these points; the possibilities are vast, and we'll delve into them in future tutorials. Until then, don't hesitate to reach out on Twitter if you have any questions. Happy RAGING :). References Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks Jerry Liu on Twitter advocating for users to build RAG from scratch

Source: https://blog.stackademic.com/rag-systems-made-easy-a-step-by-step-cost-and-implementation-guide-1213f908f590
RAG Systems Made Easy: A Step-by-Step Cost and Implementation Guide | by Saurabh Singh | Stackademic Open in app Sign up Sign in Write Sign up Sign in RAG Systems Made Easy: A Step-by-Step Cost and Implementation Guide Saurabh Singh · Follow Published in Stackademic · 8 min read · Sep 28, 2024 -- Listen Share Retrieval-augmented generation (RAG) systems combine the strengths of Language Models (LLMs) with powerful retrieval mechanisms, enabling context-rich responses from vast datasets. Unlike traditional LLMs, which rely solely on pre-existing knowledge, RAG systems can dynamically access relevant information, making them ideal for real-time applications. In this blog, we’ll provide a concise, step-by-step guide to building your RAG system, complete with an analysis of costs and hardware requirements to ensure you make informed decisions on your journey to leveraging this cutting-edge technology. 1. Introduction What is Retrieval-Augmented Generation (RAG)? Retrieval-augmented generation (RAG) is like having a super-smart assistant that not only relies on its own memory but also knows how to quickly look things up in a giant library. Here’s how it works: Retriever (Library Searcher): When you ask it a question, the retriever acts like a researcher who quickly runs to the library, scans through books, articles, or databases (external knowledge sources), and picks out the most relevant ones. Generator (Answer Composer): Once the retriever brings back the right materials, the generator (similar to AI like GPT) reads through the information and crafts a response. It doesn’t just copy what it finds; it combines the new info with its own understanding to give you a well-rounded, natural answer. Technical Summary: Retriever: Uses techniques like vector embeddings (basically finding documents with similar meanings) to pull relevant info based on your query. 2. Generator: Takes this info and uses a large language model (LLM) to generate a coherent and contextually accurate response, blending its pre-learned knowledge with fresh data. Why RAG is Important in the Age of Large Language Models (LLMs)? Retrieval-augmented generation (RAG) is vital in the era of Large Language Models (LLMs) because it overcomes their limitations, such as outdated knowledge and occasional inaccuracies. While LLMs are powerful, they have a knowledge cutoff and can generate incorrect information (“hallucinations”). RAG addresses this by retrieving real-time, reliable information from external sources, ensuring more accurate and up-to-date responses. It also allows LLMs to provide context-specific answers by pulling specialized knowledge and scaling across large databases that LLMs can’t memorize, making AI both smarter and more reliable. 2. How is RAG different than fine-tuning a LLM? RAG and fine-tuning are different in how they improve a language model. RAG is like giving the model access to a library — whenever you ask something, it can look up fresh information from external sources, keeping answers current and accurate. Fine-tuning, on the other hand, is like teaching the model new knowledge and skills and locking it in. The model becomes better at specific tasks but can’t adapt if the information changes. So, RAG is flexible and can update on the fly, while fine-tuning is more fixed and task-specific. How does Retrieval-Augmented Generation work? Retrieval-Augmented Generation (RAG) works through a combination of retrieval and generation processes. Here’s a technical breakdown of how it functions: Query Input: The user submits a query or question to the system. Retrieval Component: Document Search: The system employs a retriever model (often based on dense retrieval techniques) to search a knowledge base or document store. It uses methods like vector embeddings to find documents that are semantically relevant to the query. Ranking: The retrieved documents are ranked based on their relevance to the query. Selected Documents: The top-ranked documents are selected for further processing. These documents typically contain the information that will support the answer generation. Generation Component: Input Preparation: The selected documents and the original query are combined into a single input format that can be processed by the generative model. Text Generation: A language model (like GPT) generates a response by synthesizing the information from the retrieved documents and its own knowledge. This involves creating sentences that are coherent and contextually relevant. Output: The final output is presented to the user, which includes the newly generated response based on both the query and the retrieved data. Brief Description: In essence, RAG enhances traditional language models by incorporating real-time, relevant information from external sources. This two-step process of retrieving pertinent documents followed by generating a response allows RAG to produce more accurate, contextually enriched answers, making it particularly effective for applications requiring up-to-date knowledge or specialized information. When building a RetrievalAugmented Generation (RAG) system with a vector database for storing your extracted code, there are key considerations around cost, hardware requirements, and the step-by-step process. 3.StepbyStep Process Guide with Costing and Hardware Requirements for building your own RAG 1. Extract and Prepare Data Task: Extract the source code or relevant documents (e.g., codebase, manuals) from your repository (Azure DevOps). Cost: No direct cost unless using cloud services for hosting your data. Hardware: Basic computing power (any modern CPU-based machine should suffice). For scaling: Cloud storage services (Azure Blob Storage, AWS S3) can be used to store large datasets. 2. Embed the Code (Document Embedding) Task: Convert each piece of code into embeddings (vectors) using pre-trained models like BERT, SentenceBERT, OpenAI’s embeddings API, etc. This step will turn each document into a high-dimensional vector. Cost: Using OpenAI API: The embedding generation cost depends on the number of tokens processed (e.g., ~$0.0004 per 1k tokens for embeddings). Local Setup: If using open-source models, this step requires GPU power for faster processing but can also run on CPUs. Hardware: Local Setup: A modern CPU with at least 16 GB RAM for small projects. For large projects: GPUs (NVIDIA Tesla V100, A100) are recommended for faster embedding generation. 3. Choose and Set Up a Vector Database Task: Set up a vector database to store the embeddings of your code. Popular options include: 1. FAISS (open source, on-premise, or cloud): Great for local or small-scale projects. 2. Pinecone (cloud-based, fully managed): Ideal for enterprise-grade, large-scale projects. 3. Milvus (open-source, cloud-native): For highly scalable solutions. Cost: FAISS: Free, but you’ll need to manage infrastructure. Pinecone: Depends on usage. Starts at about $0.096/hour for a single replica on their standard plan. Milvus: Opensource, but you may incur infrastructure costs on cloud services. Hardware: FAISS: Requires basic compute resources for small scale (16–32 GB RAM, multicore CPU). Pinecone/Milvus: For larger datasets, cloud-based infrastructure (e.g., AWS, Azure, GCP) with GPUs or high memory CPUs for vector search. Typical setup: 32 GB RAM, multicore CPU for indexing millions of vectors. 4. Index the Document Embeddings Task: Store your document embeddings (vectors) in the vector database and create an index using algorithms like HNSW (Hierarchical Navigable Small World graphs) or Approximate Nearest Neighbor (ANN) for fast retrieval. Cost: FAISS (free, open-source) will only incur hardware costs. Pinecone/Milvus: Pricing is based on storage and query operations. For example, Pinecone’s starter plan costs about $72/month for small-scale storage and queries. Hardware: FAISS on-premise: Sufficient with a multicore CPU and 32 GB RAM. For enterprise-grade indexing with large datasets, consider cloud-based solutions or larger instances (e.g., 64+ GB RAM, more compute power). 5. Query Embedding Generation Task: When a user asks a question, convert the query into a vector (embedding) using the same or a similar model you used for your code embeddings. Cost: If you use OpenAI’s API, query embeddings cost around $0.0004/1k tokens. If done locally, the cost is tied to compute power (minimal if using CPUs for this task). Hardware: For real-time queries, cloud-based solutions (like OpenAI, Hugging Face) or running your embedding models on local servers can work, with CPU power for smaller queries. 6. Vector Search (Retrieval) Task: Perform a vector similarity search in the vector database to retrieve relevant code snippets or documents. The vector database searches for the top most similar code snippets based on the query vector. Cost: FAISS: No extra cost, only compute and memory usage. Pinecone/Milvus: You will incur costs based on the number of search operations. Hardware: For efficient vector search, the vector database should run on a machine with sufficient RAM and compute resources (32–64 GB RAM, multicore CPUs). Cloud options (AWS EC2, Azure VM) can scale based on query volume. 7. Retrieve Documents and Send to the LLM Task: Once the relevant code snippets are retrieved, pass them to the LLM (like GPT3 or LLaMA) for final augmented generation. Cost: Using OpenAI GPT3: Roughly $0.06–0.12 per 1000 tokens (for GPT3). If running LLMs locally, costs include infrastructure, particularly if using GPUs (like NVIDIA A100) to run models. Hardware: For local model inference: GPUs are recommended (e.g., NVIDIA V100, A100). Running the model locally will require 32–64 GB RAM and at least 1 GPU for handling the LLM inference. For cloud options: OpenAI API or Azure OpenAI can manage the inference, and you’ll only incur token usage costs without needing your own hardware. 8. Generate the Final Response Task: The LLM (with the retrieved code snippets as context) generates the final response to the user query. Cost: Similar to the previous step; either pay per token with an API or GPU costs if running locally. Hardware: Same as above, requires GPUs or cloud LLM access. Costing Breakdown: 1. Document Embedding Generation: OnPremise: GPU cost (onetime) or CPU for smaller data (~$0.01–$0.02 per query). API-based (e.g., OpenAI): ~$0.0004/1k tokens. 2. Vector Database Setup: FAISS: Free (on-premise), costs tied to the server. Pinecone: $0.096/hour or $72/month for small-scale storage. Milvus: Cloud costs tied to infrastructure usage. 3. LLM Inference: OpenAI GPT3: $0.06–0.12 per 1k tokens. Local LLMs: Requires GPUs, costing about $12k+ for infrastructure setup (A100 GPU), or can be rented via cloud for ~$3/hour for heavy inference loads. Recommended Hardware for Different Stages: CPU/Memory: For embedding generation and vector search: 32–64 GB RAM, multicore CPU. For small-scale RAG projects: Modern servergrade CPUs (Intel Xeon, AMD EPYC) work well. GPU: For embedding generation and inference (if done locally): NVIDIA V100 or A100 (recommended), or T4 for lower costs. CloudBased Alternatives: Use Pinecone or Milvus for vector storage and search, and OpenAI GPT3 for inference to avoid heavy GPU costs. Conclusion: By following these steps, you can set up your RAG system efficiently. Your code is stored as embeddings in a vector database for fast retrieval, and then relevant snippets are used to augment the LLM’s response for better, context-rich answers. You can choose to host this locally with FAISS or leverage cloud services for scaling and easier management. Stackademic 🎓 Thank you for reading until the end. Before you go: Please consider clapping and following the writer! 👏 Follow us X | LinkedIn | YouTube | Discord Visit our other platforms: In Plain English | CoFeed | Differ More content at Stackademic.com Generative Ai Tools Llm AI Web Development Tech -- -- Follow Published in Stackademic 31K Followers ·Last published 1 day ago Stackademic is a learning hub for programmers, devs, coders, and engineers. Our goal is to democratize free coding education for the world. Follow Follow Written by Saurabh Singh 13 Followers ·2 Following Freak in investing , coding and learning anything Follow No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams

Source: https://dev.to/hakeem/how-to-build-a-rag-model-from-scratch-bpf
How to build a RAG model from scratch? - DEV Community Skip to content Navigation menu Search Powered by Search Algolia Log in Create account DEV Community Close Add reaction Like Unicorn Exploding Head Raised Hands Fire Jump to Comments Save Boost More... Moderate Copy link Copy link Copied to Clipboard Share to X Share to LinkedIn Share to Facebook Share to Mastodon Share Post via... Report Abuse Hakeem Abbas Posted on Oct 28, 2024 How to build a RAG model from scratch? #rag #ai #llm #machinelearning Recent advancements in AI, particularly in the domain of large language models (LLMs) and generative models, have transformed how we interact with data. One such innovation is the Retrieval-Augmented Generation (RAG) model, which combines the benefits of retrieval-based methods and generative models. RAG enhances traditional generation methods by retrieving relevant information from a knowledge source (like a search index or document database) to augment and guide the generation process. This hybrid approach has shown to be more effective in tasks such as question answering, summarization, and knowledge-grounded dialogue. In this article, we'll cover the following: Overview of RAG Models: What are RAG models, and why do we need them? Architectural Components of a RAG Model: An in-depth look at the building blocks. Building the RAG Model: Step-by-step guide on implementing a RAG model from scratch. Training the RAG Model: Fine-tuning both the retrieval and generation components. Use Cases and Applications: Real-world applications of RAG models. By the end of this article, you'll understand how to build, train, and fine-tune your RAG model from scratch, as well as how to deploy it for real-world applications. 1. Overview of RAG Models What is a RAG Model? A RAG model is a hybrid system combining two AI techniques: Retrieval-based models: Systems that search for relevant pieces of information from a large set of documents or knowledge bases. Generative models: AI systems, such as GPT-3, that can generate coherent text from a prompt. The idea behind RAG is to combine the strengths of both approaches. Rather than generating answers from a fixed model that has limitations in knowledge scope and can hallucinate or invent facts, RAG allows the generative component to retrieve real, external knowledge first and then generate answers grounded in those retrieved facts. For instance, in question answering, a RAG model would: Use a retriever to gather relevant documents from a database. Use a generator to synthesize the final answer by combining the retrieved documents with the input query. RAG models thus excel in knowledge-intensive tasks because they can access a large and constantly updated repository of information rather than relying purely on what the language model was trained on. 2. Architectural Components of a RAG Model The RAG model has two main architectural components: the retriever and the generator. Each of these plays a crucial role in ensuring the model’s effectiveness in generating accurate, knowledge-grounded responses. 2.1 Retriever The retriever is responsible for searching and fetching relevant documents, passages, or snippets from a pre-built knowledge base. It typically involves the following steps: Indexing: Before querying, we need a pre-built index of documents or knowledge pieces, created using methods such as: - Dense passage retrieval (DPR) - BM25 (classic term-based retrieval model) - Sentence Transformers for embedding-based retrieval Embedding the Query: The input query is first transformed into a vector representation using an embedding model, typically a bi-encoder (like a BERT-based retriever). This vector is then used to search for the most relevant documents in the index. Document Scoring: Once the embeddings are obtained, the retriever computes a similarity score between the query embedding and the document embeddings. The top k documents are then selected. 2.2 Generator The generator is responsible for producing the final output based on the input query and the retrieved documents. This component is usually a pre-trained language model such as BART or T5, fine-tuned to take the retrieved documents and generate responses. Key elements of the generator: Input Formatting: The retrieved documents are concatenated with the query to form a single input sequence, which is passed to the language model. Text Generation: The model uses techniques such as beam search or nucleus sampling to generate the final output based on the input sequence. 3. Building the RAG Model from Scratch Building a RAG model from scratch involves multiple steps, from constructing the retriever to integrating the generator. Below is a step-by-step guide on building a simple RAG model using the Hugging Face Transformers library and FAISS for efficient retrieval. Step 1: Set Up the Environment To start, you'll need to install the necessary dependencies: These libraries are essential: transformers: For pre-trained language models. faiss: For building and querying the document index. sentence-transformers: For embedding queries and documents. datasets: For accessing and processing data. Step 2: Prepare the Data For the retriever, you need a large corpus of documents. Let’s assume you have a set of documents stored as text files. Each document should ideally be small (e.g., a few paragraphs) to make retrieval efficient. We will use FAISS to index and query these documents. Step 3: Implement the Retriever The retriever uses the FAISS index to retrieve relevant documents for a given query. Here's how to perform the retrieval: Now, the retriever is ready to use. Let’s test it with a query: Step 4: Implement the Generator The generator will be based on a pre-trained sequence-to-sequence model like BART or T5, which is capable of generating text from a query and additional context (retrieved documents). We will concatenate the retrieved documents and the query into a single input for the generator: Step 5: Putting it All Together Finally, we combine the retriever and the generator into a single pipeline that accepts a query and outputs a generated response: You should see the generator outputting an answer based on the retrieved documents, grounded in real-world knowledge. 4.1 Fine-Tuning the Retriever Fine-tuning the retriever can significantly improve its performance. A bi-encoder model (such as DPR) is often fine-tuned on a dataset of question-answer pairs to optimize the retrieval process. To fine-tune a retriever: Gather a dataset containing queries and their relevant documents. Use contrastive loss to train the model to maximize the similarity between queries and relevant documents while minimizing the similarity to irrelevant ones. Update the FAISS index with embeddings from the fine-tuned retriever. 4.2 Fine-Tuning the Generator Similarly, the generator can be fine-tuned on datasets like SQuAD, TriviaQA, or custom question-answer pairs. The fine-tuning process involves training the model to generate coherent answers given both a query and retrieved documents. Key steps: Gather (or create) a dataset of query, document, and answer triples. Fine-tune the sequence-to-sequence model on this dataset using cross-entropy loss. Validate and adjust hyperparameters such as learning rate and batch size. 5. Use Cases and Applications RAG models are versatile and can be applied to numerous real-world tasks, including: Open-domain Question Answering: RAG models can handle complex, knowledge-intensive questions by retrieving information from large corpora and generating answers. Knowledge-grounded Dialogue: Conversational agents can use RAG to access external knowledge during a conversation, enabling more informed and accurate responses. Document Summarization: RAG can be used to summarize lengthy documents by retrieving key information and generating concise summaries. Product Recommendations: Retrieval-augmented generation can assist in generating personalized recommendations based on retrieved user data and product descriptions. Conclusion Building a RAG model from scratch involves constructing both a retriever and a generator, fine-tuning them for optimal performance, and integrating them into a pipeline. The combination of these components results in a powerful model capable of answering knowledge-intensive questions, generating grounded responses, and interacting with external knowledge bases. With tools like Hugging Face Transformers, FAISS, and Sentence Transformers, the process of building a RAG model is accessible to AI practitioners and enthusiasts. By following the steps outlined in this guide, you can develop a fully functional RAG model tailored to your specific use cases, whether it’s open-domain question answering, knowledge-grounded dialogue, or document summarization. Top comments (0) Subscribe 
 Personal Trusted User Create template Templates let you quickly answer FAQs or store snippets for re-use. Submit Preview Dismiss Code of Conduct • Report abuse Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's permalink. Hide child comments as well Confirm For further actions, you may consider blocking this person and/or reporting abuse Read next Let AI Do Code Review For You Louis Liu - Jan 2 Top 7 Data Careers You Should Know About in 2025 TimesofAsi - Dec 2 '24 How to chat with Local LLM in Obsidian Luca Liu - Jan 4 Behavioral Questions in AI Interviews: 2025 Insights Vikas76 - Dec 5 '24 Hakeem Abbas Follow ex Facebook | ex Microsoft | ex-Skype Software Engineer | Helping people with new tech ideas Location 2010 El Camino Real # 124, Santa Clara, CA, 95050 Joined Sep 14, 2023 More from Hakeem Abbas How Small Language Models Are Redefining AI Efficiency #ai #llm #rag #programming Edge Computing and Large Language Models (LLMs): What’s the Connection? #llm #machinelearning #cloudcomputing #ai Reinforcement Learning with Human Feedback (RLHF) for Large Language Models (LLMs) #llm #machinelearning #ai #rag Thank you to our Diamond Sponsor Neon for supporting our community. DEV Community — A constructive and inclusive social network for software developers. With you every step of your journey. Home DEV++ Reading List Podcasts Videos Tags DEV Help Forem Shop Advertise on DEV DEV Challenges DEV Showcase About Contact Free Postgres Database Software comparisons Code of Conduct Privacy Policy Terms of use Built on Forem — the open source software that powers DEV and other inclusive communities. Made with love and Ruby on Rails. DEV Community © 2016 - 2025. We're a place where coders share, stay up-to-date and grow their careers. Log in Create account

Source: https://github.com/helmanofer/simple-rag
GitHub - helmanofer/simple-rag: This repository contains a simple and customizable implementation of Retrieval-Augmented Generation (RAG) using SQLModel, PostgreSQL with PGVector, and Cohere's embedding model. It accompanies the Medium blog post "Search with RAG: A Simple DIY Approach" offering a step-by-step guide to building your own RAG system. Skip to content Navigation Menu Toggle navigation Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} helmanofer / simple-rag Public Notifications You must be signed in to change notification settings Fork 1 Star 0 This repository contains a simple and customizable implementation of Retrieval-Augmented Generation (RAG) using SQLModel, PostgreSQL with PGVector, and Cohere's embedding model. It accompanies the Medium blog post "Search with RAG: A Simple DIY Approach" offering a step-by-step guide to building your own RAG system. 0 stars 1 fork Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 0 Pull requests 0 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights helmanofer/simple-rag main Branches Tags Go to file Code Folders and files Name Name Last commit message Last commit date Latest commit History 3 Commits .env .env README.md README.md db_models.py db_models.py docker-compose.yml docker-compose.yml embeddings.py embeddings.py index.py index.py rabin.txt rabin.txt requirements.txt requirements.txt search.py search.py settings.py settings.py View all files Repository files navigation README RAG-Simplified: A DIY Approach to Retrieval-Augmented Generation Introduction This repository contains a simple and customizable implementation of Retrieval-Augmented Generation (RAG) using SQLModel, PostgreSQL with PGVector, and Cohere's embedding model. It accompanies the Medium blog post "Search with RAG: A Simple DIY Approach" offering a step-by-step guide to building your own RAG system. Requirements Python 3.7 or higher PostgreSQL with PGVector extension Cohere API key (sign up at Cohere to obtain one) Setup Instructions Clone the repository: git clone git@github.com:helmanofer/simple_rag.git
 Install the required Python packages: cd simple_rag
pip install -r requirements.txt
 Set up your local PostgreSQL database using docker compose: docker compose up -d
 Update the .env file with your Cohere API key: COHERE_KEY=Your_Cohere_Key
 You're all set! Now you can start experimenting with the provided code and adapting it to your specific use case. Usage The repository includes the following key components: db_models.py: Defines the DTO models using SQLModel, including the Document class for storing text data and embeddings. embeddings.py: Contains the co_embed function, which utilizes Cohere's multilingual embedding model to generate embeddings for a list of texts. search.py: Implements the search functionality, allowing you to query the database and retrieve relevant documents based on cosine distance. index.py: Implements the indexing functionality, allowing you to store documents in the database Customization The beauty of this DIY approach is its simplicity and flexibility. You can easily adapt the code to your specific needs: Adjust the embedding model used in the co_embed function to match your requirements. Modify the Document class in document.py to include additional fields or adjust the embedding dimension. Experiment with different tokenization approaches or text preprocessing techniques to suit your data. Contributing Contributions are welcome! If you have suggestions, improvements, or bug fixes, feel free to open a pull request or create an issue. Please ensure that your contributions adhere to the project's code style and include appropriate documentation. License This project is licensed under the MIT License. See the LICENSE file for details. Happy searching, and stay tuned for more exciting NLP adventures! About This repository contains a simple and customizable implementation of Retrieval-Augmented Generation (RAG) using SQLModel, PostgreSQL with PGVector, and Cohere's embedding model. It accompanies the Medium blog post "Search with RAG: A Simple DIY Approach" offering a step-by-step guide to building your own RAG system. Topics llm retrieval-augmented-generation Resources Readme Activity Stars 0 stars Watchers 1 watching Forks 1 fork Report repository Languages Python 100.0% Footer © 2025 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can’t perform that action at this time.

Results for query: Tools and libraries for leveraging AST in improving code search accuracy

Search Engine: Yahoo
Source: https://ast-grep.github.io/advanced/how-ast-grep-works.html
How ast-grep Works: A bird's-eye view | ast-grep Skip to content ast-grep SearchK Main Navigation Guide Reference Command Line Interface Rule Config Rule Object Playground Manual Resources FAQ Rule Examples Custom Language Contributing BlogPlayground Appearance Menu Return to top Sidebar Navigation Guide Quick Start Pattern Syntax Rule Essentials Atomic Rule Relational Rule Composite Rule Utility Rule Project Setup Project Configuration Lint Rule Test Your Rule Error Report Rewrite Code Transform Code Rewriter Rule Tooling Overview Editor Integration JSON mode API Usage JavaScript API Python API Performance Tip Examples C C++ Go HTML Java Kotlin Python Ruby Rust TypeScript TSX YAML Reference Command Line Interface ast-grep run ast-grep scan ast-grep test ast-grep new Project Config Rule Config fix transformation rewriter Rule Object API Reference Language List Playground Manual Advanced Topics Frequently Asked Questions How ast-grep Works Core Concepts Pattern Syntax Pattern Match Algorithm How Rewrite Works Custom Language Support Multi-Language Documents Comparison with Other Tools Contributing Guide Development Add New Language Links Playground Blog VSCode Discord StackOverflow Reddit Docs.rs On this page How ast-grep Works: A bird's-eye view In the world of software development, efficiently searching, rewriting, linting, and analyzing code is essential for maintaining high-quality projects. This is where ast-grep comes into play. Designed as a powerful structural search tool, ast-grep simplifies these tasks by leveraging the Abstract Syntax Tree (AST) representation of code. Let's break down how ast-grep works with the help of a diagram. The Workflow of ast-grep Generally speaking, ast-grep takes user queries of various input formats, parses the code into an AST using TreeSitter, and performs search, rewrite, lint, and analysis, utilizing the full power of CPU cores. Query via Various Formats ast-grep can accept queries in multiple formats, making it flexible and user-friendly. Here are some common query formats: Pattern Query: Users can define specific patterns to search for within their codebase. YAML Rule: Structured rules written in YAML format to guide the search and analysis process. API Code: Direct API calls for more programmatic control over the searching and rewriting tasks. ast-grep's Core ast-grep's core functionality is divided into two main components: parsing and matching. 1. Parsing with Tree-Sitter The core of ast-grep's functionality relies on Tree-Sitter Parsers. TreeSitter is a powerful parsing library that converts source code into an Abstract Syntax Tree (AST). This tree structure represents the syntactic structure of the code, making it easier to analyze and manipulate. 2. Tree Matching Once the code is parsed into an AST, the ast-grep core takes over and finds the matching AST nodes based on the input queries. Written in Rust, ast-grep ensures efficient performance by utilizing full CPU cores. This means it can handle large codebases and perform complex searches and transformations quickly. Usage Scenarios ast-grep can be helpful for these scenarios. Search: Find specific patterns or constructs within the code. Rewrite: Automatically refactor or transform code based on predefined rules or patterns. Lint: Identify and report potential issues or code smells. Analyze: Perform in-depth code analysis to gather insights and metrics. Benefits of Using ast-grep Multi-Core Processing: ast-grep can handle multiple files in parallel by taking full advantage of multi-core processors. Typically ast-grep performs tasks faster than many other tools, making it suitable for large projects. Versatility: Whether you need to search for a specific code pattern, rewrite sections of code, lint for potential issues, or perform detailed analysis, ast-grep has you covered. Example in the Real World Pattern + Search: CodeRabbit uses ast-grep patterns to search code repo for code review knowledge. This example is collected from ast-grep's own dogfooding. API + Rewrite: @vue-macros/cli is a CLI for rewriting at Vue Macros powered by ast-grep. YAML + Lint: Vercel turbo is using ast-grep to lint their Rust code with custom rules. Conclusion ast-grep is a versatile and efficient tool for modern software development needs. By parsing code into an Abstract Syntax Tree and leveraging the power of Rust, it provides robust capabilities for searching, rewriting, linting, and analyzing code. With multiple input formats and the ability to utilize full CPU cores, ast-grep is designed to handle the demands of today's complex codebases. Whether you are maintaining a small project or a large enterprise codebase, ast-grep can help streamline your development workflow. Edit this page Last updated: Pager Previous pageFrequently Asked Questions Next pageCore Concepts Made with ❤️ with Rust Copyright © 2022-present Herrington Darkholme

Source: https://dbshisode.medium.com/unlocking-code-improvement-with-abstract-syntax-trees-asts-8b63de0b5d3d
Unlocking Code Improvement with Abstract Syntax Trees (ASTs) | by Deepak Shisode | Medium Open in app Sign up Sign in Write Sign up Sign in Unlocking Code Improvement with Abstract Syntax Trees (ASTs) Deepak Shisode · Follow 4 min read · Oct 10, 2024 -- Listen Share An Abstract Syntax Tree (AST) is a tree representation of the abstract syntactic structure of source code written in a programming language. Each node of the tree denotes a construct occurring in the source code. ASTs are compelling tools for code analysis, transformation, and optimization. By manipulating the AST of code, developers can automate code improvements, enforce coding standards, and even refactor code programmatically. In this answer, we’ll explore practical examples of using ASTs to improve code, focusing on: Code Analysis and Linting Code Refactoring Code Optimization Automated Testing and Debugging We’ll use Python for code examples, leveraging its built-in ast module, but the concepts apply to other programming languages with AST support. 1. Code Analysis and Linting Use Case: Detecting anti-patterns or enforcing code style guidelines. Example: Identifying unused imports or variables in Python code. Solution: Here’s how you can use AST to analyze code and find unused imports. import ast from collections import defaultdict    class ImportUsageVisitor(ast.NodeVisitor):      def __init__(self):          self.imports = {}          self.used_names = set()        def visit_Import(self, node):          for alias in node.names:              self.imports[alias.asname or alias.name] = node.lineno        def visit_ImportFrom(self, node):          module = node.module          for alias in node.names:              name = alias.asname or alias.name              full_name = f"{module}.{name}" if module else name              self.imports[name] = node.lineno        def visit_Name(self, node):          self.used_names.add(node.id)    def find_unused_imports(code):      tree = ast.parse(code)      visitor = ImportUsageVisitor()      visitor.visit(tree)      unused = set(visitor.imports.keys()) - visitor.used_names      for name in unused:          lineno = visitor.imports[name]          print(f"Unused import '{name}' at line {lineno}")    # Example usage:  code = """  import os  import sys  from collections import defaultdict    def foo():      print(os.name)  """    find_unused_imports(code) Output: Unused import 'defaultdict' at line 4 Unused import 'sys' at line 3 Explanation: We parse the code into an AST. ImportUsageVisitor collects all imported names and tracks where they are used. After traversal, we compare the imported names with the used names to find unused imports. 2. Code Refactoring Use Case: Automating repetitive refactoring tasks. Example: Renaming variables across a codebase while ensuring scope correctness. Solution: pip install astor import ast  import astor  # Third-party library to convert AST back to code    class VariableRenamer(ast.NodeTransformer):      def __init__(self, old_name, new_name):          self.old_name = old_name          self.new_name = new_name        def visit_Name(self, node):          if node.id == self.old_name:              return ast.copy_location(ast.Name(id=self.new_name, ctx=node.ctx), node)          return node    code = """  def compute_area(radius):      pi = 3.1415      area = pi * radius ** 2      return area  """    tree = ast.parse(code)  renamer = VariableRenamer('radius', 'r')  modified_tree = renamer.visit(tree)  modified_code = astor.to_source(modified_tree)  print(modified_code) Output: def compute_area(radius):     pi = 3.1415     area = pi * r ** 2     return area Explanation: VariableRenamer walks the AST and replaces instances of old_name with new_name. By using AST, we ensure that only variable names are changed, not strings or comments. 3. Code Optimization Use Case: Simplifying constant expressions to improve performance. Example: Folding constant expressions at compile time. Solution: import ast    class ConstantFolder(ast.NodeTransformer):      def visit_BinOp(self, node):          self.generic_visit(node)          if (isinstance(node.left, ast.Constant) and              isinstance(node.right, ast.Constant)):              result = eval(compile(ast.Expression(node), '', 'eval'))              return ast.copy_location(ast.Constant(value=result), node)          return node    code = """  def calculations():      x = 2 * 3      y = 4 + x      return y  """    tree = ast.parse(code)  folder = ConstantFolder()  optimized_tree = folder.visit(tree)  optimized_code = ast.unparse(optimized_tree)  print(optimized_code) Output: def calculations():     x = 6     y = 4 + x     return y Explanation: ConstantFolder evaluates binary operations with constant operands and replaces them with the result. This can reduce runtime computation, shifting it to compile time. 4. Automated Testing and Debugging Use Case: Instrumenting code to track execution or inject test hooks. Example: Adding logging statements before function calls. Solution: import ast    class FunctionCallLogger(ast.NodeTransformer):      def visit_Call(self, node):          self.generic_visit(node)          log_call = ast.Expr(value=ast.Call(              func=ast.Name(id='print', ctx=ast.Load()),              args=[ast.Constant(value=f"Calling function {ast.unparse(node.func)}")],              keywords=[]          ))          return [log_call, node]    code = """  def add(a, b):      return a + b    def multiply(x, y):      return x * y    result = multiply(add(2, 3), 4)  """    tree = ast.parse(code)  logger = FunctionCallLogger()  instrumented_tree = logger.visit(tree) instrumented_code = ast.unparse(instrumented_tree) print(instrumented_code) Output: def add(a, b):     return a + b def multiply(x, y):     return x * y result = print('Calling function multiply')multiply( print('Calling function add'), add(2, 3), 4) Explanation: FunctionCallLogger inserts a print statement before each function call. This can help in debugging by tracing function calls during execution. Additional Applications Security Analysis: Detecting vulnerable patterns, such as SQL injections or insecure function use. Code Formatters: Tools like Black (for Python) use ASTs to format code consistently. Language Transpilation: Converting code from one programming language to another by translating ASTs. Benefits of Using AST Language-Aware Transformation: AST manipulations understand the syntax and structure, reducing errors that occur with regex or string-based transformations. Safety and Accuracy: By operating on the AST, you avoid altering code in comments, strings, or unintended places. Automation: ASTs enable programmatic code transformations, which are scalable across large codebases. Differences Between AST Manipulation and Reflection Does AST Use Reflection Techniques? No, working with an Abstract Syntax Tree (AST) does not inherently involve using reflection techniques. While both AST manipulation and reflection allow a program to examine aspects of its own code, they operate at different stages and have different purposes in programming. Abstract Syntax Trees provide a powerful way to introspect and manipulate code programmatically. By leveraging ASTs, developers can automate code improvements, enforce standards, and optimize performance. Whether you’re building a linter, an automatic refactoring tool, or performing static analysis, ASTs are an indispensable tool in a developer’s toolkit Refection Abstract Syntax Tree Code Improvement Code Analysis Linting -- -- Follow Written by Deepak Shisode 1 Follower ·1 Following Follow No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams

Source: https://github.com/wallter/laravel-ai-code-analysis
GitHub - wallter/laravel-ai-code-analysis: Laravel AI Code Analysis Project: A Laravel-based tool leveraging OpenAI’s language models and PHP AST analysis for automated documentation, refactoring suggestions, and performance assessments. Enhances code maintainability and adherence to best practices with multi-pass AI-driven insights. Skip to content Navigation Menu Toggle navigation Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} wallter / laravel-ai-code-analysis Public Notifications You must be signed in to change notification settings Fork 1 Star 6 Laravel AI Code Analysis Project: A Laravel-based tool leveraging OpenAI’s language models and PHP AST analysis for automated documentation, refactoring suggestions, and performance assessments. Enhances code maintainability and adherence to best practices with multi-pass AI-driven insights. License Apache-2.0 license 6 stars 1 fork Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 0 Pull requests 1 Actions Security Insights Additional navigation options Code Issues Pull requests Actions Security Insights wallter/laravel-ai-code-analysis main Branches Tags Go to file Code Folders and files Name Name Last commit message Last commit date Latest commit History 763 Commits .vscode .vscode app app bootstrap bootstrap config config database database docs docs pass_orders pass_orders path/to path/to public public resources resources routes routes scripts scripts storage storage tests tests .editorconfig .editorconfig .env.example .env.example .env.testing .env.testing .gitattributes .gitattributes .gitignore .gitignore LICENSE LICENSE NOTICE NOTICE README.md README.md The error message The error message artisan artisan composer.json composer.json composer.lock composer.lock create.aider.admin_controller.create create.aider.admin_controller.create package-lock.json package-lock.json package.json package.json phpdoc.xml phpdoc.xml phpunit.xml phpunit.xml postcss.config.js postcss.config.js rector.php rector.php run_all_commands.sh run_all_commands.sh tailwind.config.js tailwind.config.js vite.config.js vite.config.js View all files Repository files navigation README Apache-2.0 license Laravel AI Code Analysis Project Overview This Laravel-based project seamlessly integrates OpenAI’s language models with PHP Abstract Syntax Tree (AST) analysis (powered by nikic/php-parser) along with other essential tooling such as PHPStan, PHP_CodeSniffer, Psalm, Rector, ESLint, Prettier, Pylint, golint, and Elixir’s formatter to deliver a comprehensive multi-pass code analysis. By iteratively scanning codebases across multiple languages, the system generates: Automated Documentation: Creates concise and clear documentation derived from raw code and AST data, enhancing code comprehension and maintainability. Refactoring Suggestions: Provides actionable recommendations to improve code clarity, adhere to best practices, and optimize overall structure. Functionality Assessments: Evaluates performance and maintainability aspects of the code, identifying potential bottlenecks and areas for improvement. Security Analysis: Identifies security vulnerabilities and suggests best practices to enhance code security. Performance Optimizations: Detects performance issues and recommends optimizations for efficient code execution. By leveraging queued AI operations, token usage tracking, and other advanced features, developers can enhance maintainability, optimize performance, and ensure cleaner, more efficient codebases (see config/ai.php). Table of Contents Laravel AI Code Analysis Project Overview Table of Contents How it works Usage TLDR; Running the UI & Server Features Tooling Code Parsing and Analysis Multi-Pass AI Analysis Artisan Commands Requirements Installation UI Setup Configuration AI Service Configuration Parsing Configuration Usage Artisan Commands Token & Cost Tracking Queued Analysis Testing Testing Screenshots Code Analysis Dashboard Documentation Generation Demo Output Final Summary of Analysis for ParseFilesCommand Strengths Areas for Improvement Functionality Enhancements Performance Optimizations Code Quality Improvements Overall Rating and Recommendations Contributing License How it works graph TD;
    PHP_Codebase[Your PHP Codebase] --> ParserService;
    ParserService --> AST[Abstract Syntax Tree AST Data];
    AST --> StaticAnalysis[Static Analysis Tools];
    AST --> AI_Analysis[AI Analysis];
    StaticAnalysis --> Tools[PHPStan, PHP_CodeSniffer, Psalm, Rector];
    AI_Analysis --> MultiPass[Multi-Pass AI Operations];
    MultiPass --> Documentation[Documentation Generation];
    MultiPass --> Refactoring[Refactoring Suggestions];
    MultiPass --> Functionality[Functionality Assessments];
    MultiPass --> Security[Security Analysis];
    MultiPass --> Performance[Performance Optimizations];
    Documentation --> Database[Database Storage];
    Refactoring --> Database;
    Functionality --> Database;
    Security --> Database;
    Performance --> Database;
    Tools[PHPStan, PHP_CodeSniffer, Psalm, Rector] --> Database;
 Loading Usage TLDR; composer install
npm install

# 1) Set up your .env with OPENAI_API_KEY, choose models
cp .env.example .env
php artisan key:generate

# 2) Migrate DB
php artisan migrate:fresh

# 3) Set your file/folder scanning in config/parsing.php

# 4) Run all static analyses and AI passes
php artisan static-analysis:run
php artisan analyze:files

# 5) Process queued jobs
php artisan queue:progress

# Alternatively, run all steps with a single command
composer run app

# Ensure you have a queue worker running if using asynchronous jobs
php artisan queue:work --stop-when-empty Check console output Running the UI & Server Start the Laravel development server to run the application locally: php artisan serve The server will start at http://localhost:8000 by default. You can access the application by navigating to this URL in your web browser. Monitor Laravel logs storage/logs/laravel.log for detailed output Features graph LR;
    subgraph Tooling
        PHPStan[PHPStan];
        PHPCS[PHP_CodeSniffer];
        Psalm[Psalm];
        Rector[Rector];
        ESLint[ESLint];
        Prettier[Prettier];
        Pylint[Pylint];
        GoLint[golint];
        ElixirFormatter[Elixir Formatter];
        Tinker[Laravel Tinker];
        OpenAI[OpenAI-PHP/Laravel];
    end;

    subgraph Code Parsing and Analysis
        Parsing[Comprehensive Parsing];
        AST[AST Insights];
        Metadata[Granular Metadata];
        Tracking[Persistent Tracking];
        Advanced[Supports Advanced Use Cases];
    end;

    subgraph Multi-Pass AI Analysis
        DocGen[Documentation Generation];
        Refactor[Refactoring Suggestions];
        Functionality[Functionality Analysis];
        Style[Style & Convention Review];
        Security[Security Analysis];
        Performance[Performance Optimizations];
    end;

    subgraph Artisan Commands
        Parse[parse:files];
        StaticRun[static-analysis:run];
        Analyze[analyze:files];
        Backup[db:backup];
        Restore[db:backup:restore];
    end;

    Tooling --> CodeParsing[Code Parsing and Analysis];
    CodeParsing --> AIAnalysis[Multi-Pass AI Analysis];
    AIAnalysis --> Commands[Artisan Commands];
 Loading Tooling This project leverages a suite of powerful tools to enhance code analysis and maintainability: PHPStan: A static analysis tool for PHP that focuses on finding bugs in your code without running it. PHP_CodeSniffer: Detects violations of a defined coding standard in your PHP code. Psalm: A static analysis tool for finding errors in PHP applications. Rector: An automated refactoring tool that upgrades and refactors your PHP code. ESLint: A static analysis tool for identifying problematic patterns found in JavaScript and TypeScript code. Prettier: An opinionated code formatter for maintaining consistent code style across JavaScript, TypeScript, and other languages. Pylint: A static code analyzer for identifying coding errors in Python. golint: A linter for Go source code. Elixir Formatter: A built-in formatter for Elixir code. Laravel Tinker: An interactive REPL for the Laravel framework, aiding in debugging and testing. OpenAI-PHP/Laravel: Facilitates integration with OpenAI’s API for AI-driven features. Code Parsing and Analysis Comprehensive Parsing: Analyzes files across multiple languages to extract detailed information about classes, methods, functions, traits, and annotations, providing a holistic view of the codebase. AST Insights: Captures detailed AST data, including node types, attributes, and structural relationships, enabling advanced analysis of code structure and behavior. Granular Metadata: Extracts metadata such as namespaces, file paths, line numbers, and method parameters to facilitate in-depth understanding and precise debugging. Persistent Tracking: Stores parsed data in a database, allowing for historical tracking, cross-referencing, and analysis over time. Supports Advanced Use Cases: Enables scenarios like dependency mapping, identifying code smells, and generating tailored documentation or refactoring suggestions based on rich structural insights. Multi-Pass AI Analysis Documentation Generation: Automatically creates concise, structured documentation from both AST data and raw code. Summarizes class purposes, key methods, parameters, and usage context with clarity. Refactoring Suggestions: Offers actionable recommendations to improve code structure, maintainability, and adherence to SOLID principles, with a focus on reducing duplication and enhancing clarity. Functionality Analysis: Evaluates the code for functional correctness, identifies edge cases, and highlights performance bottlenecks. Provides suggestions for enhancing scalability, reliability, and testability. Style & Convention Review: Ensures adherence to PSR standards and highlights inconsistencies in formatting, naming conventions, and documentation. Recommends improvements for readability and consistency. Security Analysis: Identifies security vulnerabilities and recommends best practices to enhance code security. Performance Optimizations: Detects performance issues and suggests optimizations for efficient code execution. Dependency Review: Analyzes external dependencies for compatibility, security risks, and outdated packages. Recommends updates and alternatives for deprecated or inefficient libraries. Requirements PHP: >= 8.0 Composer: To manage PHP dependencies. Laravel: Version 11.x SQLite: For the database. OpenAI API Key: To enable AI-driven features. Node.js & npm: For JavaScript/TypeScript tooling. Python: For running Pylint. Go: For running golint. Elixir: For running Elixir formatter. Installation Clone the Repository git clone https://github.com/your-username/laravel-ai-code-analysis.git
cd laravel-ai-code-analysis Install Dependencies composer install
npm install Set Up Environment Variables Copy the example environment file and configure the necessary variables. cp .env.example .env Open .env and set your OPENAI_API_KEY along with other configurations as needed. Generate Application Key php artisan key:generate Run Migrations php artisan migrate Install Additional Tools Depending on the static analysis tools you intend to use, install them via the appropriate package managers: Rector: composer require rector/rector --dev JavaScript/TypeScript Tools: npm install eslint prettier --save-dev
npx eslint --init
npx prettier --init Python Tools: pip install pylint Go Tools: go install golang.org/x/lint/golint@latest Elixir Tools: Ensure Elixir and Mix are installed. The Elixir formatter is built-in. UI Setup Install Frontend Dependencies Navigate to the resources/js directory and install the necessary npm packages: cd resources/js
npm install Build Frontend Assets Compile the frontend assets using Laravel Mix: npm run dev For production builds, use: npm run production Configuration AI Service Configuration The AI capabilities are configured in config/ai.php. This file defines the AI operations, multi-pass analysis settings, static analysis tools, and default model configurations. Configure via .env. API Credentials 'openai_api_key' => env('OPENAI_API_KEY'), Default AI Settings 'default' => [
    'model'       => env('AI_DEFAULT_MODEL', 'gpt-3.5-turbo'),
    'max_tokens'  => env('AI_DEFAULT_MAX_TOKENS', 500),
    'temperature' => env('AI_DEFAULT_TEMPERATURE', 0.5),
    'system_message' => 'You are a helpful AI assistant.',
], AI Operations Define each AI operation with specific configurations. 'passes' => [

    'doc_generation' => [
        'operation_identifier' => OperationIdentifier::DOC_GENERATION->value,
        'model' => 'gpt-4o',
        'max_tokens' => env('AI_DOC_GENERATION_MAX_TOKENS', 1200),
        'temperature' => env('AI_DOC_GENERATION_TEMPERATURE', 0.25),
        'type' => PassType::BOTH->value,
        'system_message' => 'You generate concise PHP documentation from code and AST to complement phpdoc.',
        'prompt_sections' => [
            'base_prompt' => 'Analyze the following code:',
            'guidelines' => [
                '- Create short but clear documentation from the AST data and raw code.',
                '- Summarize the purpose, methods, parameters, and usage context.',
                '- Avoid documenting __construct, getter, setter, and similar functions.',
                '- Exclude comment code blocks from the documentation.',
                '- Mention custom annotations, such as @url.',
                '- Limit the documentation to approximately 200 words.',
            ],
            'response_format' => 'Provide concise, human-readable documentation.',
        ],
    ],

    // ... other passes ...

], Static Analysis Tools Configuration Define static analysis tools and their configurations. 'static_analysis_tools' => [
    'PHPStan' => [
        'enabled' => true,
        'command' => 'vendor/bin/phpstan',
        'options' => ['analyse', '--no-progress', '--error-format=json'],
        'output_format' => 'json',
        'language' => 'php',
    ],
    'PHP_CodeSniffer' => [
        'enabled' => true,
        'command' => 'vendor/bin/phpcs',
        'options' => ['--report=json'],
        'output_format' => 'json',
        'language' => 'php',
    ],
    'Psalm' => [
        'enabled' => true,
        'command' => 'vendor/bin/psalm',
        'options' => ['--output-format=json'],
        'output_format' => 'json',
        'language' => 'php',
    ],
    'Rector' => [
        'enabled' => true,
        'command' => 'vendor/bin/rector',
        'options' => ['process'],
        'output_format' => 'json',
        'language' => 'php',
    ],
    'ESLint' => [
        'enabled' => true,
        'command' => 'npx eslint',
        'options' => ['--format=json'],
        'output_format' => 'json',
        'language' => 'javascript',
    ],
    'Prettier' => [
        'enabled' => true,
        'command' => 'npx prettier',
        'options' => ['--check'],
        'output_format' => 'json',
        'language' => 'javascript',
    ],
    'Pylint' => [
        'enabled' => true,
        'command' => 'pylint',
        'options' => ['--output-format=json'],
        'output_format' => 'json',
        'language' => 'python',
    ],
    'golint' => [
        'enabled' => true,
        'command' => 'golint',
        'options' => ['./...'],
        'output_format' => 'json',
        'language' => 'go',
    ],
    'ElixirFormatter' => [
        'enabled' => true,
        'command' => 'mix',
        'options' => ['format', '--check-formatted'],
        'output_format' => 'text',
        'language' => 'elixir',
    ],
    // Add more tools as needed
], Multi-Pass Analysis Order Configure the order and specifics of each analysis pass. 'operations' => [
    'multi_pass_analysis' => [
        'pass_order' => [
            'doc_generation',
            'functional_analysis',
            'style_convention',
            'static_analysis',
            'consolidation_pass',
            'scoring_pass',
            'laravel_migration',
            'laravel_migration_scoring',
            'security_analysis',
            'performance_analysis',
        ],
    ],
], Parsing Configuration In config/parsing.php, define: Folders to scan (recursively) for various languages Specific files to parse Ignore patterns similar to .gitignore Analysis Limits Set global limits to control the scope of analysis inside the config/ai.php configuration. Each of the parse:files and code:analyze commands offers options (--limit-class=1 --limit-method=1) to allow for fine-grained testing and calibration of AI prompts. 'analysis_limits' => [
    'limit_class'  => env('ANALYSIS_LIMIT_CLASS', 0),
    'limit_method' => env('ANALYSIS_LIMIT_METHOD', 0),
], File: config/parsing.php <?php

return [
    /*
    |--------------------------------------------------------------------------
    | Files or Directories to Parse
    |--------------------------------------------------------------------------
    |
    | An array of paths (absolute or relative) that should be scanned when
    | no explicit arguments are passed to the parse commands. For example:
    | ('folders' OR 'files') => [
    |     base_path('app/Services'), // a directory
    |     base_path('app/Helpers.php'), // single file
    | ],
    |
    */
    'folders' => [
        // Directories to parse recursively for various languages
        base_path('resources/access-app'),
        base_path('resources/views'),
        base_path('resources/js'),
        base_path('resources/css'),
        base_path('resources/typescript'),
        base_path('resources/python'),
        base_path('resources/go'),
        base_path('resources/elixir'),
        // Add more directories as needed
    ],

    'files' => [
        // Individual files to parse
        // base_path('app/Models/CodeAnalysis.php'),
        // base_path('app/Services/AI/CodeAnalysisService.php'),
    ],

    /*
    |--------------------------------------------------------------------------
    | Ignore Patterns
    |--------------------------------------------------------------------------
    |
    | Patterns for paths you want excluded (equivalent to your .gitignore or
    | .aiderignore logic). You can adapt or remove if you’re managing this
    | differently.
    |
    */
    'ignore' => [
        '.git',
        'vendor',
        'node_modules',
        'storage',
        'bootstrap/cache',
        'public/vendor',
        'public/js',
        'public/css',
        'resources/js',
        'resources/css',
        'resources/typescript',
        'resources/python',
        'resources/go',
        'resources/elixir',
        'tests/Fixtures',
        'tests/Temporary',
        // etc.
    ],
]; Usage Artisan Commands Parse Files php artisan parse:files --output-file=docs/parse_all.json --verbose Description: Collects files across multiple languages, stores discovered items (classes, functions, etc.) in the DB (via ParsedItem or similar). Run Static Analysis Tools php artisan static-analysis:run Description: Runs all enabled static analysis tools on CodeAnalysis entries that do not have existing static analyses. Analyze Files (Multi-Pass AI Operations) php artisan analyze:files --output-file=docs/analyze_all.json --verbose Description: Creates or updates a CodeAnalysis record for each file. Queues AI passes based on the configured multi-pass analysis. DB Backup / Restore php artisan db:backup
php artisan db:backup:restore Description: Backup or restore the SQLite DB as needed. Run Scripts (Optional) If you have additional scripts or tools to run across different languages, consider adding corresponding Artisan commands or integrating them into existing commands. Database Management Utilizes SQLite for simplicity and ease of use. Provides migration files to set up necessary database tables. Logging with Contextual Information Implements detailed logging using Laravel's Context facade for enhanced traceability and debugging. Token & Cost Tracking OpenAIService captures usage stats (prompt_tokens, completion_tokens, total_tokens) per request. AIResult stores the usage in metadata->usage. If desired, you can compute a cost estimate in USD by applying your own rate (e.g., $0.002 per 1K tokens). (See ProcessAnalysisPassJob or your service logic for examples.) Queued Analysis Multi-pass analysis (e.g., doc generation, performance, style, security, etc.) is queued via ProcessAnalysisPassJob. This prevents blocking the main process and improves reliability (retries on fail). Ensure you have a queue worker running: php artisan queue:work --stop-when-empty Once completed, results are in ai_results table. Testing php artisan test Coverage: Some tests focus on AST parsing or command execution. CI: Integrate into GitHub Actions for continuous testing. Testing The project includes PHPUnit tests to ensure the reliability of its features. Run Tests php artisan test Test Structure Tests are located in the tests directory and extend the base TestCase class. You can find feature tests for various commands and services. Screenshots Code Analysis Dashboard Documentation Generation Demo Output Click to expand `composer app` output Final Summary of Analysis for ParseFilesCommand The ParseFilesCommand class is a PHP console command designed to parse files across multiple languages and extract information about classes, methods, functions, traits, and annotations. The analysis has highlighted several strengths and areas for improvement regarding functionality, performance, error handling, code style, and compliance with language-specific standards. Strengths Well-Structured Code: The code is modular and adheres to the single responsibility principle, making it relatively easy to follow and maintain. Comprehensive Language Support: Supports multiple programming languages, enabling wide-ranging codebase analyses. Extensible Configuration: Easily add or remove static analysis tools through configuration files, enhancing flexibility. Automated Documentation: Generates clear and concise documentation, improving code maintainability. AI-Driven Insights: Provides actionable refactoring suggestions and functionality assessments using AI, aiding in code optimization and reliability. Security and Performance Analysis: Identifies potential security vulnerabilities and performance bottlenecks, contributing to robust and efficient codebases. Areas for Improvement Functionality Enhancements Enhanced Language Detection: Improve the accuracy of language detection based on file extensions and content to ensure appropriate tools are applied. Tool-Specific Configurations: Allow for more granular configurations of each static analysis tool to cater to project-specific needs. Parallel Processing: Implement parallel processing for running static analysis tools to reduce total analysis time. Performance Optimizations Efficient Database Queries: Optimize database interactions to handle large codebases more efficiently. Caching Mechanisms: Introduce caching for repeated analyses to decrease redundant processing. Resource Management: Monitor and manage system resources to prevent overconsumption during intensive analyses. Code Quality Improvements Comprehensive Testing: Expand the test suite to cover more scenarios and edge cases, ensuring greater reliability. Logging Enhancements: Implement more detailed logging levels (info, warning, error) to aid in debugging and monitoring. Documentation Consistency: Ensure all methods and classes are uniformly documented for better developer understanding. Error Handling: Implement more robust error handling to gracefully manage unexpected issues during analyses. Overall Rating and Recommendations Overall Rating: 8.5/10 Final Recommendations: Enhance language detection and tool configurations to better suit diverse projects. Optimize performance through parallel processing and efficient resource management. Expand the testing suite to cover a wider range of scenarios, ensuring robust and reliable analyses. Improve logging and documentation practices to facilitate easier maintenance and debugging. By addressing these recommendations, the Laravel AI Code Analysis Project can achieve higher efficiency, broader language support, and improved reliability, ultimately providing a more powerful tool for developers to maintain and optimize their codebases. Contributing Contributions are welcome! Please follow these steps to contribute: Fork the Repository Create a New Branch git checkout -b feature/YourFeatureName Make Your Changes Run Tests Ensure all tests pass before submitting.     # Optimize the code
    composer rector

    # Run Laravel Pint to format your code
    composer pint Commit Your Changes git commit -m "Add your detailed description here" Push to Your Fork git push origin feature/YourFeatureName Create a Pull Request Submit a pull request outlining your changes and their purpose. License This project is open-sourced software licensed under the Apache License 2.0. You may use, modify, and distribute this software under the terms of the Apache License. For more details, see the LICENSE file included in this repository. About Laravel AI Code Analysis Project: A Laravel-based tool leveraging OpenAI’s language models and PHP AST analysis for automated documentation, refactoring suggestions, and performance assessments. Enhances code maintainability and adherence to best practices with multi-pass AI-driven insights. Resources Readme License Apache-2.0 license Activity Stars 6 stars Watchers 1 watching Forks 1 fork Report repository Releases No releases published Packages 0 No packages published Languages PHP 69.0% Blade 30.4% Other 0.6% Footer © 2025 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can’t perform that action at this time.

Source: https://github.com/ast-grep/ast-grep
GitHub - ast-grep/ast-grep: ⚡A CLI tool for code structural search, lint and rewriting. Written in Rust Skip to content Navigation Menu Toggle navigation Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} ast-grep / ast-grep Public Notifications You must be signed in to change notification settings Fork 197 Star 8k ⚡A CLI tool for code structural search, lint and rewriting. Written in Rust ast-grep.github.io/ License MIT license 8k stars 197 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 36 Pull requests 11 Discussions Actions Projects 2 Wiki Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Wiki Security Insights ast-grep/ast-grep main Branches Tags Go to file Code Folders and files Name Name Last commit message Last commit date Latest commit History 3,089 Commits .cargo .cargo .github .github benches benches crates crates npm npm schemas schemas xtask xtask .editorconfig .editorconfig .gitignore .gitignore .pre-commit-config.yaml .pre-commit-config.yaml CHANGELOG.md CHANGELOG.md Cargo.lock Cargo.lock Cargo.toml Cargo.toml LICENSE LICENSE README.md README.md pyproject.toml pyproject.toml renovate.json renovate.json rust-toolchain.toml rust-toolchain.toml rustfmt.toml rustfmt.toml View all files Repository files navigation README MIT license ast-grep(sg) ast-grep(sg) is a CLI tool for code structural search, lint, and rewriting. Introduction ast-grep is an abstract syntax tree based tool to search code by pattern code. Think of it as your old-friend grep, but matching AST nodes instead of text. You can write patterns as if you are writing ordinary code. It will match all code that has the same syntactical structure. You can use $ sign + upper case letters as a wildcard, e.g. $MATCH, to match any single AST node. Think of it as regular expression dot ., except it is not textual. Try the online playground for a taste! Screenshot See more screenshots on the website. Installation You can install it from npm, pip, cargo, cargo-binstall, homebrew, scoop or MacPorts! npm install --global @ast-grep/cli
pip install ast-grep-cli
brew install ast-grep Click for more installation methods cargo install ast-grep --locked
cargo binstall ast-grep

# install via scoop, thank @brian6932
scoop install main/ast-grep

# install via MacPorts
sudo port install ast-grep

# try ast-grep in nix-shell
nix-shell -p ast-grep Or you can build ast-grep from source. You need to install rustup, clone the repository and then cargo install --path ./crates/cli --locked Packages are available on other platforms too. Command line usage example ast-grep has following form. sg --pattern 'var code = $PATTERN' --rewrite 'let code = new $PATTERN' --lang ts
 Example Rewrite code in null coalescing operator sg -p '$A && $A()' -l ts -r '$A?.()' Rewrite Zodios sg -p 'new Zodios($URL,  $CONF as const,)' -l ts -r 'new Zodios($URL, $CONF)' -i Implement eslint rule using YAML. Sponsor If you find ast-grep interesting and useful for your work, please buy me a coffee so I can spend more time on the project! Feature Highlight ast-grep's core is an algorithm to search and replace code based on abstract syntax tree produced by tree-sitter. It can help you to do lightweight static analysis and massive scale code manipulation in an intuitive way. Key highlights: An intuitive pattern to find and replace AST. ast-grep's pattern looks like ordinary code you would write every day (you could say the pattern is isomorphic to code). jQuery like API for AST traversal and manipulation. YAML configuration to write new linting rules or code modification. Written in compiled language, with tree-sitter based parsing and utilizing multiple cores. Beautiful command line interface :) ast-grep's vision is to democratize abstract syntax tree magic and to liberate one from cumbersome AST programming! If you are an open-source library author, ast-grep can help your library users adopt breaking changes more easily. if you are a tech lead in your team, ast-grep can help you enforce code best practice tailored to your business need. If you are a security researcher, ast-grep can help you write rules much faster. About ⚡A CLI tool for code structural search, lint and rewriting. Written in Rust ast-grep.github.io/ Topics refactoring search rust tree-sitter babel typescript command-line linter static-analysis ast codemod command-line-tool grep codereview structural-search Resources Readme License MIT license Activity Custom properties Stars 8k stars Watchers 24 watching Forks 197 forks Report repository Releases 141 0.34.4 Latest Feb 2, 2025 + 140 releases Sponsor this project Sponsor Learn more about GitHub Sponsors Packages 0 No packages published Contributors 54 + 40 contributors Languages Rust 93.7% TypeScript 3.5% Python 2.6% Other 0.2% Footer © 2025 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can’t perform that action at this time.

Source: https://www.projectscouts.com/ast-grep-an-efficient-code-search-tool-for-developers/
AST-Grep: An Efficient Code Search Tool for Developers Project Scouts Home About Contact us Privacy Policy TOS Add your project Sign in Subscribe By Project Scouts in Tree — Mar 2, 2024 AST-Grep: An Efficient Code Search Tool for Developers A brief introduction to the project: AST-Grep is a powerful code search tool developed to simplify and accelerate the process of finding specific code patterns and keywords within a large codebase. It is an open-source project hosted on GitHub and is designed to help developers find and analyze code quickly and efficiently. With the ability to search through code using abstract syntax trees (AST), AST-Grep offers developers a highly efficient and accurate code search solution. Significance and relevance of the project: As the size and complexity of software projects continue to grow, developers often face challenges when it comes to finding specific code snippets or understanding the overall structure of a project. Traditional text-based search tools can be slow and imprecise, leading to wasted time and effort. AST-Grep addresses these issues by utilizing abstract syntax trees, which provide a more accurate representation of the underlying code structure. Project Overview: AST-Grep aims to simplify the process of code search by allowing developers to search for code patterns, specific functions, or keywords within a codebase based on their abstract syntax trees. By working directly with the AST, AST-Grep can provide more precise and context-aware search results. This tool can help developers navigate through complex codebases, understand the relationships between different components, and identify potential issues or improvements. The target audience for AST-Grep includes software developers, engineers, and programmers who work with large codebases. It is particularly useful for those working in industries with complex software systems, such as web development, machine learning, data science, and artificial intelligence. Project Features: - Advanced Code Search: AST-Grep allows users to search for code patterns based on their abstract syntax tree representation, enabling more accurate and context-aware search results. - Regular Expression Support: Users can leverage regular expressions to create complex and powerful search patterns, providing enhanced flexibility and customization options. - Multiple Language Support: AST-Grep supports multiple programming languages, including Python, JavaScript, Java, C++, and more, making it suitable for a wide range of projects. - Command-Line Interface: The tool provides a command-line interface, making it easy to integrate into existing development workflows and automation scripts. - Fast and Efficient: AST-Grep is optimized for speed and efficiency, allowing users to search through large codebases quickly without compromising accuracy. Technology Stack: AST-Grep is primarily implemented in Python, leveraging its robust libraries and tools for parsing and manipulating abstract syntax trees. The choice of Python allows for easy integration with existing development ecosystems and enables cross-language support. Additionally, AST-Grep utilizes various Python libraries, such as `ast` and `regex`, to perform parsing and pattern matching operations efficiently. Project Structure and Architecture: AST-Grep follows a modular architecture that enables extensibility and flexibility. The main components of the project include: - Parser: Responsible for parsing the code files and generating the abstract syntax trees. - Search Engine: Performs search operations on the AST, matching user-defined patterns and keywords. - Command-Line Interface: Provides a user-friendly interface for running AST-Grep and passing search parameters. The project employs the visitor pattern, a well-known design pattern, to traverse the abstract syntax trees and perform code search operations efficiently. It also makes use of various data structures and algorithms, such as hash tables and regular expressions, to optimize search performance. Contribution Guidelines: AST-Grep is an open-source project that encourages contributions from the developer community. To contribute, developers can follow the guidelines outlined in the project's README file. These guidelines cover topics such as submitting bug reports, feature requests, and code contributions. It is important for contributors to adhere to the project's coding standards and documentation guidelines to maintain code quality and consistency. Overall, AST-Grep is a powerful tool that simplifies code search for developers working with large and complex codebases. By leveraging abstract syntax trees, it provides a more accurate and efficient solution, enabling developers to navigate through code, identify patterns, and improve their understanding of complex systems. With its open-source nature, AST-Grep encourages collaboration and contributions from the developer community, making it a valuable resource for improving code search workflows. Previous project Kotlin Language Server: A Powerful Tool for Kotlin Developers Next project RetroPie-Setup: Unlocking the Power of Retro Gaming Consoles Subscribe to Project Scouts Don’t miss out on the latest projects. Subscribe now to gain access to email notifications. tim@projectscouts.com Subscribe Project Scouts © 2025 Sign up

Source: https://github.com/cloudbring/ast-kg-rag-code-gen
GitHub - cloudbring/ast-kg-rag-code-gen: Research project leveraging Abstract Syntax Trees and Knowledge Graphs with Retrieval-Augmented Generation to develop an advanced, context-aware code generation system using Language Server Protocol and Large Language Models. Skip to content Navigation Menu Toggle navigation Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} cloudbring / ast-kg-rag-code-gen Public Notifications You must be signed in to change notification settings Fork 1 Star 6 Research project leveraging Abstract Syntax Trees and Knowledge Graphs with Retrieval-Augmented Generation to develop an advanced, context-aware code generation system using Language Server Protocol and Large Language Models. License MIT license 6 stars 1 fork Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 0 Pull requests 0 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights cloudbring/ast-kg-rag-code-gen master Branches Tags Go to file Code Folders and files Name Name Last commit message Last commit date Latest commit History 4 Commits .devcontainer .devcontainer .github .github .vscode .vscode docs docs notebooks notebooks research research src src tests tests .gitignore .gitignore CONTRIBUTING.md CONTRIBUTING.md LICENSE LICENSE README.md README.md requirements.txt requirements.txt View all files Repository files navigation README MIT license AST-KG-RAG Code Generation Project Project Overview This research project aims to leverage Language Servers and Abstract Syntax Trees (ASTs) to create a Knowledge Graph, which will then be used as the basis for a Retrieval-Augmented Generation (RAG) system to power LLM-based code generation. The project combines cutting-edge techniques in natural language processing, code analysis, and machine learning to create a novel approach to AI-assisted programming. Table of Contents Project Goals Technology Stack Project Phases Installation Usage Contributing License Project Goals Develop a system to parse and analyze code using Language Servers and ASTs Create a Knowledge Graph representation of code structures and relationships Implement a RAG system that utilizes the Knowledge Graph for context-aware retrieval Integrate the RAG system with an LLM to generate high-quality, context-aware code Evaluate the system's performance and demonstrate improvements over existing code generation techniques Research Our ongoing research is documented in the research folder. For an overview of our research organization and current topics, please see the research README . Technology Stack 💡 This will evolve over time Programming Language: Python 3.8+ AST Parsing: ast module (Python standard library) Language Server: Python Language Server Knowledge Graph: Neo4j or NetworkX Machine Learning Framework: PyTorch LLM Integration: Hugging Face Transformers Data Processing: Pandas, NumPy Visualization: Matplotlib, Plotly Project Phases Phase 1: Research and Preparation (4-6 weeks) Literature review on Language Servers, ASTs, and Knowledge Graphs Study of RAG systems and LLM-based code generation Tool and library selection Deliverables: Research summary report Annotated bibliography Initial project setup and environment configuration Phase 2: Design and Prototyping (6-8 weeks) Architecture design for AST parsing and Knowledge Graph construction Prototype implementation of AST to Knowledge Graph conversion RAG system architecture design Basic RAG prototype development Deliverables: System architecture document Proof-of-concept implementations Initial test results and performance metrics Phase 3: Implementation (8-10 weeks) Language Server integration and AST parsing implementation Knowledge Graph construction module development RAG system implementation with Knowledge Graph integration LLM integration for code generation Deliverables: Functional AST parsing and Knowledge Graph construction pipeline Integrated RAG system with LLM-based code generation capabilities Comprehensive test suite Phase 4: Testing and Evaluation (4-6 weeks) Development of test cases and evaluation metrics Thorough testing and performance evaluation Results analysis and implementation iteration Deliverables: Test results report Performance analysis document Refined and optimized system implementation Phase 5: Documentation and Presentation (2-3 weeks) Comprehensive documentation writing Final report preparation System demonstration or presentation creation Deliverables: User and developer documentation Final project report Project presentation or demo Installation (To be updated with specific installation instructions as the project progresses) Clone the repository: git clone https://github.com/cloudbring/ast-kg-rag-code-gen.git
cd ast-kg-rag-code-gen
 Set up a virtual environment: python -m venv venv
source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
 Install dependencies: pip install -r requirements.txt
 Set up additional components (e.g., Neo4j database) as required. Usage (To be updated with usage instructions as the project develops) Contributing This is a research project, and contributions are welcome. Please read the CONTRIBUTING.md file for details on our code of conduct and the process for submitting pull requests. License This project is licensed under the MIT License - see the LICENSE.md file for details. About Research project leveraging Abstract Syntax Trees and Knowledge Graphs with Retrieval-Augmented Generation to develop an advanced, context-aware code generation system using Language Server Protocol and Large Language Models. Resources Readme License MIT license Activity Stars 6 stars Watchers 4 watching Forks 1 fork Report repository Releases No releases published Packages 0 No packages published Footer © 2025 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can’t perform that action at this time.

Source: https://dl.acm.org/doi/10.1145/3661167.3661233
Leveraging Statistical Machine Translation for Code Search | Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering skip to main content Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search 10.1145/3661167.3661233acmotherconferencesArticle/Chapter ViewAbstractPublication PageseaseConference Proceedingsconference-collections ease Conference Proceedings Upcoming Events Authors Affiliations Award Winners More Home Conferences EASE Proceedings EASE '24 Leveraging Statistical Machine Translation for Code Search research-article Open access Share on Leveraging Statistical Machine Translation for Code Search Authors: Hung Phan, Ali JannesariAuthors Info & Claims EASE '24: Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering Pages 191 - 200 https://doi.org/10.1145/3661167.3661233 Published: 18 June 2024 Publication History 2citation296Downloads Metrics Total Citations2 Total Downloads296 Last 12 Months296 Last 6 weeks62 Get Citation Alerts New Citation Alert added! This alert has been successfully added and will be sent to: You will be notified whenever a record that you have chosen has been cited. To manage your alert preferences, click on the button below. Manage my Alerts New Citation Alert! Please log in to your account All formatsPDF Contents EASE '24: Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering Leveraging Statistical Machine Translation for Code Search Pages 191 - 200 PREVIOUS CHAPTER LEGION: Harnessing Pre-trained Language Models for GitHub Topic Recommendations with Distribution-Balance Loss Previous NEXT CHAPTER LLM-Based Chatbots for Mining Software Repositories: Challenges and Opportunities Next Abstract Supplemental Material References Information & Contributors Bibliometrics & Citations View Options References Figures Tables Media Share Abstract Machine Translation (MT) has numerous applications in Software Engineering (SE). Recently, it has been employed not only for programming language translation but also as an oracle for deriving information for various research problems in SE. In this application branch, MT’s impact has been assessed through metrics measuring the accuracy of these problems rather than traditional translation evaluation metrics. For code search, a recent work, ASTTrans, introduced an MT-based model for extracting relevant non-terminal nodes from the Abstract Syntax Tree (AST) of an implementation based on natural language descriptions. While ASTTrans demonstrated the effectiveness of MT in enhancing code search on small datasets with low embedding dimensions, it struggled to improve the accuracy of code search on the standard benchmark CodeSearchNet. In this work, we present Oracle4CS, a novel approach that integrates the classical MT model called Statistical Machine Translation to support modernized models for code search. To accomplish this, we introduce a new code representation technique called ASTSum, which summarizes each code snippet using a limited number of AST nodes. Additionally, we devise a fresh approach to code search, replacing natural language queries with a new representation that incorporates the results of our query-to-ASTSum translation process. Through experiments, we demonstrate that Oracle4CS can enhance code search performance on both the original BERT-based model UniXcoder and the optimized BERT-based model CoCoSoDa by up to 1.18% and 2% in Mean Reciprocal Rank (MRR) across eight selected well-known datasets. We also explore ASTSum as a promising code representation for supporting code search, potentially improving MRR by over 17% on average when paired with an optimal SMT model for query-to-ASTSum translation. Supplemental Material MP4 File This is the video presentation of my paper Download 65.00 MB References [1] [n. d.]. Article on treesitter. https://tinyurl.com/y2a86znt. Accessed: 2022-6-20. Google Scholar [2] [n. d.]. Articles about Machine Translation. https://tinyurl.com/5n7d6wrd. Accessed: 2023-5-1. Google Scholar [3] Yitian Chai, Hongyu Zhang, Beijun Shen, and Xiaodong Gu. 2022. Cross-Domain Deep Code Search with Meta Learning. In Proceedings of the 44th International Conference on Software Engineering (Pittsburgh, Pennsylvania) (ICSE ’22). Association for Computing Machinery, New York, NY, USA, 487–498. https://doi.org/10.1145/3510003.3510125 Digital Library Google Scholar [4] Saikat Chakraborty, Toufique Ahmed, Yangruibo Ding, Premkumar Devanbu, and Baishakhi Ray. 2022. NatGen: Generative pre-training by "Naturalizing" source code. arxiv:2206.07585 [cs.PL] Google Scholar [5] Le Chen, Quazi Ishtiaque Mahmud, Hung Phan, Nesreen K. Ahmed, and Ali Jannesari. 2023. Learning to Parallelize with OpenMP by Augmented Heterogeneous AST Representation. arxiv:2305.05779 [cs.LG] Google Scholar [6] Yihong Dong, Jiazheng Ding, Xue Jiang, Ge Li, Zhuo Li, and Zhi Jin. 2023. CodeScore: Evaluating Code Generation by Learning Code Execution. arxiv:2301.09043 [cs.SE] Google Scholar [7] Aryaz Eghbali and Michael Pradel. 2023. CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code. In Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering (Rochester, MI, USA) (ASE ’22). Association for Computing Machinery, New York, NY, USA, Article 28, 12 pages. https://doi.org/10.1145/3551349.3556903 Digital Library Google Scholar [8] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. In Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics, Online, 1536–1547. https://doi.org/10.18653/v1/2020.findings-emnlp.139 Crossref Google Scholar [9] Spence Green, Daniel Cer, and Christopher D Manning. 2014. Phrasal: A toolkit for new directions in statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation. 114–121. Crossref Google Scholar [10] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022. UniXcoder: Unified Cross-Modal Pre-training for Code Representation. https://doi.org/10.48550/ARXIV.2203.03850 Crossref Google Scholar [11] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. 2020. GraphCodeBERT: Pre-training Code Representations with Data Flow. CoRR abs/2009.08366 (2020). arXiv:2009.08366https://arxiv.org/abs/2009.08366 Google Scholar [12] Vincent J. Hellendoorn, Christian Bird, Earl T. Barr, and Miltiadis Allamanis. 2018. Deep Learning Type Inference(ESEC/FSE 2018). Association for Computing Machinery, New York, NY, USA, 152–162. https://doi.org/10.1145/3236024.3236051 Digital Library Google Scholar [13] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing Source Code with Transferred API Knowledge. In Proceedings of the 27th International Joint Conference on Artificial Intelligence (Stockholm, Sweden) (IJCAI’18). AAAI Press, 2269–2275. Digital Library Google Scholar [14] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of Semantic Code Search. https://doi.org/10.48550/ARXIV.1909.09436 Crossref Google Scholar [15] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2021. Code Prediction by Feeding Trees to Transformers. arxiv:2003.13848 [cs.SE] Google Scholar [16] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy Liang. 2019. SPoC: Search-based Pseudocode to Code. https://doi.org/10.48550/ARXIV.1906.04908 Crossref Google Scholar [17] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. 2022. CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning. arxiv:2207.01780 [cs.LG] Google Scholar [18] Haau-Sing Li, Mohsen Mesgar, André F. T. Martins, and Iryna Gurevych. 2023. Python Code Generation by Asking Clarification Questions. arxiv:2212.09885 [cs.CL] Google Scholar [19] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out. Association for Computational Linguistics, Barcelona, Spain, 74–81. https://aclanthology.org/W04-1013 Google Scholar [20] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arxiv:1907.11692 [cs.CL] Google Scholar [21] Adam Lopez. 2008. Statistical machine translation. ACM Computing Surveys (CSUR) 40, 3 (2008), 1–49. Digital Library Google Scholar [22] Yingwei Ma, Yue Yu, Shanshan Li, Zhouyang Jia, Jun Ma, Rulin Xu, Wei Dong, and Xiangke Liao. 2023. MulCS: Towards a Unified Deep Representation for Multilingual Code Search. In 2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). 120–131. https://doi.org/10.1109/SANER56733.2023.00021 Crossref Google Scholar [23] Yuetian Mao, Chengcheng Wan, Yuze Jiang, and Xiaodong Gu. 2023. Self-Supervised Query Reformulation for Code Search. arxiv:2307.00267 [cs.SE] Google Scholar [24] Daye Nam, Baishakhi Ray, Seohyun Kim, Xianshan Qu, and Satish Chandra. 2022. Predictive Synthesis of API-Centric Code. arxiv:2201.03758 [cs.SE] Google Scholar [25] Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N. Nguyen. 2014. Migrating Code with Statistical Machine Translation. In Companion Proceedings of the 36th International Conference on Software Engineering (Hyderabad, India) (ICSE Companion 2014). Association for Computing Machinery, New York, NY, USA, 544–547. https://doi.org/10.1145/2591062.2591072 Digital Library Google Scholar [26] Hoan Anh Nguyen, Hung Dang Phan, Samantha Syeda Khairunnesa, Son Nguyen, Aashish Yadavally, Shaohua Wang, Hridesh Rajan, and Tien Nguyen. 2023. A Hybrid Approach for Inference between Behavioral Exception API Documentation and Implementations, and Its Applications. In Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering (Rochester, MI, USA) (ASE ’22). Association for Computing Machinery, New York, NY, USA, Article 2, 13 pages. https://doi.org/10.1145/3551349.3560434 Digital Library Google Scholar [27] Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2015. Learning to generate pseudo-code from source code using statistical machine translation. In 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 574–584. Digital Library Google Scholar [28] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311–318. Google Scholar [29] Hung Phan and Ali Jannesari. 2023. Evaluating and Optimizing the Effectiveness of Neural Machine Translation in Supporting Code Retrieval Models: A Study on the CAT Benchmark. arXiv preprint arXiv:2308.04693 (2023). Google Scholar [30] Hung Phan, Hoan Anh Nguyen, Ngoc M. Tran, Linh H. Truong, Anh Tuan Nguyen, and Tien N. Nguyen. 2018. Statistical Learning of API Fully Qualified Names in Code Snippets of Online Forums. In Proceedings of the 40th International Conference on Software Engineering (Gothenburg, Sweden) (ICSE ’18). Association for Computing Machinery, New York, NY, USA, 632–642. https://doi.org/10.1145/3180155.3180230 Digital Library Google Scholar [31] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. CodeBLEU: a Method for Automatic Evaluation of Code Synthesis. arxiv:2009.10297 [cs.SE] Google Scholar [32] Ensheng Shi, Yanlin Wang, Wenchao Gu, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, and Hongbin Sun. 2023. CoCoSoDa: Effective Contrastive Learning for Code Search. In Proceedings of the 45th International Conference on Software Engineering (Melbourne, Victoria, Australia) (ICSE ’23). IEEE Press, 2198–2210. https://doi.org/10.1109/ICSE48619.2023.00185 Digital Library Google Scholar [33] Lin Shi, Fangwen Mu, Xiao Chen, Song Wang, Junjie Wang, Ye Yang, Ge Li, Xin Xia, and Qing Wang. 2022. Are We Building on the Rock? On the Importance of Data Preprocessing for Code Summarization. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (Singapore, Singapore) (ESEC/FSE 2022). Association for Computing Machinery, New York, NY, USA, 107–119. https://doi.org/10.1145/3540250.3549145 Digital Library Google Scholar [34] Zejian Shi, Yun Xiong, Yao Zhang, Zhijie Jiang, Jinjing Zhao, Lei Wang, and Shanshan Li. 2023. Improving Code Search with Multi-Modal Momentum Contrastive Learning. In 2023 IEEE/ACM 31st International Conference on Program Comprehension (ICPC). 280–291. https://doi.org/10.1109/ICPC58990.2023.00043 Crossref Google Scholar [35] Mohammed Latif Siddiq, Beatrice Casey, and Joanna C. S. Santos. 2023. A Lightweight Framework for High-Quality Code Generation. arxiv:2307.08220 [cs.SE] Google Scholar [36] Nikita Sorokin, Dmitry Abulkhanov, Sergey Nikolenko, and Valentin Malykh. 2023. CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search. arxiv:2305.11626 [cs.CL] Google Scholar [37] Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu Zhang. 2019. TreeGen: A Tree-Based Transformer Architecture for Code Generation. arxiv:1911.09983 [cs.LG] Google Scholar [38] Xunzhu Tang, zhenghan Chen, Saad Ezzini, Haoye Tian, Yewei Song, Jacques Klein, and Tegawende F. Bissyande. 2023. Hyperbolic Code Retrieval: A Novel Approach for Efficient Code Search Using Hyperbolic Space Embeddings. arxiv:2308.15234 [cs.SE] Google Scholar [39] Sindhu Tipirneni, Ming Zhu, and Chandan K. Reddy. 2023. StructCoder: Structure-Aware Transformer for Code Generation. arxiv:2206.05239 [cs.LG] Google Scholar [40] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S. Yu. 2018. Improving Automatic Source Code Summarization via Deep Reinforcement Learning. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering (Montpellier, France) (ASE ’18). Association for Computing Machinery, New York, NY, USA, 397–407. https://doi.org/10.1145/3238147.3238206 Digital Library Google Scholar [41] Deze Wang, Boxing Chen, Shanshan Li, Wei Luo, Shaoliang Peng, Wei Dong, and Xiangke Liao. 2023. One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization. arxiv:2303.15822 [cs.SE] Google Scholar [42] Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, and Qun Liu. 2022. Compilable Neural Code Generation with Compiler Feedback. arxiv:2203.05132 [cs.CL] Google Scholar [43] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, and Steven C. H. Hoi. 2023. CodeT5+: Open Code Large Language Models for Code Understanding and Generation. arXiv preprint (2023). Google Scholar [44] Yuanbo Wen, Qi Guo, Qiang Fu, Xiaqing Li, Jianxing Xu, Yanlin Tang, Yongwei Zhao, Xing Hu, Zidong Du, Ling Li, Chao Wang, Xuehai Zhou, and Yunji Chen. 2022. BabelTower: Learning to Auto-parallelized Program Translation. In Proceedings of the 39th International Conference on Machine Learning(Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (Eds.). PMLR, 23685–23700. https://proceedings.mlr.press/v162/wen22b.html Google Scholar [45] Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, and Houari Sahraoui. 2023. Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models. arxiv:2308.10462 [cs.SE] Google Scholar [46] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. CoRR abs/1609.08144 (2016). arXiv:1609.08144http://arxiv.org/abs/1609.08144 Google Scholar [47] Wang Yue, Wang Weishi, Joty Shafiq, and Hoi Steven, C.H.2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021. Google Scholar [48] Shuyan Zhou, Uri Alon, Sumit Agarwal, and Graham Neubig. 2023. CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code. (2023). https://arxiv.org/abs/2302.05527 Google Scholar Cited By View all Xiao YZuo XLu XDong JCao XBeschastnikh I(2025)Promises and perils of using Transformer-based models for SE researchNeural Networks10.1016/j.neunet.2024.107067184(107067)Online publication date: Apr-2025 https://doi.org/10.1016/j.neunet.2024.107067 Bibi NMaqbool ARana TAfzal FKhan A(2024)C2B: A Semantic Source Code Retrieval Model Using CodeT5 and Bi-LSTMApplied Sciences10.3390/app1413579514:13(5795)Online publication date: 2-Jul-2024 https://doi.org/10.3390/app14135795 Index Terms Leveraging Statistical Machine Translation for Code Search Software and its engineering Software notations and tools Software libraries and repositories Recommendations Syntactic discriminative language model rerankers for statistical machine translation This article describes a method that successfully exploits syntactic features for n-best translation candidate reranking using perceptrons. We motivate the utility of syntax by demonstrating the superior performance of parsers over n-gram language ... Read More Preventing translation quality deterioration caused by beam search decoding in neural machine translation using statistical machine translation Graphical abstract Display Omitted Abstract Decoding is an important part of machine translation systems, and the most popular inference algorithm used here is beam search. Beam search algorithm improves translation by allowing a larger search space to be traversed than greedy ... Read More Integrating source-language context into phrase-based statistical machine translation The translation features typically used in Phrase-Based Statistical Machine Translation (PB-SMT) model dependencies between the source and target phrases, but not among the phrases in the source language themselves. A swathe of research has demonstrated ... Read More Comments Please enable JavaScript to view thecomments powered by Disqus. Information & Contributors Information Published In EASE '24: Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering June 2024 728 pages ISBN:9798400717017 DOI:10.1145/3661167 Copyright © 2024 Owner/Author. This work is licensed under a Creative Commons Attribution International 4.0 License. Publisher Association for Computing Machinery New York, NY, United States Publication History Published: 18 June 2024 Check for updates Author Tags Abstract Syntax Tree Information Retrieval Statistical Machine Translation Qualifiers Research-article Research Refereed limited Data Availability This is the video presentation of my paper https://dl.acm.org/doi/10.1145/3661167.3661233#ease2024-66-presentation.mp4 Funding Sources Nation Science Foundation Conference EASE 2024 EASE 2024: 28th International Conference on Evaluation and Assessment in Software Engineering June 18 - 21, 2024 Salerno, Italy Acceptance Rates Overall Acceptance Rate 71 of 232 submissions, 31% Contributors Other Metrics View Article Metrics Bibliometrics & Citations Bibliometrics Article Metrics 2 Total Citations View Citations 296 Total Downloads Downloads (Last 12 months)296 Downloads (Last 6 weeks)62 Reflects downloads up to 02 Feb 2025 Other Metrics View Author Metrics Citations Cited By View all Xiao YZuo XLu XDong JCao XBeschastnikh I(2025)Promises and perils of using Transformer-based models for SE researchNeural Networks10.1016/j.neunet.2024.107067184(107067)Online publication date: Apr-2025 https://doi.org/10.1016/j.neunet.2024.107067 Bibi NMaqbool ARana TAfzal FKhan A(2024)C2B: A Semantic Source Code Retrieval Model Using CodeT5 and Bi-LSTMApplied Sciences10.3390/app1413579514:13(5795)Online publication date: 2-Jul-2024 https://doi.org/10.3390/app14135795 View Options View options PDF View or Download as a PDF file. PDF eReader View online with eReader. eReader HTML Format View this article in HTML Format. HTML Format Login options Check if you have access through your login credentials or your institution to get full access on this article. Sign in Full Access Get this Publication Figures Tables Media Share Share Share this Publication link Copy Link Copied! Copying failed. Share on social media XLinkedInRedditFacebookemail Affiliations Hung Phan Department of Computer Science, Iowa State University, USA https://orcid.org/0000-0001-7464-1597 View Profile Ali Jannesari Department of Computer Science, Iowa State University, USA https://orcid.org/0000-0001-8672-5317 View Profile Download PDF Go to Go to Show all references Request permissions Expand All Collapse Expand Table Authors Info & Affiliations View Table of Conten Export Citations Select Citation formatBibTeXEndNoteACM Ref Please download or close your previous search result export first before starting a new bulk export. Preview is not available. By clicking download,a status dialog will open to start the export process. The process may takea few minutes but once it finishes a file will be downloadable from your browser. You may continue to browse the DL while the export process is in progress. Download Download citation Copy citation Footer Categories Journals Magazines Books Proceedings SIGs Conferences Collections People About About ACM Digital Library ACM Digital Library Board Subscription Information Author Guidelines Using ACM Digital Library All Holdings within the ACM Digital Library ACM Computing Classification System Accessibility Statement Join Join ACM Join SIGs Subscribe to Publications Institutions and Libraries Connect Contact us via email ACM on Facebook ACM DL on X ACM on Linkedin Send Feedback Submit a Bug Report The ACM Digital Library is published by the Association for Computing Machinery. Copyright © 2025 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Your Search Results Download Request We are preparing your search results for download ... We will inform you here when the file is ready. Download now! Your Search Results Download Request Your file of search results citations is now ready. Download now! Your Search Results Download Request Your search export query has expired. Please try again.

Results for query: How to integrate AST with RAG workflows for efficient code retrieval?

Search Engine: Yahoo
Source: https://cloud.google.com/blog/products/ai-machine-learning/optimizing-rag-retrieval
RAG systems: Best practices to master evaluation for accurate and reliable AI. | Google Cloud Blog Jump to Content Cloud Blog Contact sales Get started for free Cloud Blog Solutions & technology AI & Machine Learning API Management Application Development Application Modernization Chrome Enterprise Compute Containers & Kubernetes Data Analytics Databases DevOps & SRE Maps & Geospatial Security Security & Identity Threat Intelligence Infrastructure Infrastructure Modernization Networking Productivity & Collaboration SAP on Google Cloud Storage & Data Transfer Sustainability Ecosystem IT Leaders Industries Financial Services Healthcare & Life Sciences Manufacturing Media & Entertainment Public Sector Retail Supply Chain Telecommunications Partners Startups & SMB Training & Certifications Inside Google Cloud Google Cloud Next & Events Google Cloud Consulting Google Maps Platform Google Workspace Developers & Practitioners Transform with Google Cloud Contact sales Get started for free AI & Machine Learning Optimizing RAG retrieval: Test, tune, succeed December 19, 2024 Hugo Selbie Customer Partner & Solutions Engineer Tom Pakeman Customer Partner & Solutions Engineer Join us at Google Cloud Next Early bird pricing available now through Feb 14th. Register Retrieval-augmented generation (RAG) supercharges large language models (LLMs) by connecting them to real-time, proprietary, and specialized data. This helps LLMs deliver more accurate, relevant, and contextually aware responses, minimizing hallucinations and building trust in AI applications. But RAG can be a double-edged sword: while the concept is straightforward – find relevant information and feed it to the LLM – its implementation is difficult to master. Done incorrectly, it can impact user trust in your AI's reliability. The culprit is often a lack of thorough evaluation. RAG systems that are not thoroughly evaluated lead to ‘silent failures’ which can undermine the reliability and trustworthiness of the system as a whole. In this blog post, we'll equip you with a series of best practices to identify issues within your RAG system and fix them with a transparent, automated evaluation framework. $300 in free credit to try Google Cloud AI and ML Build and test your proof of concept with $300 in free credit for new customers. Plus, all customers get free monthly usage of 20+ products, including AI APIs. Start building for free Step 1. Create a testing framework Testing a RAG system consists of running a set of queries against the tool and evaluating the output. A key prerequisite for rapid testing and iteration is to decide on a set of metrics as the definition of success, and calculate them in a rigorous, automated, and repeatable fashion. Below are some guidelines: Assemble a test dataset of high-quality questions Ensure that your test set covers a broad subset of the underlying data, and includes variations in phrasing and question complexity that match real-world use cases. Pro tip: It’s a good idea to consult with stakeholders and end users here to ensure the quality and relevance of this dataset. Assemble a ‘golden’ reference dataset of desired outputs to use in evaluation Although some metrics can be calculated without a reference dataset, having a set of known-good outputs allows us to produce a more comprehensive and nuanced range of evaluation metrics. Only change one variable at a time between test runs There are many features of a RAG pipeline that can make a difference – by changing them one at a time, we can be sure that a change in evaluation scores is attributable to a single feature alone. Similarly, we must ensure that between test runs we do not change the evaluation questions being used, the reference answers, or any system-wide parameters and settings. The basic process here is to change one aspect of the RAG system, run the battery of tests, adapt the feature, run the exact same battery of tests again and then see how the test results have changed. Once you are satisfied that a feature cannot be improved, freeze the configuration and move on to testing a separate part of the process. This testing framework can be visualized as three components: Reference questions and answers: The set of queries to be evaluated. Depending on which metrics are being calculated we may include corresponding reference answers. RAG processes The retrieval and summarization techniques being changing and evaluated Question outputs The evaluation outputs as scored by the testing framework Choosing appropriate metrics Establishing the best metrics to assess your system involves trial and error. Predefined testing frameworks exist that have been designed to speed up the process by providing prebuilt metrics that can also be adapted to your specific use case. This allows you to quickly generate baseline scores for the evaluation and refinement of your RAG system. From this baseline, you can then systematically modify retrieval and generation capabilities and measure any improvements. Common RAG evaluation frameworks include: Ragas Ragas is an open-source tool for evaluating RAG systems. It measures key aspects like factual accuracy, answer relevance, and how well retrieved content matches the question. Ragas also helps generate test data, making it easier for developers to improve RAG systems for accuracy and usefulness. Vertex AI gen AI evaluation service The Vertex AI gen AI evaluation service helps users test and compare generative models or applications based on custom metrics. It supports model selection, prompt engineering, and fine-tuning and allows users to define metrics, prepare data, run evaluations, and review results. The service works with Google's models, third-party models, and open models across various languages, using both model-based and computation-based assessment methods. Example metrics Model-based metrics utilize a proprietary Google model to assess the output of a candidate model. Functioning as an evaluator, this model scores responses based on predefined criteria. Pointwise metrics: The judge model assigns a numerical score (e.g., on a scale of 0-5) to the candidate model's output, indicating its alignment with the evaluation criteria. A higher score signifies a better fit. Pairwise metrics: The judge model compares the responses of two models and identifies the superior one. This approach is frequently employed to benchmark a candidate model against a baseline model. Computation-based metrics: These metrics utilize mathematical formulas to compare the model's output against a ground truth or reference. Popular examples include ROUGE and BLEU. Opinionated tiger team actions Collaborate with stakeholders to develop a set of "golden" question inputs. These questions should accurately reflect the main use cases the RAG system is intended to address. It's crucial to include a diverse range of query types, such as simple, complex, multi-part, and misspelled queries to ensure comprehensive testing. Make use of the Vertex AI generative AI evaluation framework. This framework allows developers to quickly implement multiple test metrics, and run multiple tests on a model’s performance with minimal setup. It offers a fast feedback loop, so improvements can be made rapidly. Conduct a pointwise evaluation of the RAG retrieval system. Generate model scores based on the following criteria: Response groundedness: The extent to which the generated text aligns with the factual information retrieved from the source documents. Verbosity: The length and detail of the response. While beneficial for providing comprehensive understanding, excessive verbosity may indicate difficulty in concisely and accurately answering the question. You may wish to tune this metric based on your use case. Instruction following: The system's ability to generate text that accurately and comprehensively adheres to given instructions, ensuring the output is relevant and aligned with user intent. Question answer quality as related to instructions: The ability of the RAG system to generate text that correctly answers a user's question with a high level of detail and coherence. Store results in a shared location such as Vertex AI Experiments, which allows for simple comparisons over time. Step 2. Root cause analysis and iterative testing The goal of setting up a repeatable testing framework is ideally understanding the root cause of issues. RAG is fundamentally based on two components: (1) the retrieval accuracy of your nearest neighbor matches and (2) the context that you provide to the LLM that generates your responses. Identifying and isolating these components individually allows you to determine the specific areas that may be causing problems and formulating testable hypotheses that can be performed as experiments and run in Vertex AI using the Gen AI evaluation framework. Typically when performing a root cause analysis exercise, the user will execute a testing run as a baseline, modify the implementation of one of the RAG components, and re-execute the testing run. The delta between the output scores of the testing metrics is the influence of the RAG component that was altered. The goal in this phase is to modify and document the components carefully, aiming to optimize towards a maximum score for each of the chosen metrics. Often the temptation is to make multiple modifications between testing runs which can mask the impact of a specific process and whether it was successful in creating a measurable change in your RAG system. Examples of RAG experiments to run Example RAG components to experiment with: What is the ideal number of neighbors for a document chunk that gets passed into an LLM to improve answer generation? How does embedding model choice affect retrieval accuracy? How do different chunking strategies affect quality? For example, adjusting variables like chunk size or overlap, or exploring strategies such as pre-processing chunks to summarize or paraphrase them with a language model. When it comes to generation, simply comparing Model A vs. Model B or Prompt A vs. Prompt B is particularly useful for fine-tuning prompt design or adjusting model configurations, helping developers to optimize models and prompts for specific use cases. What happens when you enrich documents with metadata like title, author, and tags for better retrieval signals? Opinionated tiger team actions Test model A vs model B for generation tasks (simple and can produce measurable results) Test chunking strategies for retrieval within a single embedding model (400 chars, 600 chars, 1200 chars, Full document text) Test pre-processing of long chunks to summarize them to smaller chunk sizes. Test what data is passed to the LLM as context. For example, do we pass the matched chunks themselves, or use these as a lookup to find the source document and pass the whole document text to the LLM, making use of long context windows. Step 3. Human evaluation Although quantitative metrics created by your testing framework provide valuable data, qualitative feedback from real users is also crucial. Automated testing tools are efficient for scalability and rapid iteration, but they cannot replicate human judgment in ensuring high-quality output. Human testers can evaluate subtle aspects like the tone of responses, the clarity of explanations, and potential ambiguity. Combining qualitative and quantitative testing provides a more holistic understanding of your RAG system’s performance. Human tests are typically run after you’ve achieved a solid level of baseline answer quality by optimizing evaluation metrics through the automated testing framework. You may wish to include human response evaluation as part of your broader user-testing motions for the system as a whole, such as performance, UX, etc. Similar to previous experiments, human testers can focus on specific system features following structured steps, or they can assess the overall application and provide comprehensive qualitative feedback. Because human testing is time consuming and repetitive, it is essential to identify users who are engaged and willing to provide meaningful feedback. Opinionated tiger team actions Identify key personas based on the RAG system’s target users Recruit a representative sample of participants that matches these personas to ensure realistic feedback. If possible, include both technical and non-technical user groups for testing Sit with the user (if possible) to ask follow-up questions and dig into the detail of their responses Conclusion To begin your own evaluation, explore Google Cloud’s generative AI evaluation service, where you can create both prebuilt and custom evaluation methodologies to enhance your RAG system. Posted in AI & Machine Learning Developers & Practitioners Related articles AI & Machine Learning How to build a strong brand logo with Imagen 3 and Gemini By Layolin Jesudhass • 4-minute read Sustainability Designing sustainable AI: A deep dive into TPU efficiency and lifecycle emissions By David Patterson • 4-minute read Partners Helping our partners co-market faster with AI By Aimee Catalano • 6-minute read Application Development Improving model performance with PyTorch/XLA 2.6 By Kyle Meggs • 5-minute read Footer Links Follow us Google Cloud Google Cloud Products Privacy Terms Cookies management controls Help Language‪English‬‪Deutsch‬‪Français‬‪한국어‬‪日本語‬

Source: https://vitalflux.com/create-rag-application-langgraph-langchain-example-code/
Creating a RAG Application Using LangGraph: Example Code Analytics Yogi Reimagining Data-driven Society with Agentic AI Home Prompt Library DS/AI Trends Stats Tools AI Research Admissions Courses Interview Questions Generative AI Statistics Machine Learning NLP Deep Learning Select a pageHome Prompt Library DS/AI Trends Stats Tools AI Research Admissions Courses Interview Questions Generative AI Statistics Machine Learning NLP Deep Learning Creating a RAG Application Using LangGraph: Example Code February 1, 2025 by Ajitesh Kumar · Leave a comment Retrieval-Augmented Generation (RAG) is an innovative generative AI method that combines retrieval-based search with large language models (LLMs) to enhance response accuracy and contextual relevance. Unlike traditional retrieval systems that return existing documents or generative models that rely solely on pre-trained knowledge, RAG technique dynamically integrates context as retrieved information related to query with LLM outputs. LangGraph, an advanced extension of LangChain, provides a structured workflow for developing RAG applications. This guide will walk through the process of building a RAG system using LangGraph with example implementations. Table of Contents Toggle Setting Up the Environment Initializing the Language Model Selecting an Embedding Model and Vector Store Collecting Data for the Knowledge Base Processing Documents and Storing Vectors Designing the RAG Workflow Setting Up the Prompt Defining the System State Implementing Retrieval and Response Generation Building and Running the Workflow Conclusion Setting Up the Environment To get started, we need to install the necessary dependencies. The following commands will ensure that all required LangChain and Langgraph packages are available: !pip install langchain-openapi langchain-community langchain-text-splitters langgraph --quiet --upgrade
!pip install langchain_openai
 Next, configure environment variables to enable seamless API interactions. We set up LangSmith key and tracing for audit and OpenAI API key to integrate with OpenAI chat models. import os

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = 'lsv2_pt-xxx'
os.environ["OPENAI_API_KEY"] = 'sk-proj-xxxx'
 Initializing the Language Model A core component of the RAG system is the LLM, which generates responses by integrating context based on retrieving contextual information related to user question. Here, we initialize OpenAI’s GPT-4o-mini model: from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")
 Selecting an Embedding Model and Vector Store To efficiently retrieve relevant information, we use an embedding model to convert text into vector representations. An embedding model maps words, sentences, or entire documents into high-dimensional numerical vectors, capturing semantic relationships between them. This transformation enables efficient similarity searches, allowing the system to retrieve contextually relevant information based on query inputs. OpenAI provides several powerful embedding models, including text-embedding-3-large, text-embedding-3-small, and text-embedding-ada-002, each designed for different performance and efficiency trade-offs. text-embedding-3-large offers high accuracy and deeper semantic understanding, making it suitable for complex retrieval tasks, while text-embedding-3-small is optimized for lower computational costs with good performance. text-embedding-ada-002, a widely used earlier model, balances performance and efficiency for various natural language processing tasks. The following code can be used for incorporate embedding model to convert text into vector representations. These vectors are stored in an in-memory vector database for quick lookup: from langchain_openai import OpenAIEmbeddings
from langchain_core.vectorstores import InMemoryVectorStore

embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")
vectorstore = InMemoryVectorStore(embedding_model)
 Collecting Data for the Knowledge Base To enable retrieval, we need a dataset. In this example, we extract content from publicly available news articles related to digital arrest scams. This can be built into a chat application which helps users interact and validate whether they are becoming victim of digital arrest scam. The idea is to use content from upcoming news and create embeddings and store them in a vector store. When user sends the query, appropriate piece of information is retrieved from the vectorstore matching the query, and the context and query can be passed as prompt to LLM to generate the answer. import requests
from bs4 import BeautifulSoup
import json

urls = [
    "https://timesofindia.indiatimes.com/city/vijayawada/seven-cybercons-arrested-for-digital-arrest-scam-in-andhra-pradesh/articleshow/117649089.cms",
    "https://timesofindia.indiatimes.com/city/bengaluru/digital-arrest-fraud-elderly-law-professor-in-bengaluru-duped-of-rs-7-lakh/articleshow/117751902.cms"
]

documents = []
for url in urls:
    try:
        response = requests.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            script_tags = soup.find_all('script', {'type': 'application/ld+json'})
            for script in script_tags:
                try:
                    json_data = json.loads(script.string)
                    if "articleBody" in json_data:
                        documents.append(json_data["articleBody"])
                        break
                except (json.JSONDecodeError, TypeError):
                    continue
        else:
            print(f"Failed to fetch {url}, Status code: {response.status_code}")
    except requests.RequestException as e:
        print(f"Error fetching {url}: {e}")
 Processing Documents and Storing Vectors To optimize retrieval, we split lengthy documents into smaller segments before storing them in the vector database. This process improves search precision and contextual matching: from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

docs = [Document(page_content=article) for article in documents]
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
all_splits = text_splitter.split_documents(docs)
vectorstore.add_documents(documents=all_splits)
 Designing the RAG Workflow Setting Up the Prompt The code initializes a predefined prompt template from LangChain’s hub to guide the response generation process. By pulling the “rlm/rag-prompt” template, it ensures that the language model follows a structured format when generating answers. from langchain import hub
prompt = hub.pull("rlm/rag-prompt")
 This is how the rag prompt look like: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question} 
Context: {context} 
Answer:
 Defining the System State The application state in this RAG workflow is defined using a TypedDict named State, which organizes data at different stages of processing. It includes the user’s input question, a list of retrieved documents providing contextual information, and the generated response. This structured representation ensures smooth data flow between retrieval and generation steps, enabling efficient knowledge retrieval and response synthesis within the LangGraph framework. The RAG system requires a structured state that tracks user queries, retrieved contexts, and generated responses: from langgraph.graph import START, StateGraph
from typing_extensions import List, TypedDict

class State(TypedDict):
    question: str
    context: List[Document]
    answer: str
 Implementing Retrieval and Response Generation The retrieval function fetches relevant content from the vector database, while the generation function synthesizes an AI-driven response: def retrieve(state: State):
    retrieved_docs = vectorstore.similarity_search(state["question"])
    return {"context": retrieved_docs}

def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {"answer": response.content}
 Building and Running the Workflow The LangGraph workflow connects retrieval and generation in a structured manner to ensure seamless execution: graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()
 Testing the system with an example query: response = graph.invoke({"question": "List three different scenarios of digital arrest?"})
print(response["answer"])
 Conclusion This guide provided a step-by-step approach to building a RAG system using LangGraph and LangChain. By integrating retrieval with AI-generated responses, we created a structured knowledge retrieval system that can process queries with improved accuracy. For further learning, explore the official documentation for LangChain and LangGraph. Engaging with research papers on Retrieval-Augmented Generation and participating in community forums will provide deeper insights into this evolving field. Author Recent Posts Ajitesh Kumar I have been recently working in the area of Data analytics including Data Science and Machine Learning / Deep Learning. I am also passionate about different technologies including programming languages such as Java/JEE, Javascript, Python, R, Julia, etc, and technologies such as Blockchain, mobile computing, cloud-native technologies, application security, cloud computing platforms, big data, etc. I would love to connect with you on Linkedin. Check out my latest book titled as First Principles Thinking: Building winning products using first principles thinking. Latest posts by Ajitesh Kumar (see all) Creating a RAG Application Using LangGraph: Example Code - February 1, 2025 Building a RAG Application with LangChain: Example Code - January 31, 2025 Building an OpenAI Chatbot with LangChain - January 30, 2025 Ajitesh Kumar I have been recently working in the area of Data analytics including Data Science and Machine Learning / Deep Learning. I am also passionate about different technologies including programming languages such as Java/JEE, Javascript, Python, R, Julia, etc, and technologies such as Blockchain, mobile computing, cloud-native technologies, application security, cloud computing platforms, big data, etc. I would love to connect with you on Linkedin. Check out my latest book titled as First Principles Thinking: Building winning products using first principles thinking. Posted in agentic ai, LangChain, Large Language Models, RAG. Tagged with agentic ai, langchain, LLMs, RAG. ← Building a RAG Application with LangChain: Example Code Search for: ChatGPT Prompts (250+) Generate Design Ideas for App Expand Feature Set of App Create a User Journey Map for App Generate Visual Design Ideas for App Generate a List of Competitors for App More... Recent Posts Creating a RAG Application Using LangGraph: Example Code Building a RAG Application with LangChain: Example Code Building an OpenAI Chatbot with LangChain How Indexing Works in LLM-Based RAG Applications Retrieval Augmented Generation (RAG) & LLM: Examples Data Science / AI Trends • Sentiment Analysis Real World Examples • Prepend any arxiv.org link with talk2 to load the paper into a responsive chat application • Custom LLM and AI Agents (RAG) On Structured + Unstructured Data - AI Brain For Your Organization • Guides, papers, lecture, notebooks and resources for prompt engineering • Common tricks to make LLMs efficient and stable More... Free Online Tools Create Scatter Plots Online for your Excel Data Histogram / Frequency Distribution Creation Tool Online Pie Chart Maker Tool Z-test vs T-test Decision Tool Independent samples t-test calculator More... Newsletter Name Email Tag Cloud ai (98) Angular (50) angularjs (104) api (16) Application Security (22) artificial intelligence (20) AWS (23) big data (41) blockchain (63) career planning (21) chatgpt (16) data (21) data analytics (36) datascience (33) Data Science (504) Deep Learning (60) docker (26) freshers (14) generative ai (45) google (14) hyperledger (18) Interview questions (79) Java (92) javascript (103) Kubernetes (19) LLMs (16) machine learning (500) mongodb (16) news (16) nlp (46) nosql (17) online courses (13) python (166) quantum computing (13) reactjs (15) r programming (13) sklearn (30) spring framework (16) statistics (85) testing (16) tools (12) tutorials (14) UI (13) Unit Testing (18) web (16) Recent Comments Justice on Occam’s Razor in Machine Learning: ExamplesMarch 21, 2024 I found it very helpful. However the differences are not too understandable for me AYUSH on Why & When to use Eigenvalues & Eigenvectors?February 20, 2024 Very Nice Explaination. Thankyiu very much, Muhammed Tmeizeh on Hyperledger Fabric – Are Channels Private Blockchain? (Deep Dive)February 16, 2024 in your case E respresent Member or Oraganization which include on e or more peers? Ajay Salve on ESG Concepts: Reports, Metrics & KPIsFebruary 10, 2024 Such a informative post. Keep it up Ashok Reddyboina on Sklearn LabelEncoder Example – Single & Multiple ColumnsFebruary 8, 2024 Thank you....for your support. you given a good solution for me. Welcome to Vitalflux.com - your hub for AI, Machine Learning, Data Science and Data Analytics topics. Learn through detailed, real-life examples in AI/ML and Data Management. Gain practical insights and apply them to real-world scenarios! Data Science Machine Learning Deep Learning Statistics Generative AI Courses Admissions Interview Questions Educational Presentations Privacy policy Contact us Analytics Yogi © 2025 Powered by WordPress. Design by WildWebLab

Source: https://blog.demir.io/advanced-rag-implementing-advanced-techniques-to-enhance-retrieval-augmented-generation-systems-0e07301e46f4
Advanced RAG: Implementing Advanced Techniques to Enhance Retrieval-Augmented Generation Systems | by Necati Demir | Medium Open in app Sign up Sign in Write Sign up Sign in Advanced RAG: Implementing Advanced Techniques to Enhance Retrieval-Augmented Generation Systems Necati Demir · Follow 16 min read · May 16, 2024 -- 2 Listen Share I think it is a well-known fact by now that RAG (Retrieval Augmented Generation) techniques are becoming de-facto tools during interactions with LLMs. In one of my recent articles, I have built and end to end RAG pipeline, step by step, with code examples. In this article, we will move this initiative one more step ahead and implement an advanced RAG pipeline. 1. Advanced Retrieval-Augmented Generation (RAG) Pipeline: An Overview A basic RAG workflow can be divided into three steps: indexing, retrieval, and generation. During the indexing phase, the text is converted into embeddings, which are then stored in a vector database to create a searchable index. In the retrieval step, the user’s query is also converted into an embedding. This embedding is used to search the vector database for the most relevant text data. Finally, in the generation step, the query is augmented with the relevant documents retrieved earlier, and a large language model uses this enhanced prompt to generate an answer to the user’s question. An advanced RAG pipeline is an enhanced version of this pipeline where new steps (sub-steps) are added. The following is a list of enhancement points that will be discussed in this article, but the overall list is not limited to these. Data Indexing Optimizations: Techniques such as using sliding windows for text chunking and effective metadata utilization to create a more searchable and organized index. Query Enhancement: Modifying or expanding the initial user query with synonyms or broader terms to improve the retrieval of relevant documents. Hybrid Search: Combining traditional keyword-based search with semantic search using embedding vectors to handle a variety of query complexities. Fine Tuning Embedding Model: Adjusting a pre-trained model to better understand specific domain nuances, enhancing the accuracy and relevance of retrieved documents. Response Summarization: Condensing retrieved text to provide concise and relevant summaries before final response generation. Re-ranking and Filtering: Adjusting the order of retrieved documents based on relevance and filtering out less pertinent results to refine the final output. The possible improvements are not limited to what I listed here. There is paper that surveys RAG techniques where you can find many more improvements that can be used to enhance the quality of a RAG pipeline. The following diagram shows the steps of an advanced RAG pipeline. In this article, I assume you have familiarity with a basic RAG pipeline and I will focus only on the enhancements. If you are not familiar, I would recommend you to read my previous article: Hands-On with RAG: Step-by-Step Guide to Integrating Retrieval Augmented Generation in LLMs. 2. Pre-Retrieval Optimizations for Advanced Retrieval-Augmented Generation (RAG) Pre-retrieval is the step that defines a) how the indexing is done and b) what happens to the user query before using it for retrieval. Below, I’ll discuss various strategies for pre-retrieval optimization, including data indexing and query enhancements, and will provide sample Python code samples. 2.1. Data Indexing Optimizations Before doing anything else, we have to store data so it can be queried later. That is called indexing. This includes setting the right chunk sizes, using metadata effectively, and choosing an embedding model. 2.1.1. Sliding Window for Text Chunking A simple way to index the text is just splitting the text into n part and then converting them to embedding vectors and then storing them in a vector database. The sliding window approach creates overlapping text chunks to ensure that no contextual information is lost at the boundaries of the chunks. The following code sample uses nltk library to split the text by sentences. import nltk from nltk.tokenize import sent_tokenize nltk.download('punkt')  # Ensure the punkt tokenizer is downloaded def sliding_window(text, window_size=3):     """     Generate text chunks using a sliding window approach.     Args:     text (str): The input text to chunk.     window_size (int): The number of sentences per chunk.     Returns:     list of str: A list of text chunks.     """     sentences = sent_tokenize(text)     return [' '.join(sentences[i:i+window_size]) for i in range(len(sentences) - window_size + 1)] # Example usage text = "This is the first sentence. Here comes the second sentence. And here is the third one. Finally, the fourth sentence." chunks = sliding_window(text, window_size=3) for chunk in chunks:     print(chunk)     print("-----")     # here, you can convert the chunk to embedding vector     # and, save it to a vector database 2.1.2. Metadata Utilization Metadata can include information such as document creation dates, authors, or relevant tags. This information can be used to filter or prioritize documents during retrieval, enhancing the search process. The following code samples create a vector database with faiss library and inserts vectors into it and search by metadata (tag). import numpy as np import faiss documents = [     "Document 1 content here",     "Content of the second document",     "The third one has different content", ] metadata = [     {"date": "20230101", "tag": "news"},     {"date": "20230102", "tag": "update"},     {"date": "20230103", "tag": "report"}, ] # Dummy function to generate embeddings def generate_embeddings(texts):     """Generate dummy embeddings for the sake of example."""     return np.random.rand(len(texts), 128).astype('float32')  # 128-dimensional embeddings # Generate embeddings for documents doc_embeddings = generate_embeddings(documents) # Create a FAISS index for the embeddings (using FlatL2 for simplicity) index = faiss.IndexFlatL2(128)  # 128 is the dimensionality of the vectors index.add(doc_embeddings)  # Add embeddings to the index # Example search function that uses metadata def search(query_embedding, metadata_key, metadata_value):     """Search the index for documents that match metadata criteria."""     k = 2  # Number of nearest neighbors to find     distances, indices = index.search(np.array([query_embedding]), k)  # Perform the search     results = []     for idx in indices[0]:         if metadata[idx][metadata_key] == metadata_value:             results.append((documents[idx], metadata[idx]))     return results # Generate a query embedding (in a real scenario, this would come from a similar process) query_embedding = generate_embeddings(["Query content here"])[0] # Search for documents tagged with 'update' matching_documents = search(query_embedding, 'tag', 'update') print(matching_documents) 2.2. Query Enhancement There will be cases where a user will not be able to articulate the question clearly. In those scenerioes, we can enhance the query by rewriting it completely or expanding it. We can utilize the LLM itself. We can just send the question to LLM and ask it to articulate it better. The following prompt will help for that. Given the prompt: '{prompt}', generate 3 question that are better articulated. Once we have the new queries, we can convert the new queries to embedding vectors and use them to search in the vector database. 3. Retrieval Techniques for Advanced Retrieval-Augmented Generation (RAG) Retrieval is the step in which the query is used to search the previously indexed database. Below, I’ll discuss various strategies for retrieval. 3.1. Hybrid Search Models So far, we have always discussed to search the query in a vector database — where we store embedding vectors. Let’s take this one step ahead and combine it with traditional keyword-based search. This approach ensures that the retrieval system can handle a variety of query types; from those requiring exact keyword matches to more complex queries needing understanding of context. Let’s build a hybrid search model. We will use Elasticsearch for traditional search mechanism and faiss as vector database for semantic search. You can find the full code here: https://github.com/ndemir/machine-learning-projects/tree/main/hybrid-search — which also includes a bonus Dockerfile to show how to run Elasticsearch. 3.1.1. Indexing with Elasticsearch Let’s start with assuming that all our documents are in a `documents` dictionary and we already get the embedding vectors and stored them in the dictionary. The following code block connects to Elasticsearch 8.13.4 and create an indexed for the given sample documents. ES_NODES = "http://localhost:9200" documents = [     {"id": 1, "text": "How to start with Python programming.", "vector": [0.1, 0.2, 0.3]},     {"id": 2, "text": "Advanced Python programming tips.", "vector": [0.1, 0.3, 0.4]},     # More documents... ] from elasticsearch import Elasticsearch es = Elasticsearch(     hosts=ES_NODES, ) for doc in documents:     es.index(index="documents", id=doc['id'], document={"text": doc['text']}) 3.1.2. Indexing with Faiss In this part, we are using faiss as vector database and indexing the vectors. import numpy as np import faiss dimension = 3  # Assuming 3D vectors for simplicity faiss_index = faiss.IndexFlatL2(dimension) vectors = np.array([doc['vector'] for doc in documents]) faiss_index.add(vectors) 3.1.3. Hybrid Search The following code block illustrates the hybrid search. This is a sample that you can use, or you can build something from scratch for your own use case. def hybrid_search(query_text, query_vector, alpha=0.5):     # Perform a keyword search using Elasticsearch on the "documents" index, matching the provided query_text.     response = es.search(index="documents", query={"match": {"text": query_text}})     # Extract the document IDs and their corresponding scores from the Elasticsearch response.     keyword_results = {hit['_id']: hit['_score'] for hit in response['hits']['hits']}     # Prepare the query vector for vector search: reshape and cast to float32 for compatibility with Faiss.     query_vector = np.array(query_vector).reshape(1, -1).astype('float32')     # Perform a vector search with Faiss, retrieving indices of the top 5 closest documents.     _, indices = faiss_index.search(query_vector, 5)     # Create a dictionary of vector results with scores inversely proportional to their rank (higher rank, higher score).     vector_results = {str(documents[idx]['id']): 1/(rank+1) for rank, idx in enumerate(indices[0])}     # Initialize a dictionary to hold combined scores from keyword and vector search results.     combined_scores = {}     # Iterate over the union of document IDs from both keyword and vector results.     for doc_id in set(keyword_results.keys()).union(vector_results.keys()):         # Calculate combined score for each document using the alpha parameter to balance the influence of both search results.         combined_scores[doc_id] = alpha * keyword_results.get(doc_id, 0) + (1 - alpha) * vector_results.get(doc_id, 0)     # Return the dictionary containing combined scores for all relevant documents.     return combined_scores # Example usage query_text = "Python programming" query_vector = [0.1, 0.25, 0.35] # Execute the hybrid search function with the specified query text and vector. results = hybrid_search(query_text, query_vector) # Print the results of the hybrid search to see the combined scores of documents. print(results) The hybrid_search function starts with a keyword search using Elasticsearch. In the next step; it performs a vector search using the Faiss and Faiss returns the indices of the top five closest documents and those indices are used to create scores documents inversely based on their rank (i.e., the closest document scores highest). And, once we have results from both Elasticsearch and Faiss, we combine scores from both methods. Each document's final score is a weighted average calculated using the alpha parameter, alpha=.5 gives equal weight to both results. 3.2. Fine Tuning Embedding Model Fine-tuning embedding models is an effective step to enhance the performance of Retrieval-Augmented Generation systems. Fine-tuning a pre-trained model helps the model to understand the nuances of a specific domain or dataset and as a consequence you can significantly improve the relevance and accuracy of retrieved documents. We can summarize the importance of fine-tuning models with the following key points: Improved Semantic Understanding: Fine-tuning helps the model grasp domain-specific terminology and concepts that might not be well-represented in the original training data. Adaptation to Evolving Content: The information in some fields (such as medical or technological domains) are rapidly changing, keeping the embeddings updated through fine-tuning can maintain the system’s effectiveness. Increased Retrieval Precision: By aligning the embedding space more closely with the target use case, fine-tuning ensures that semantically relevant texts are retrieved more reliably. In the next 3 sub-section, we will see the code samples to prepare data for fine-tuning, fine-tuning (training) the model and using the model. You can see the full code on my github repo: https://github.com/ndemir/machine-learning-projects/tree/main/fine-tuning-embedding-model 3.2.1. Preparing Data for Fine Tuning The following code block is the first step to fine-tune a model. It initializes a pipeline for fine-tuning a pre-trained masked language model, loads a model and tokenizer, adjusts for device compatibility (GPU or CPU). After initialization, it processes a sample dataset through tokenization and dynamic token masking. This setup prepares the model for self-supervised learning where it predicts masked tokens, enhancing its semantic understanding of the input data. # Define the model name using a pre-trained model from the Sentence Transformers library model_name = "sentence-transformers/all-MiniLM-L6-v2" # Load the tokenizer for the specified model from Hugging Face's transformers library tokenizer = AutoTokenizer.from_pretrained(model_name) # Load the model for masked language modeling based on the specified model model = AutoModelForMaskedLM.from_pretrained(model_name) # Determine if a GPU is available and set the device accordingly; use CPU if GPU is not available device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # Move the model to the appropriate device (GPU or CPU) model.to(device) # Define a generator function to create a dataset; this should be replaced with actual data loading logic def dataset_generator():     # Example dataset composed of individual sentences; replace with your actual dataset sentences     dataset = ["sentence1", "sentence2", "sentence3"]     # Yield each sentence as a dictionary with the key 'text'     for sentence in dataset:         yield {"text": sentence} # Create a dataset object using Hugging Face's Dataset class from the generator function dataset = Dataset.from_generator(dataset_generator) # Define a function to tokenize the text data def tokenize_function(example):     # Tokenize the input text and truncate it to the maximum length the model can handle     return tokenizer(example["text"], truncation=True) # Apply the tokenization function to all items in the dataset, batch processing them for efficiency tokenized_datasets = dataset.map(tokenize_function, batched=True) # Initialize a data collator for masked language modeling which randomly masks tokens # This is used for training the model in a self-supervised manner data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15) 3.2.2. Fine Tuning the Model Once the data is ready, we can start with fine-tuning phase. In this phase, we will use the existing weights of the model and start updating them. This following code block sets up and executes the training of the language model using Hugging Face’s Trainer API. It starts with defining the training parameters (epochs, batch size, and learning rate, etc ). Trainer object then uses these settings, along with a pre-loaded model, a tokenized dataset, and a data collator for masked language modeling (the model, tokenized dataset and the data collator was created in previous step). When the training is completed, the new updated model and its tokenizer are saved to be used in the next step. # Define training arguments to configure the training session training_args = TrainingArguments(     output_dir="output",  # Directory where the outputs (like checkpoints) will be saved     num_train_epochs=3,  # Total number of training epochs to perform     per_device_train_batch_size=16,  # Batch size per device during training     learning_rate=2e-5,  # Learning rate for the optimizer ) # Initialize the Trainer, which handles the training loop and evaluation trainer = Trainer(     model=model,  # The model to be trained, already loaded and configured     args=training_args,  # The training arguments defining the training setup     train_dataset=tokenized_datasets,  # The dataset to train on, already tokenized and prepared     data_collator=data_collator,  # The data collator that handles input formatting and masking ) # Start the training process trainer.train() # Define the paths where the fine-tuned model and tokenizer will be saved model_path = "./model" tokenizer_path = "./tokenizer" # Save the fine-tuned model to the specified path model.save_pretrained(model_path) # Save the tokenizer used in training to the specified path tokenizer.save_pretrained(tokenizer_path) 3.2.3. Using the Fine Tuned Model It is now time to use the saved model and tokenizer to generate embedding vectors. The following code block is used for that purpose. This following code block loads the model and the tokenizer to generate embeddings for given sentences. First, the model and tokenizder are loaded from the saved path and loaded to GPU or CPU. The sentences (in the context of this article, they are queries) are tokenized. The model processes these inputs without updating its parameters, this is called the inference mode and this is done after under with torch.no_grad() block. We are not using this model to predict the next token; instead, we aim to extract embedding vectors from the hidden states of the model. And as a final step, these embedding vectors are moved back to the CPU. # Load the tokenizer and model from saved paths, ensuring the model is allocated to the appropriate device (GPU or CPU) tokenizer = AutoTokenizer.from_pretrained(tokenizer_path) model = AutoModelForMaskedLM.from_pretrained(model_path).to(device) # Define a function to tokenize input sentences, configuring padding and truncation to handle variable sentence lengths def tokenize_function_embedding(example):     return tokenizer(example["text"], padding=True, truncation=True) # List of example sentences to generate embeddings for sentences = ["This is the first sentence.", "This is the second sentence."] # Create a Dataset object directly from these sentences dataset_embedding = Dataset.from_dict({"text": sentences}) # Apply the tokenization function to the dataset, preparing it for embedding generation tokenized_dataset_embedding = dataset_embedding.map(tokenize_function_embedding, batched=True, batch_size=None) # Extract 'input_ids' and 'attention_mask' needed for the model to understand which parts of the input are padding and which are actual content input_ids = tokenized_dataset_embedding["input_ids"] attention_mask = tokenized_dataset_embedding["attention_mask"] # Convert these lists into tensors and ensure they are on the correct device (GPU or CPU) for processing input_ids = torch.tensor(input_ids).to(device) attention_mask = torch.tensor(attention_mask).to(device) # Generate embeddings using the model without updating gradients to save computational resources with torch.no_grad():     outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)     # Extract the last layer's hidden states as embeddings, specifically the first token (typically used in BERT-type models for representing sentence embeddings)     embeddings = outputs.hidden_states[-1][:, 0, :] # Move the embeddings from the GPU back to CPU for easy manipulation or saving embeddings = embeddings.cpu() # Print each sentence with its corresponding embedding vector for sentence, embedding in zip(sentences, embeddings):     print(f"Sentence: {sentence}")     print(f"Embedding: {embedding}\n") 4. Post-Retrieval Techniques for Advanced Retrieval-Augmented Generation (RAG) The magic doesn’t stop once the relevant information is retrieved; possible improvements come in the next step: post-retrieval. The user not only needs accurate results but also digestible results given in correct order. In the next 2 sub-sections, we will explain how to use summarization and re-ranking to improve the quality of RAG. 4.1. Response Summarization This step may be necessary if you stored large of chunk texts’ vectors in database during the indexing process. If the texts are already small, you may not need this step. The following code block can be used for the summarization process. The following code block uses the transformers library to summarize text with a pre-trained BART model. The function summarize_text receives text and uses the model to produce a concise summary based on defined maximum and minimum length parameters. from transformers import pipeline def summarize_text(text, max_length=130):       # Load a pre-trained summarization model from Hugging Face's model hub.     # 'facebook/bart-large-cnn' is chosen for its proficiency in generating concise summaries.     summarizer = pipeline("summarization", model="facebook/bart-large-cnn")       # The summarizer uses the BART model to condense the input text into a summary.     # 'max_length' specifies the maximum length of the summary output.     # 'min_length' sets the minimum length to ensure the summary is not too terse.     # 'do_sample' is set to False to use a deterministic approach for summary generation.     summary = summarizer(text, max_length=max_length, min_length=30, do_sample=False)         # The output from the summarizer is a list of dictionaries.     # We extract the summary text from the first dictionary in the list.     return summary[0]['summary_text'] # Example text to be summarized. # This text discusses the importance of summarization in retrieval-augmented generation systems. long_text = "Summarization are vital steps in the workflow of retrieval-augmented generation systems. They ensure the output is not only accurate but also concise and digestible. These techniques are essential, especially in domains where the accuracy and precision of information are crucial." # Call the summarize_text function to compress the example text. summarized_text = summarize_text(long_text) # Print the summarized text to see the output of the summarization model. print("Summarized Text:", summarized_text) 4.2. Re-ranking and Filtering During the retrieval process, you should have received the “score” of each documents — which is actually the similarity scores of the vectors to the the vector of the query. This information can be used to re-rank the documents and filter the results based on a given threshold . The following code block shows a sample of how to re-rank and filter. 4.2.1. Basic Re-Ranking and Filtering The code block provided defines a list of documents, each represented by a dictionary containing an ID, text, and a relevance score. It then implements two main functions: re_rank_documents and filter_documents. The re_rank_documents function sorts the documents by their relevance scores in descending order and after re-ranking, the filter_documents function is applied to exclude any documents with relevance scores below a specified threshold of 0.75. # Define a list of documents. Each document is represented as a dictionary with an ID, text, and a relevance score. documents = [     {"id": 1, "text": "Advanced RAG systems use sophisticated techniques for text summarization.", "relevance_score": 0.82},     {"id": 2, "text": "Basic RAG systems primarily focus on retrieval and basic processing.", "relevance_score": 0.55},     {"id": 3, "text": "Re-ranking improves the quality of responses by ordering documents by relevance.", "relevance_score": 0.89} ] # Define a function to re-rank documents based on their relevance scores. def re_rank_documents(docs):     # Use the sorted function to order the documents by 'relevance_score'.     # The key for sorting is specified using a lambda function, which extracts the relevance score from each document.     # 'reverse=True' sorts the list in descending order, placing documents with higher relevance scores first.     return sorted(docs, key=lambda x: x['relevance_score'], reverse=True) # Re-rank the documents using the defined function and print the result. ranked_documents = re_rank_documents(documents) print("Re-ranked Documents:", ranked_documents) # Define a function to filter documents based on a relevance score threshold. def filter_documents(docs, relevance_threshold=0.75):       # Use a list comprehension to create a new list that includes only those documents whose 'relevance_score'     # is greater than or equal to the 'relevance_threshold'.     return [doc for doc in docs if doc['relevance_score'] >= relevance_threshold] # Filter the re-ranked documents using the defined function with a threshold of 0.75 and print the result. filtered_documents = filter_documents(ranked_documents) print("Filtered Documents:", filtered_documents) 4.2.2. Advanced Re-Ranking with Machine Learning For a more sophisticated approach, machine learning models can be used to re-rank documents. In this approach, the challenge is; how to know which documents are relevant so we can train the machine learning model to rank the documents? In this approach, we need to assume that we have a system that stores the interaction between the user and the system and stores if a document is relevant for a given query. Once we have this dataset, we can use query embedding vector and the document embedding to predict the score. # assumung the data is stored in the following format in a database # query_text | response_text | user_clicked query_embeddings = get_embedding_vector(database.query_text) response_embeddings = get_embedding_vector(database.response_text) # create the dataset X = concat(query_embeddings, response_embeddings) y = database.user_clicked model = model.train(X, y) model.predict_proba(...) The pseudo-code provided above outlines the approach for using machine learning to re-rank documents based on relevance, specifically by predicting how likely a user is to find a document relevant based on past interactions. Here’s a step-by-step explanation of the process described in the pseudo-code: Generating Embeddings: For both the query and the response documents, embedding vectors are created to capture their semantic content. Creating the Dataset: These embeddings are concatenated to form feature vectors (X), with the target variable (y) indicating whether a user clicked on the document. Model Training: A classification model is trained on this dataset to predict the likelihood of a document being clicked based on the combined query and document embeddings. Prediction: The trained model can then predict the probability of clicks for new query-document pairs, helping to re-rank documents based on predicted relevance to enhance search result accuracy. 5. Conclusion Implementing a simple Retrieval-Augmented Generation (RAG) system may solve your problem, but adding enhancements will improve your results and help your system produce more precise answers. In this article, we discussed several enhancements aimed at achieving this goal, including Data Indexing Optimizations, Query Enhancement, Hybrid Search, Fine Tuning of Embedding Models, Response Summarization, and Re-ranking and Filtering. By integrating these enhancements, you have the opportunity to significantly improve performance. Continue to explore and apply these methods, experimenting to see what suits your needs best. Llm Retrieval Augmented Gen Machine Learning Deep Learning -- -- 2 Follow Written by Necati Demir 297 Followers ·418 Following PhD | ML | AI | DataScience | DevOps Follow Responses (2) See all responses Help Status About Careers Press Blog Privacy Terms Text to speech Teams

Source: https://medium.com/@sametarda.dev/deep-dive-into-corrective-rag-implementations-and-workflows-111c0c10b6cf
Deep Dive into Corrective RAG: Implementations and Workflows | by Samet Arda Erdogan | Medium Open in app Sign up Sign in Write Sign up Sign in Deep Dive into Corrective RAG: Implementations and Workflows Samet Arda Erdogan · Follow 11 min read · Oct 4, 2024 -- Listen Share In the fast-changing world of artificial intelligence, Retrieval-Augmented Generation (RAG) is an exciting method that combines the knowledge in language models with accurate information from external sources. In this blog post, we’ll explore Corrective RAG, breaking down how it works with easy explanations and relatable examples. Whether you’re an experienced developer or just curious about AI, this guide is here to make the concept clear and easy to understand. Table of Contents Understanding Retrieval-Augmented Generation (RAG) Understanding Corrective Retrieval Augmented Generation (CRAG) Breaking Down the Code Analogies to Simplify Corrective RAG Conclusion Understanding Retrieval-Augmented Generation (RAG) Retrieval-Augmented Generation (RAG) is a method that mixes the strengths of language models with a system that finds information. Instead of just using the knowledge built into the model, RAG looks for relevant documents or data from outside sources to give more accurate and relevant answers. Think about trying to answer a specific question without having the latest information available. A regular language model might have a hard time or give outdated answers. But RAG works like a careful researcher, gathering the most important information first before creating a response. Understanding Corrective Retrieval Augmented Generation (CRAG) Corrective Retrieval Augmented Generation (CRAG) stands out as a crucial method to improve the accuracy of large language models (LLMs). This section delves deeper into what CRAG is, how it enhances the standard RAG approach, its benefits, and practical applications. What is Corrective Retrieval Augmented Generation (CRAG) ? from paper (https://arxiv.org/abs/2401.15884) CRAG is a sophisticated technique designed to make LLMs more accurate by tackling one of their biggest problems: hallucinations. Hallucinations happen when models generate wrong or misleading information. CRAG addresses this by focusing on the following key components: Key Features Reducing Hallucinations: CRAG stops LLMs from producing false information by using a strong evaluation system. Retrieval Evaluator: This lightweight tool checks the quality of the documents retrieved in response to a user’s question. It gives a confidence score that helps decide the next steps. By removing incorrect or irrelevant documents, CRAG ensures only accurate information is used. How CRAG Works Input and Output: When given a question (X) and a set of retrieved documents (D), the Retrieval Evaluator assigns a relevance score to each document. Relevance Score: This score shows how related the document is to the question. Confidence Level: Based on these scores, documents are labeled as Correct, Incorrect, or Ambiguous, which determines the next actions in processing the information. The CRAG Workflow Web Searches: CRAG uses extensive web searches to add to static document sources, improving information retrieval. Knowledge Refinement: An algorithm picks and filters important information from the retrieved documents, removing irrelevant parts. Decomposition: Each document is broken down into smaller, easy-to-manage pieces for individual evaluation. Filtering: These pieces are scored for relevance, and irrelevant ones are discarded. Recombination: The relevant pieces are combined to form a clear and accurate answer. Example of the Process For the question “Who is the screenwriter for Death of a Batman?”, CRAG might retrieve several documents. The process would involve: Decomposition: Breaking down the documents into specific facts. Filtering: Removing unrelated details and keeping only mentions of Hamm. Recompose: Combining the retained information into the final answer. How CRAG is Different from Standard RAG CRAG improves upon the traditional Retrieval Augmented Generation (RAG) method in several ways: Error Correction: CRAG uses an evaluation tool to fix low-quality documents. Web Search Integration: It adds extensive online searches to static data for more thorough information. Information Refinement: CRAG carefully processes documents to keep only essential details. Flexibility and Compatibility: CRAG can easily work with different RAG-based methods, boosting performance across various tasks. Pros and Cons of CRAG Advantages Enhanced Accuracy: Removes incorrect information for more reliable results. Flexibility: Easily works with various methods for different applications. Web Search Integration: Expands information access beyond fixed sources. Disadvantages Computational Cost: Additional evaluations and web searches need more resources. Potential Bias: The quality of information from the web can vary, which might introduce misinformation. Use Cases for CRAG Medical: Ensuring accuracy in medical diagnosis systems. Information-Intensive Tasks: Creating reliable texts for medical or scientific reports. Customer Service: Improving chatbot responses with accurate, up-to-date information. Education: Developing educational materials that provide precise information. News Generation: Producing dependable news reports, reducing the risk of misinformation. CRAG represents a significant advancement in making AI-generated content more reliable and accurate. By addressing the issue of hallucinations and integrating robust evaluation systems, CRAG enhances the performance of large language models across various fields. Breaking Down the Code Let’s delve into the provided code to understand how Corrective RAG is implemented. We’ll explore each component, breaking it down with clear explanations and analogies. Setting Up the Environment & Imports import os import uuid from typing import List from typing_extensions import TypedDict from IPython.display import Image, display from langchain_google_genai import ChatGoogleGenerativeAI from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_community.document_loaders import WebBaseLoader from langchain_community.vectorstores import SKLearnVectorStore from langchain_huggingface import HuggingFaceEmbeddings from langchain.prompts import PromptTemplate from langchain_core.output_parsers import JsonOutputParser, StrOutputParser from langchain.schema import Document from langgraph.graph import START, END, StateGraph from langchain_community.tools.tavily_search import TavilySearchResults def set_environment_variables():     print("Please enter your API keys:")         keys = [         "GOOGLE_API_KEY",         "TAVILY_API_KEY",         "LANGCHAIN_API_KEY"     ]         for key in keys:         value = input(f"Enter your {key}: ")         os.environ[key] = value         # Set other non-sensitive environment variables     os.environ["LANGCHAIN_TRACING_V2"] = "true"     os.environ["LANGSMITH_ENDPOINT"] = "https://api.smith.langchain.com"     os.environ["LANGCHAIN_PROJECT"] = "default"         print("Environment variables have been set.") Before diving into the core functionalities, it’s essential to set up the environment. This function prompts the user to input necessary API keys, storing them securely as environment variables. Additionally, it sets non-sensitive variables that configure the behavior of the LangChain framework, a pivotal tool in our RAG implementation. Initializing the Language Model def initialize_llm():     # we are using gemini model. You can use different models.     gemini_ai_model = 'gemini-1.5-flash'     llm = ChatGoogleGenerativeAI(model=gemini_ai_model, temperature=0)     metadata = f"CRAG, {gemini_ai_model}"     return llm, metadata This function initializes the language model using Google’s Generative AI, specifically the gemini-1.5-flash model. The temperature parameter is set to 0, ensuring deterministic outputs, which is crucial for reliability in corrective scenarios. Loading and Preparing Documents def load_documents(urls):     docs = [WebBaseLoader(url).load() for url in urls]     return [item for sublist in docs for item in sublist] The function takes a list of URLs, loads the content from each using WebBaseLoader, and aggregates all documents into a single list. These documents serve as the external knowledge base that the RAG system will reference. Vector Store Initialization def initialize_vectorstore(docs_list):     text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(         chunk_size=250, chunk_overlap=0     )     doc_splits = text_splitter.split_documents(docs_list)         model_name = "sentence-transformers/all-mpnet-base-v2"     model_kwargs = {'device': 'cpu'}     encode_kwargs = {'normalize_embeddings': False}     hf_embeddings = HuggingFaceEmbeddings(         model_name=model_name,         model_kwargs=model_kwargs,         encode_kwargs=encode_kwargs     )         vectorstore = SKLearnVectorStore.from_documents(         documents=doc_splits,         embedding=hf_embeddings,     )     return vectorstore.as_retriever(k=4) This function preprocesses the loaded documents by splitting them into manageable chunks using RecursiveCharacterTextSplitter. It then generates embeddings for these chunks using Hugging Face's sentence-transformers/all-mpnet-base-v2 model. These embeddings are stored in a vector store (SKLearnVectorStore), enabling efficient retrieval of relevant documents based on semantic similarity. The as_retriever(k=4) method configures the system to fetch the top 4 most relevant documents during retrieval. Grading Document Relevance class RetrievalGrader:     def __init__(self, llm):         self.llm = llm         self.prompt = PromptTemplate(             template="""You are a teacher grading a quiz. You will be given:             1/ a QUESTION             2/ A FACT provided by the student                         You are grading RELEVANCE RECALL:             A score of 1 means that ANY of the statements in the FACT are relevant to the QUESTION.             A score of 0 means that NONE of the statements in the FACT are relevant to the QUESTION.             1 is the highest (best) score. 0 is the lowest score you can give.                         Explain your reasoning in a step-by-step manner. Ensure your reasoning and conclusion are correct.                         Avoid simply stating the correct answer at the outset.                         Question: {question} \n             Fact: \n\n {documents} \n\n                         Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \n             Provide the binary score as a JSON with a single key 'score' and no premable or explanation.             """,             input_variables=["question", "documents"],         )         self.chain = self.prompt | self.llm | JsonOutputParser()         def grade(self, question, documents):         return self.chain.invoke({"question": question, "documents": documents}) The RetrievalGrader class employs the language model to assess the relevance of retrieved documents concerning the user's query. It uses a meticulously crafted prompt that instructs the model to act as a teacher grading the relevance of each fact presented. The output is a binary score (yes or no) encapsulated in JSON, indicating the document's pertinence. Building the RAG Workflow class RAGChain:     def __init__(self, llm):         self.llm = llm         self.prompt = PromptTemplate(             template="""You are an assistant for question-answering tasks.                         Use the following documents to answer the question.                         If you don't know the answer, just say that you don't know.                         Use three sentences maximum and keep the answer concise:             Question: {question}             Documents: {documents}             Answer:             """,             input_variables=["question", "documents"],         )         self.chain = self.prompt | self.llm | StrOutputParser()         def generate(self, question, documents):         return self.chain.invoke({"documents": documents, "question": question}) The RAGChain class is responsible for generating the final answer to the user's query. It uses the curated and relevant documents to craft a concise response. The prompt guides the language model to answer succinctly, ensuring clarity and brevity. Create Workflow Nodes class GraphState(TypedDict):     """     Represents the state of our graph.     Attributes:         question: question         generation: LLM generation         search: whether to add search         documents: list of documents     """     question: str     generation: str     search: str     documents: List[str]     steps: List[str] def retrieve(state):     """     Retrieve documents     Args:         state (dict): The current graph state     Returns:         state (dict): New key added to state, documents, that contains retrieved documents     """     question = state["question"]     documents = retriever.invoke(question)     steps = state["steps"]     steps.append("retrieve_documents")     return {"documents": documents, "question": question, "steps": steps} def generate(state):     """     Generate answer     Args:         state (dict): The current graph state     Returns:         state (dict): New key added to state, generation, that contains LLM generation     """     question = state["question"]     documents = state["documents"]     generation = rag_chain.invoke({"documents": documents, "question": question})     steps = state["steps"]     steps.append("generate_answer")     return {         "documents": documents,         "question": question,         "generation": generation,         "steps": steps,     } def grade_documents(state):     """     Determines whether the retrieved documents are relevant to the question.     Args:         state (dict): The current graph state     Returns:         state (dict): Updates documents key with only filtered relevant documents     """     question = state["question"]     documents = state["documents"]     steps = state["steps"]     steps.append("grade_document_retrieval")     filtered_docs = []     search = "No"     for d in documents:         score = retrieval_grader.invoke(             {"question": question, "documents": d.page_content}         )         grade = score["score"]         if grade == "yes":             filtered_docs.append(d)         else:             search = "Yes"             continue     return {         "documents": filtered_docs,         "question": question,         "search": search,         "steps": steps,     } def web_search(state):     """     Web search based on the re-phrased question.     Args:         state (dict): The current graph state     Returns:         state (dict): Updates documents key with appended web results     """     question = state["question"]     documents = state.get("documents", [])     steps = state["steps"]     steps.append("web_search")     web_results = web_search_tool.invoke({"query": question})     documents.extend(         [             Document(page_content=d["content"], metadata={"url": d["url"]})             for d in web_results         ]     )     return {"documents": documents, "question": question, "steps": steps} def decide_to_generate(state):     """     Determines whether to generate an answer, or re-generate a question.     Args:         state (dict): The current graph state     Returns:         str: Binary decision for next node to call     """     search = state["search"]     if search == "Yes":         return "search"     else:         return "generate" Executing the Workflow def setup_workflow():     workflow = StateGraph(GraphState)         # Define the nodes     workflow.add_node("retrieve", retrieve)     workflow.add_node("grade_documents", grade_documents)     workflow.add_node("generate", generate)     workflow.add_node("web_search", web_search)         # Build graph     workflow.add_edge(START, "retrieve")     workflow.add_edge("retrieve", "grade_documents")     workflow.add_conditional_edges(         "grade_documents",         decide_to_generate,         {             "search": "web_search",             "generate": "generate",         },     )     workflow.add_edge("web_search", "generate")     workflow.add_edge("generate", END)         return workflow.compile() This function constructs the workflow graph that orchestrates the retrieval, grading, and generation processes. Using StateGraph, it defines the sequence and conditional transitions between different states: Retrieve: Fetch relevant documents based on the query. Grade Documents: Assess the relevance of each document. Decide to Generate or Search: Depending on the grading, decide whether to proceed to answer generation or conduct a web search for additional information. Generate: Craft the final answer using the graded documents. Executing the Workflow if __name__ == "__main__":     set_environment_variables()     llm, metadata = initialize_llm()         urls = [         "https://lilianweng.github.io/posts/2023-06-23-agent/",         "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",         "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",     ]     docs_list = load_documents(urls)     retriever = initialize_vectorstore(docs_list)         retrieval_grader = RetrievalGrader(llm)     rag_chain = RAGChain(llm)         web_search_tool = TavilySearchResults(k=3)         custom_graph = setup_workflow()         # Optionally display the graph     # display(Image(custom_graph.get_graph(xray=True).draw_mermaid_png()))         example = {"input": "What are the types of agent memory?"}     response = predict_custom_agent_local_answer(example, custom_graph)     print(response) This is the entry point of the script. It sequentially: Sets up environment variables. Initializes the language model. Loads documents from specified URLs. Prepares the vector store for retrieval. Instantiates the RetrievalGrader and RAGChain. Sets up the web search tool (TavilySearchResults). Compiles the workflow graph. Executes the workflow with a sample query: “What are the types of agent memory?” You can check the full code in this GitHub repository: https://github.com/ardasamett/RAG-Notes/blob/main/CorrectiveRag/CRAG.py Analogies to Simplify Corrective RAG Imagine you go to a library and ask, “Who is the screenwriter of Batman?” Input Query: You’re asking the librarian for information. (Example: “Who is the screenwriter of Batman?”) Retrieval Evaluator: The librarian fetches a few books and articles for you. They check to see if the information they brought is correct. (Example: “This book says Hamm is the screenwriter.”) Action Trigger: The librarian makes a decision based on whether the information is correct or not. (If it’s correct, they say, “Okay, Hamm is correct.” If it’s wrong, they say, “That’s not right; I need to check again.” If it’s ambiguous, they might say, “I need to do a bit more research.”) Knowledge Refinement: If the information is correct, the librarian notes down only the important part. (Example: They write down “Hamm.”) Web Search: If the information is wrong, the librarian looks online for new information. (Example: They search online, “Who is the screenwriter of Batman?”) Knowledge Selection: The librarian selects the most accurate information from both the sources they have and what they found online. (Example: “Hamm” is confirmed and also found online.) Generation: Finally, the librarian provides you with the final answer. (Example: “The screenwriter of Batman is Hamm.”) Conclusion In this exploration of Corrective Retrieval-Augmented Generation (CRAG), we’ve uncovered a transformative approach to enhancing the accuracy and reliability of language models. By integrating robust retrieval mechanisms, relevance grading, and web searches, CRAG effectively mitigates the notorious issue of hallucinations, paving the way for more dependable AI interactions. Through detailed code breakdowns and relatable analogies, we’ve illustrated how CRAG works in practice — from setting up the environment to executing complex workflows. This not only provides clarity on the underlying mechanics but also demonstrates the practical implications of CRAG across various fields, such as healthcare, education, and customer service. As the demand for accurate and contextually relevant AI responses grows, CRAG stands out as a powerful tool for developers and organizations aiming to leverage AI responsibly. By refining how AI interacts with external data sources, we can foster trust and enhance user experiences, making AI not just a source of information but a reliable partner in problem-solving. In a world where misinformation is rampant, CRAG represents a significant step forward in creating AI systems that prioritize accuracy, relevance, and user satisfaction. Whether you’re a developer looking to implement this technology or a curious reader interested in the future of AI, the insights shared here can serve as a foundation for understanding and utilizing Corrective RAG in practical applications. Feel free to explore and experiment with the code to gain a deeper understanding of Corrective RAG and its applications! You can also follow me for more insights and updates, and don’t hesitate to reach out if you have any questions or want to discuss further! Retrieval Augmented Retrieval Augmented Gen AI Rag Corrective Rag -- -- Follow Written by Samet Arda Erdogan 32 Followers ·8 Following https://www.linkedin.com/in/ardasamet/ Follow No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams

Source: https://www.chitika.com/rag-for-code-generation/
RAG for Code Generation: Automate Coding with AI & LLMs Home Resources GraphRAG Podcasts About Us Contact Sign in Subscribe RAG Applications How To Use RAG for Code Generation Learn how to use Retrieval-Augmented Generation (RAG) for code generation by integrating structured knowledge retrieval with AI models. Improve code accuracy, automate development tasks, enhance debugging, and generate context-aware code snippets efficiently. Arooj 03 Feb 2025 • 17 min read The very tools designed to make coding easier—frameworks, libraries, and templates—are now the bottleneck. Developers spend hours sifting through documentation, piecing together boilerplate code, and debugging repetitive patterns. It’s ironic, isn’t it? The tools meant to save time often end up consuming it. Retrieval-Augmented Generation (RAG) flips this script entirely. By combining the precision of information retrieval with the creativity of generative AI, RAG doesn’t just automate code generation—it tailors it to your project’s unique context. This isn’t about generic snippets; it’s about creating code that feels like it was written by someone who knows your system inside out. But here’s the real question: can RAG truly replace the manual grind of coding without sacrificing quality or control? And if so, what does that mean for the future of software development? Let’s explore. Image source: reliasoftware.com What is Retrieval-Augmented Generation (RAG)? RAG is a powerful AI approach that goes beyond pre-trained knowledge by actively retrieving real-time data from external sources. It’s like combining a researcher and a writer—one gathers facts, the other crafts responses—ensuring accuracy and context. RAG shines where static models struggle, like legal tech (pulling case precedents for arguments) or e-commerce (fetching product specs for recommendations). However, its effectiveness depends on the quality of retrieval, making optimization crucial. When done right, RAG bridges the gap between static AI and dynamic, domain-specific intelligence. The Evolution of Code Generation Code generation has come a long way, but the real breakthrough lies in how RAG integrates contextual awareness. Unlike traditional models that spit out generic snippets, RAG tailors its output by retrieving project-specific data—like internal libraries or team coding standards. This isn’t just automation; it’s collaboration between AI and developer intent. Take debugging, for instance. Instead of manually sifting through error logs, RAG pulls relevant fixes from your issue tracker or even Stack Overflow. It doesn’t just save time—it reduces cognitive load, letting developers focus on solving problems rather than finding them. RAG also democratizes coding. By turning natural language prompts into functional code, it empowers non-developers to prototype ideas. The result? Faster innovation cycles and fewer bottlenecks. Moving forward, the challenge will be balancing this accessibility with maintaining code quality and security—a frontier worth exploring. Image source: youtube.com Benefits of Using RAG in Code Generation RAG helps reduce technical debt by ensuring code consistency across projects. By retrieving reusable components and design patterns, it minimizes redundant or poorly structured code, making systems more scalable and maintainable. In large teams, RAG aligns outputs with internal coding standards by referencing style guides or past commits, reducing code review efforts and improving collaboration—like having a senior developer available 24/7. Beyond developers, RAG also empowers cross-functional teams. A product manager, for instance, can generate prototype code that follows best practices, speeding up development without compromising quality. To maximize its benefits, organizations need well-curated knowledge bases. Understanding the Core Concepts Retrieval-Augmented Generation (RAG) marries two forces: retrieval and generation. Imagine a chef who not only remembers recipes but also picks the freshest ingredients to create a dish just for you. This blend ensures outputs are relevant and rich in context. However, RAG’s success hinges on a strong knowledge base—a poor one can lead to misguided results, much like using expired ingredients. When paired with trusted sources like GitHub repositories or internal docs, RAG becomes a powerful tool for producing clean, functional code. The Mechanics of Retrieval Systems The retriever in RAG is like a skilled librarian who finds the exact page you need rather than just any book. Using techniques like vector embeddings, it maps queries and documents into a shared space for precise matching. In environments with domain-specific data, such as proprietary APIs, this method can save significant debugging time. Regular updates and a feedback loop are key to keeping the retriever sharp, ensuring it anticipates user needs accurately. Language Models in Code Generation Language models in code generation need solid context to excel. By fine-tuning these models on domain-specific data, they can grasp the nuances of internal coding standards or niche frameworks. For example, a fintech firm fine-tuned its model on proprietary APIs, leading to code suggestions that met internal security protocols while reducing manual reviews. Enhancing outputs further, detailed prompts act like a well-drawn map, guiding the model with clarity. Integrating Retrieval with Generation A retrieval-first workflow grounds the generative process in real-time, domain-specific data, reducing hallucinations and aligning outputs with current, relevant info. Take a healthcare chatbot that fetches up-to-date clinical guidelines before offering advice—this builds user trust and ensures accuracy. The success of this hybrid approach depends on well-tuned vector embeddings and ongoing fine-tuning, creating a system that evolves with changing data while maintaining top performance. Setting Up a RAG System for Code Generation Think of setting up a RAG system like building a custom toolbox—it’s all about having the right tools for the job. Start with a well-curated knowledge base. This isn’t just about dumping documentation into a database; it’s about structuring it for fast, accurate retrieval. For example, a fintech company might index API docs, compliance rules, and past code snippets to ensure the system generates secure, functional code. Next, focus on the retrieval mechanism. Tools like FAISS or Elasticsearch can help, but the real game-changer is embedding quality. Poor embeddings? Your system retrieves junk. Fine-tune embeddings using domain-specific data to ensure precision. Here’s the twist: don’t overlook feedback loops. Developers can flag irrelevant suggestions, which the system uses to refine future outputs. It’s like teaching a junior developer—iterative and rewarding. Finally, integrate lightweight prompt engineering. Clear, structured prompts guide the generative model, ensuring outputs align with your coding standards. Image source: together.ai 1. Required Tools and Libraries When it comes to tools, vector databases like FAISS or Pinecone are non-negotiable. Why? They make retrieval lightning-fast by organizing your data into embeddings that the system can search efficiently. But the quality of these embeddings depends heavily on your choice of embedding models. OpenAI’s Ada or Cohere’s embeddings are great starting points, but fine-tuning them on your domain-specific data can take your RAG system from “meh” to “wow.” For libraries, LangChain is a standout. It simplifies the integration of retrieval and generation workflows, letting you focus on building rather than debugging. Pair it with Hugging Face Transformers to leverage pre-trained models for tasks like code generation or documentation. Pro tip: Don’t ignore monitoring tools. Libraries like Prometheus or Grafana can track retrieval accuracy and latency, helping you spot bottlenecks early. Think of them as your system’s health dashboard—essential for scaling effectively. 2. Preparing the Knowledge Base Your knowledge base isn’t just a repository—it’s the backbone of your RAG system. Start by curating high-quality, domain-specific data. This means pulling from internal documentation, codebases, and even issue trackers. But don’t stop there. Include external sources like GitHub repositories or Stack Overflow threads to add depth and diversity. Now, let’s talk structure. Use embedding models to convert your data into searchable vectors. But here’s a twist: prioritize metadata tagging. For example, tagging code snippets with programming languages, frameworks, or even project phases can drastically improve retrieval accuracy. Think of it as giving your system a map instead of a maze. One overlooked factor? Data freshness. Outdated knowledge leads to irrelevant results. Automate updates with tools like DVC (Data Version Control) to keep your base current. Trust me, a well-maintained knowledge base doesn’t just support your RAG system—it supercharges it. 3. Configuring the Retrieval Component Your retrieval component isn’t just about speed—it’s about precision. Start by using vector embeddings tailored to your domain. For example, if you’re working with Python code, opt for embeddings fine-tuned on programming languages. This ensures your system retrieves relevant snippets, not random noise. But here’s where it gets interesting: multi-vector indexing. Instead of relying on a single embedding, use multiple embeddings for different data types—like code, documentation, and error logs. This layered approach boosts accuracy by matching queries to the right context. Think of it as having a specialist for every task. Now, let’s address a hidden factor: query optimization. Use techniques like query expansion to include synonyms or related terms. For instance, a search for “bug” could also retrieve results tagged with “error” or “issue.” The result? A retrieval system that feels intuitive and delivers spot-on results every time. 4. Connecting Retrieval to the Generation Model The magic happens when retrieval and generation work in sync. The key is embedding alignment—ensuring the vector space used for retrieval matches the one the generation model understands. Without this, your generative model might misinterpret retrieved data, leading to irrelevant or incoherent outputs. Now, let’s talk context packaging. Instead of dumping raw retrieved data into the model, structure it. For example, prepend retrieved code snippets with metadata like file paths or function names. This gives the model a richer context to generate more accurate and actionable outputs. Here’s a lesser-known trick: dynamic weighting. Not all retrieved data is equally relevant. Use scoring mechanisms to prioritize high-confidence results. For instance, in debugging, prioritize logs with recent timestamps. This approach not only improves output quality but also reduces noise, making your RAG system feel smarter and more intuitive. Implementing Code Generation with RAG Using RAG for code generation is like having a supercharged assistant that knows your codebase inside and out. Instead of starting from scratch, it pulls in relevant code snippets from repositories or internal knowledge bases, grounding its outputs in real-world examples. For instance, if you're building an API, RAG can fetch boilerplate code for authentication, adjust it to your framework, and even suggest parameter tweaks—cutting down on bugs and aligning with best practices. It doesn’t replace your creativity but handles the repetitive work, letting you focus on unique challenges. Image source: arunapattam.medium.com Creating Effective Prompts Crafting detailed prompts is key. A vague request like “generate a login function” may produce generic code, but adding specifics—framework details, authentication methods, or edge cases—turns it into a precise blueprint. Think of it as giving clear instructions: the more detail you provide (like “write a Python function using json.loads to parse a JSON string and handle invalid inputs with exceptions”), the better the output. Iterative refinement also helps: start broad, review, and tweak until the code fits perfectly. Managing Context and Dependencies RAG excels when you structure retrieved data to reflect your project’s flow. When generating a function that interacts with a database, include details like schema, connection methods, and constraints to minimize guesswork. A handy trick is dependency mapping—retrieving related modules or libraries beforehand ensures seamless integration. Don’t forget to specify library or framework versions in your prompts to avoid subtle, hard-to-debug issues. Generating Modular Code Break down code generation into smaller, self-contained units rather than one large module. For example, instead of asking for a complete authentication module, prompt RAG to generate specific components like token validation or password hashing. Using incremental prompts—starting with high-level requirements and then zooming in on details—helps maintain logical consistency and makes debugging easier. Testing and Validation Integrate testing right into the generated code. Prompt RAG to produce both a function and its corresponding unit tests to catch errors early. Pair your RAG outputs with static analyzers or linters (like Pylint for Python) to ensure the code meets your standards and handles edge cases. Testing across multiple library versions further ensures compatibility in fast-changing environments. In short, treat validation as a continuous loop: test, refine, and iterate for robust, production-ready code. Advanced Usage and Optimization Imagine RAG as a smart GPS that recalculates your route based on traffic. With dynamic retrieval prioritization, you can assign weights to data—favoring recent commits or highly-rated snippets—to ensure your generated code stays current. For example, pulling from active GitHub repositories minimizes outdated solutions. Plus, by integrating domain-specific elements (like medical ontologies for healthcare apps), RAG can deliver code that not only works but also complies with industry regulations. Image source: trantorinc.com 1. Customizing the Retrieval Strategy Not all retrieval strategies are equal. For projects with complex, layered dependencies—like microservices—recursive retrieval digs deep, pulling every related component (from endpoints to authentication methods). In dynamic settings, such as debugging a live e-commerce platform during a flash sale, adaptive retrieval prioritizes recent logs and commits, ensuring fixes are based on the latest context. Fine-tuning your queries with domain-specific keywords further sharpens the system’s ability to fetch the right data. 2. Fine-Tuning Models for Specific Languages Fine-tuning isn’t just about more data—it’s about understanding language nuances. Whether it’s Python’s flexible syntax or Java’s strict type system, training with language-specific idioms (like Python’s list comprehensions or Java’s try-with-resources) ensures that the generated code feels natural. Domain-adaptive pretraining on real-world projects, such as Django for Python or Spring Boot for Java, reduces syntax errors and makes suggestions more contextually relevant. Don’t forget to analyze and adjust for common error patterns, like off-by-one mistakes, to create a system that writes code you’d actually use. 3. Scaling RAG Systems for Large Projects As projects grow, so do data sources. Federated retrieval—querying multiple databases in parallel—helps avoid bottlenecks and speeds up response times. Pre-ranking data sources using metadata or historical query patterns can significantly cut down on unnecessary retrievals, as demonstrated by a DevOps team that reduced latency by 40%. Leveraging auto-scaling cloud services also ensures that your system adapts smoothly to spikes in demand, proving that smart resource management is key to scaling successfully. 4. Integrating Feedback Loops for Improvement Real-time feedback is a game-changer. Embedding user input (like thumbs-up/down ratings) directly into the retrieval and generation cycle lets the system adjust dynamically, favoring more relevant data over time. Automatic feedback mechanisms—such as classifiers that flag low-confidence outputs—can trigger corrective actions without human intervention. Moreover, contextual feedback (tied to specific use cases like debugging logs versus API documentation) ensures that the system evolves in meaningful ways. Ultimately, these feedback loops help build a RAG system that learns smarter, not just harder. Case Studies and Real-World Applications Let’s talk about how RAG is making waves in the real world. Take GitHub Copilot, for example. By integrating RAG, it retrieves context-specific code snippets from vast repositories, helping developers write efficient code faster. One team reported a 40% reduction in development time for repetitive tasks like boilerplate generation. That’s not just faster—it’s smarter. Now, contrast that with healthcare. A hospital network used RAG to enhance its knowledge base, retrieving clinical guidelines tailored to specific cases. The result? Doctors spent 30% less time searching for information, allowing them to focus on patient care. It’s like having a medical librarian on speed dial. RAG isn’t just about speed—it’s about precision. In e-commerce, a platform improved product search by combining retrieval with generative descriptions, boosting conversion rates by 15%. Whether it’s debugging code or saving lives, RAG proves that context is king. Image source: astera.com Automating Boilerplate Code Generation Boilerplate code is the broccoli of software development—necessary but tedious. RAG flips the script by pulling reusable patterns from internal repositories and external libraries, then tailoring them to your project’s context. For instance, a fintech startup used RAG to auto-generate API integration templates, cutting onboarding time for new developers by 50%. That’s not just efficiency; it’s onboarding with a turbo boost. Here’s why it works: RAG doesn’t just retrieve code—it understands why it’s relevant. By analyzing metadata like function usage and dependencies, it ensures the generated code aligns with your architecture. Think of it as a sous-chef who preps exactly what you need, no more, no less. Enhancing Developer Productivity Let’s talk about meetings—the productivity killer. RAG can eliminate the need for endless clarification sessions by acting as a real-time knowledge hub. For example, a SaaS company integrated RAG into their IDE, enabling developers to instantly retrieve project-specific guidelines, past decisions, and even code review notes. The result? Fewer interruptions, more deep work. Here’s the magic: RAG thrives on context. By combining retrieval with generative capabilities, it delivers not just answers but actionable insights tailored to the task at hand. This is especially powerful during debugging, where RAG can surface relevant error logs and past fixes, slashing resolution times. RAG in Collaborative Coding Environments Team coding can suffer from miscommunication, but RAG helps keep everyone on the same page. For example, a fintech startup used RAG to centralize code reviews, allowing developers to instantly access annotated feedback and historical decisions. This streamlined process reduced redundant discussions and reinforced coding standards. RAG works best with a structured knowledge base—tagged code snippets with metadata like author notes and dependencies create a shared context for all. This is especially beneficial for remote teams, where asynchronous collaboration can lead to gaps in understanding. For even better results, integrate RAG with version control systems. This way, each pull request comes with context-aware suggestions, making reviews faster and more insightful. The end result is a collaborative coding environment that feels seamless rather than chaotic. Challenges and Considerations Let’s talk about the elephant in the room: data quality. RAG systems are only as good as the knowledge base they pull from. Imagine feeding outdated or poorly tagged code snippets into the system—it’s like asking a GPS to navigate with a 10-year-old map. A real-world example? A retail company’s RAG tool suggested deprecated APIs, leading to hours of rework. The fix? Regularly auditing and updating the knowledge base. Another curveball? Balancing retrieval precision with speed. Developers want answers fast, but overly broad retrieval can flood them with irrelevant results. Think of it like searching for a needle in a haystack—except the haystack keeps growing. Techniques like multi-vector indexing can help, but they require careful tuning. Finally, privacy concerns can’t be ignored. Using proprietary or sensitive data in RAG systems demands airtight compliance with regulations like GDPR. Without it, you’re risking more than just bad code—you’re risking trust. Image source: code-rag-bench.github.io Ensuring Code Quality and Reliability Context is king when it comes to code quality. RAG systems thrive on well-structured, domain-specific knowledge bases, but here’s a twist—context layering can make or break reliability. For example, a fintech company used RAG to generate compliance-critical code. By embedding regulatory guidelines directly into the retrieval layer, they avoided costly errors. Without this, the system might’ve generated code that passed tests but failed audits. Another overlooked factor? Dependency mapping. RAG can retrieve snippets that work in isolation but break when integrated. Think of it like assembling IKEA furniture without checking if all the screws fit. A better approach? Use dependency-aware prompts that account for versioning and compatibility. Finally, don’t underestimate automated testing integration. Embedding unit tests into generated code isn’t just a nice-to-have—it’s essential. It’s like having a safety net that catches issues before they spiral into production nightmares. Addressing Security Concerns Let’s talk about data leakage—a sneaky risk in RAG systems. when RAG retrieves sensitive data, it’s not just about encryption. The real game-changer is query anonymization. By stripping identifiable metadata from queries, you prevent exposing proprietary information during retrieval. For instance, a healthcare provider used this approach to safeguard patient data while still leveraging RAG for real-time diagnostics. Another underappreciated tactic? Access throttling. Think of it as a speed bump for malicious actors. By limiting query rates and implementing role-based access controls, you reduce the risk of unauthorized data scraping. This is especially critical in multi-tenant environments where one breach could cascade. Finally, don’t sleep on red teaming. Simulating attacks on your RAG pipeline uncovers vulnerabilities you didn’t know existed. It’s like stress-testing a bridge before letting traffic flow. The takeaway? Security isn’t static—it’s a living, breathing process. Ethical Implications of Automated Code Generation Let’s zero in on algorithmic bias—a silent disruptor in automated code generation. bias often sneaks in through training data. If your RAG system pulls from repositories with skewed patterns (e.g., underrepresentation of certain programming paradigms), it can perpetuate inequities. For example, a fintech company found their model favoring legacy systems over modern, inclusive frameworks, stalling innovation. One effective fix? Diverse data curation. By sourcing from a wide range of repositories and auditing for inclusivity, you can reduce bias at the root. Another game-changer is bias-aware prompts. These guide the system to prioritize fairness, like ensuring gender-neutral variable names or avoiding culturally loaded terms. But here’s the twist: bias isn’t just technical—it’s cultural. Engaging diverse stakeholders during development can surface blind spots you’d never catch alone. The result? Systems that don’t just generate code but foster equity in tech. FAQ 1. What is Retrieval-Augmented Generation (RAG) and how does it enhance code generation? Retrieval-Augmented Generation (RAG) is a cutting-edge AI technique that combines the strengths of retrieval-based methods and generative models to produce contextually accurate and information-rich outputs. In the context of code generation, RAG enhances the process by retrieving relevant, real-time data from external knowledge bases, such as code repositories, documentation, or issue trackers, and integrating this information into the generated code. This approach ensures that the outputs are not only aligned with the latest standards and practices but also tailored to specific project requirements. By reducing reliance on static training data, RAG minimizes errors, accelerates development workflows, and empowers developers to create robust, high-quality code with greater efficiency. 2. How can RAG be integrated into existing software development workflows? RAG can be seamlessly integrated into existing software development workflows by embedding it within commonly used tools and processes. Start by incorporating RAG-enabled plugins into Integrated Development Environments (IDEs) to provide context-aware code suggestions and completions. Enhance your debugging process by using RAG to retrieve relevant error resolutions from internal issue trackers or external forums. For documentation, RAG can automate updates by analyzing code changes and generating synchronized documentation. Additionally, integrating RAG with version control systems allows for contextually enriched code reviews, ensuring alignment with project standards. By tailoring RAG to your team’s specific needs and maintaining an up-to-date knowledge base, it becomes a powerful assistant that complements and enhances your existing development practices. 3. What are the best practices for preparing a knowledge base for RAG in code generation? Preparing a knowledge base for RAG in code generation requires a structured and meticulous approach to ensure relevance and accuracy. Begin by curating high-quality, domain-specific data, including code snippets, documentation, and best practices, while eliminating outdated or redundant information. Organize the data with metadata tagging to enhance retrieval precision and ensure that the knowledge base is easily searchable. Automate regular updates to keep the information current and aligned with evolving project requirements. Incorporate feedback loops from developers to refine the knowledge base continuously, addressing gaps or inaccuracies. Finally, prioritize security and compliance by anonymizing sensitive data and adhering to regulatory standards, ensuring the knowledge base is both robust and trustworthy. 4. How does RAG ensure the accuracy and reliability of generated code? RAG ensures the accuracy and reliability of generated code by grounding its outputs in real-time, contextually relevant data retrieved from trusted external sources. By leveraging a well-maintained knowledge base, RAG minimizes the risk of hallucinations and outdated information, producing code that aligns with current standards and practices. The retrieval component dynamically fetches precise snippets or documentation tailored to the task, while the generative model integrates this data to create coherent and functional outputs. Additionally, incorporating automated testing and validation mechanisms during the generation process further enhances reliability, ensuring that the generated code meets quality benchmarks and integrates seamlessly into existing systems. 5. What tools and frameworks are recommended for implementing RAG in code generation projects? Implementing RAG in code generation projects benefits greatly from leveraging specialized tools and frameworks. Vector databases like FAISS and Pinecone are essential for efficient data retrieval, enabling quick access to relevant information. Libraries such as LangChain and Hugging Face Transformers provide robust support for integrating retrieval and generation components, streamlining the development process. LlamaIndex is another valuable tool, aiding in data processing and model training to ensure seamless integration of machine learning components. For monitoring and maintaining system health, tools like Prometheus and Grafana are highly recommended. By combining these resources, developers can build scalable, efficient, and reliable RAG systems tailored to their specific project needs. Conclusion RAG isn’t just a tool—it’s a bridge between static knowledge and dynamic problem-solving in code generation. Think of it as a well-trained sous-chef in a bustling kitchen, fetching the exact ingredients you need, exactly when you need them. By integrating RAG into workflows, developers can focus on creativity while the system handles the grunt work of retrieval and synthesis. Take GitHub Copilot, for example. It’s not just speeding up coding; it’s reducing bugs by referencing real-world examples from vast repositories. Similarly, in a healthcare project, RAG cut development time by 30% by pulling domain-specific code snippets directly into prototypes. These aren’t just efficiency gains—they’re paradigm shifts. But RAG thrives on quality inputs. A poorly curated knowledge base is like a cluttered pantry—chaos in, chaos out. Invest in your data, and RAG will return the favor tenfold. Featured Building a Retrieval-Augmented Generation System with Deep Seek R1 This guide explores advanced strategies for optimizing DeepSeek R1 in RAG systems, including dynamic embedding scaling, multi-modal data integration, adaptive indexing, query re-ranking, caching, parallelization, and domain-specific fine-tuning. 04 Feb 2025 17 min read Featured Is RAG Dead? How DeepSeek R1 is Redefining Custom RAG Chatbots With the rise of DeepSeek R1, RAG chatbots are evolving beyond their limits. This post explores how advanced retrieval techniques, dynamic embeddings, and real-time adaptation are redefining chatbot intelligence, making them more accurate, scalable, and context-aware. 04 Feb 2025 13 min read DeepSeek's Viral Surge: RAG Implications for Enterprise AI Deployments DeepSeek's rapid adoption is making waves in enterprise AI, redefining Retrieval-Augmented Generation (RAG). This analysis explores its implications for AI deployments, efficiency, and how businesses can leverage its capabilities for smarter, scalable solutions. 03 Feb 2025 2 min read Chitika: Explore Retrieval Augmented Generation Trends © 2025 Terms of Service Privacy Policy Powered by Ghost Social Media Handles

Source: https://medium.com/nanonets/retrieval-augmented-generation-and-rag-workflows-1bd26f4da74b
Retrieval-Augmented Generation and RAG Workflows | by Prithiv Sassisegarane | NanoNets | Medium Open in app Sign up Sign in Write Sign up Sign in Retrieval-Augmented Generation and RAG Workflows Introduction Prithiv Sassisegarane · Follow Published in NanoNets · 15 min read · Oct 25, 2023 -- Listen Share Retrieval Augmented Generation, or RAG, is a mechanism that helps large language models (LLMs) like GPT become more useful and knowledgeable by pulling in information from a store of useful data, much like fetching a book from a library. Here’s how RAG makes magic with simple AI workflows: Knowledge Base (Input): Think of this as a big library full of useful stuff — FAQs, manuals, documents, etc. When a question pops up, this is where the system looks for answers. Trigger/Query (Input): This is the starting point. Usually, it’s a question or a request from a user that tells the system, “Hey, I need you to do something!” Task/Action (Output): Once the system gets the trigger, it swings into action. If it’s a question, it digs up an answer. If it’s a request to do something, it gets that thing done. Now, let’s break down the RAG mechanism into simple steps: Retrieval: First off, when a question or request comes in, RAG scours through the Knowledge Base to find relevant info. Augmentation: Next, it takes this info and mixes it up with the original question or request. This is like adding more detail to the basic request to make sure the system understands it fully. Generation: Lastly, with all this rich info at hand, it feeds it into a large language model which then crafts a well-informed response or performs the required action. So, in a nutshell, RAG is like having a smart assistant that first looks up useful info, blends it with the question at hand, and then either gives out a well-rounded answer or performs a task as needed. This way, with RAG, your AI system isn’t just shooting in the dark; it has a solid base of information to work from, making it more reliable and helpful. What problem do they solve? Bridging the Knowledge Gap Generative AI, powered by LLMs, is proficient at spawning text responses based on a colossal amount of data it was trained on. While this training enables the creation of readable and detailed text, the static nature of the training data is a critical limitation. The information within the model becomes outdated over time, and in a dynamic scenario like a corporate chatbot, the absence of real-time or organization-specific data can lead to incorrect or misleading responses. This scenario is detrimental as it undermines the user’s trust in the technology, posing a significant challenge especially in customer-centric or mission-critical applications. The RAG Solution RAG comes to the rescue by melding the generative capabilities of LLMs with real-time, targeted information retrieval, without altering the underlying model. This fusion allows the AI system to provide responses that are not only contextually apt but also based on the most current data. For instance, in a sports league scenario, while an LLM could provide generic information about the sport or teams, RAG empowers the AI to deliver real-time updates about recent games or player injuries by accessing external data sources like databases, news feeds, or even the league’s own data repositories. Data that stays up-to-date The essence of RAG lies in its ability to augment the LLM with fresh, domain-specific data. The continual updating of the knowledge repository in RAG is a cost-effective way to ensure the generative AI remains current. Moreover, it provides a layer of context that a generalized LLM lacks, thereby enhancing the quality of responses. The ability to identify, correct, or delete incorrect information within the RAG’s knowledge repository further adds to its appeal, ensuring a self-correcting mechanism for more accurate information retrieval. Examples of RAG Workflows In the evolving realm of artificial intelligence, Retrieval-Augmented Generation (RAG) is making a substantial impact across various business sectors by significantly enhancing the capabilities of Large Language Models (LLMs). Let us look at a few examples to get a sense of how RAG workflows automate tasks - Customer Service Enhancement: Scenario: A telecommunications company implements a RAG-augmented chatbot to handle customer queries regarding plan details, billing, and troubleshooting common issues. Workflow: Retrieval: On receiving a query about a specific plan’s data allowance, the system references the latest plans and offers from its database. Augmentation: It combines this retrieved information with the customer’s current plan details (from the customer profile) and the original query. Generation: The system generates a tailored response, explaining the data allowance differences between the customer’s current plan and the queried plan. Legal Research and Case Preparation: Scenario: A law firm integrates RAG to expedite legal research and case preparation. Workflow: Retrieval: On input about a new case, it pulls up relevant legal precedents, statutes, and recent judgements. Augmentation: It correlates this data with the case details. Generation: The system drafts a preliminary case brief, significantly reducing the time attorneys spend on preliminary research. Automated Marketing Campaigns: Scenario: A digital marketing agency implements RAG to automate the creation and deployment of marketing campaigns based on real-time market trends and consumer behavior. Workflow: Retrieval: Whenever a new lead comes into the system, the RAG mechanism fetches relevant details of the lead and their organization and triggers the start of the workflow. Augmentation: It combines this data with the client’s marketing objectives, brand guidelines, and target demographics. Task Execution: The system autonomously designs and deploys a tailored marketing campaign across various digital channels to capitalize on the identified trend, tracking the campaign’s performance in real-time for possible adjustments. Inventory Management and Reordering: Scenario: An e-commerce company employs a RAG-augmented system to manage inventory and automatically reorder products when stock levels fall below a predetermined threshold. Workflow: Retrieval: When a product’s stock reaches a low level, the system checks the sales history, seasonal demand fluctuations, and current market trends from its database. Augmentation: Combining the retrieved data with the product’s reorder frequency, lead times, and supplier details, it determines the optimal quantity to reorder. Task Execution: The system then interfaces with the company’s procurement software to automatically place a purchase order with the supplier, ensuring that the e-commerce platform never runs out of popular products. Employee Onboarding and IT Setup: Scenario: A multinational corporation uses a RAG-powered system to streamline the onboarding process for new employees, ensuring that all IT requirements are set up before the employee’s first day. Workflow: Retrieval: Upon receiving details of a new hire, the system consults the HR database to determine the employee’s role, department, and location. Augmentation: It correlates this information with the company’s IT policies, determining the software, hardware, and access permissions the new employee will need. Task Execution: The system then communicates with the IT department’s ticketing system, automatically generating tickets to set up a new workstation, install necessary software, and grant appropriate system access. This ensures that when the new employee starts, their workstation is ready, and they can immediately dive into their responsibilities. These examples underscore the versatility and practical benefits of employing RAG workflows in addressing complex, real-time business challenges across a myriad of domains. Connect your data and apps with Nanonets AI Assistant to chat with data, deploy custom chatbots & agents, and create RAG workflows. Schedule a Demo How to build your own RAG Workflows? Process of Building an RAG Workflow The process of building a Retrieval Augmented Generation (RAG) workflow can be broken down into several key steps. These steps can be categorized into three main processes: ingestion, retrieval, and generation, as well as some additional preparation: 1. Preparation: Knowledge Base Preparation: Prepare a data repository or a knowledge base by ingesting data from various sources — apps, documents, databases. This data should be formatted to allow efficient searchability, which basically means that this data should be formatted into a unified ‘Document’ object representation. 2. Ingestion Process: Vector Database Setup: Utilize Vector Databases as knowledge bases, employing various indexing algorithms to organize high-dimensional vectors, enabling fast and robust querying ability. Data Extraction: Extract data from these documents. Data Chunking: Break down documents into chunks of data sections. Data Embedding: Transform these chunks into embeddings using an embeddings model like the one provided by OpenAI. Develop a mechanism to ingest your user query. This can be a user interface or an API-based workflow. 3. Retrieval Process: Query Embedding: Get the data embedding for the user query. Chunk Retrieval: Perform a hybrid search to find the most relevant stored chunks in the Vector Database based on the query embedding. Content Pulling: Pull the most relevant content from your knowledge base into your prompt as context. 4. Generation Process: Prompt Generation: Combine the retrieved information with the original query to form a prompt. Now, you can perform - Response Generation: Send the combined prompt text to the LLM (Large Language Model) to generate a well-informed response. Task Execution: Send the combined prompt text to your LLM data agent which will infer the correct task to perform based on your query and perform it. For example, you can create a Gmail data agent and then prompt it to “send promotional emails to recent Hubspot leads” and the data agent will - fetch recent leads from Hubspot. use your knowledge base to get relevant info regarding leads. Your knowledge base can ingest data from multiple data sources — LinkedIn, Lead Enrichment APIs, and so on. curate personalized promotional emails for each lead. send these emails using your email provider / email campaign manager. 5. Configuration and Optimization: Customization: Customize the workflow to fit specific requirements, which might include adjusting the ingestion flow, such as preprocessing, chunking, and selecting the embedding model. Optimization: Implement optimization strategies to improve the quality of retrieval and reduce the token count to process, which could lead to performance and cost optimization at scale. Implementing One Yourself Implementing a Retrieval Augmented Generation (RAG) workflow is a complex task that involves numerous steps and a good understanding of the underlying algorithms and systems. Below are the highlighted challenges and steps to overcome them for those looking to implement a RAG workflow: Challenges in building your own RAG workflow: Novelty and Lack of Established Practices: RAG is a relatively new technology, first proposed in 2020, and developers are still figuring out the best practices for implementing its information retrieval mechanisms in generative AI. Cost: Implementing RAG will be more expensive than using a Large Language Model (LLM) alone. However, it’s less costly than frequently retraining the LLM. Data Structuring: Determining how to best model structured and unstructured data within the knowledge library and vector database is a key challenge. Incremental Data Feeding: Developing processes for incrementally feeding data into the RAG system is crucial. Handling Inaccuracies: Putting processes in place to handle reports of inaccuracies and to correct or delete those information sources in the RAG system is necessary. Connect your data and apps with Nanonets AI Assistant to chat with data, deploy custom chatbots & agents, and create RAG workflows. Schedule a Demo How to get started with creating your own RAG Workflow: Implementing a RAG workflow requires a blend of technical knowledge, the right tools, and continuous learning and optimization to ensure its effectiveness and efficiency in meeting your objectives. For those looking to implement RAG workflows themselves, we have curated a list of comprehensive hands-on guides that walk you through the implementation processes in detail - Nanonets tutorial on building RAG workflows using Llamaindex. Medium tutorial on building a chatbot with GPT and LLMs. Nanonets blog on how to build your own Zendesk Answer Bot with LLMs? Introducting Llamaindex Data Agents. Scalable RAG applications on GCP with Serverless architecture. AWS tutorial on deploying tool-using LLM agents using AWS SageMaker. Streamlit tutorial on building a chatbot with custom data sources. LangChain Agents: Simply Explained! Building a LangChain Custom Medical Agent with Memory. Each of the tutorials comes with a unique approach or platform to achieve the desired implementation on the specified topics. If you are looking to delve into building your own RAG workflows, we recommend checking out all of the articles listed above to get a holistic sense required to get started with your journey. Implement RAG Workflows using ML Platforms While the allure of constructing a Retrieval Augmented Generation (RAG) workflow from the ground up offers a certain sense of accomplishment and customization, it’s undeniably a complex endeavor. Recognizing the intricacies and challenges, several businesses have stepped forward, offering specialized platforms and services to simplify this process. Leveraging these platforms can not only save valuable time and resources but also ensure that the implementation is based on industry best practices and is optimized for performance. For organizations or individuals who may not have the bandwidth or expertise to build a RAG system from scratch, these ML platforms present a viable solution. By opting for these platforms, one can: Bypass the Technical Complexities: Avoid the intricate steps of data structuring, embedding, and retrieval processes. These platforms often come with pre-built solutions and frameworks tailored for RAG workflows. Leverage Expertise: Benefit from the expertise of professionals who have a deep understanding of RAG systems and have already addressed many of the challenges associated with its implementation. Scalability: These platforms are often designed with scalability in mind, ensuring that as your data grows or your requirements change, the system can adapt without a complete overhaul. Cost-Effectiveness: While there’s an associated cost with using a platform, it might prove to be more cost-effective in the long run, especially when considering the costs of troubleshooting, optimization, and potential re-implementations. Let us take a look at platforms offering RAG workflow creation capabilities. Nanonets Nanonets offers secure AI assistants, chatbots, and RAG workflows powered by your company’s data. It enables real-time data synchronization between various data sources, facilitating comprehensive information retrieval for teams. The platform allows the creation of chatbots along with deployment of complex workflows through natural language, powered by Large Language Models (LLMs). It also provides data connectors to read and write data in your apps, and the ability to utilize LLM agents to directly perform actions on external apps. Nanonets AI Assistant Product Page AWS Generative AI AWS offers a variety of services and tools under its Generative AI umbrella to cater to different business needs. It provides access to a wide range of industry-leading foundation models from various providers through Amazon Bedrock. Users can customize these foundation models with their own data to build more personalized and differentiated experiences. AWS emphasizes security and privacy, ensuring data protection when customizing foundation models. It also highlights cost-effective infrastructure for scaling generative AI, with options such as AWS Trainium, AWS Inferentia, and NVIDIA GPUs to achieve the best price performance. Moreover, AWS facilitates the building, training, and deploying of foundation models on Amazon SageMaker, extending the power of foundation models to a user’s specific use cases. AWS Generative AI Product Page Generative AI on Google Cloud Google Cloud’s Generative AI provides a robust suite of tools for developing AI models, enhancing search, and enabling AI-driven conversations. It excels in sentiment analysis, language processing, speech technologies, and automated document management. Additionally, it can create RAG workflows and LLM agents, catering to diverse business requirements with a multilingual approach, making it a comprehensive solution for various enterprise needs. Google Cloud Generative AI Oracle Generative AI Oracle’s Generative AI (OCI Generative AI) is tailored for enterprises, offering superior models combined with excellent data management, AI infrastructure, and business applications. It allows refining models using user’s own data without sharing it with large language model providers or other customers, thus ensuring security and privacy. The platform enables the deployment of models on dedicated AI clusters for predictable performance and pricing. OCI Generative AI provides various use cases like text summarization, copy generation, chatbot creation, stylistic conversion, text classification, and data searching, addressing a spectrum of enterprise needs. It processes user’s input, which can include natural language, input/output examples, and instructions, to generate, summarize, transform, extract information, or classify text based on user requests, sending back a response in the specified format. Oracle Generative AI Cloudera In the realm of Generative AI, Cloudera emerges as a trustworthy ally for enterprises. Their open data lakehouse, accessible on both public and private clouds, is a cornerstone. They offer a gamut of data services aiding the entire data lifecycle journey, from the edge to AI. Their capabilities extend to real-time data streaming, data storage and analysis in open lakehouses, and the deployment and monitoring of machine learning models via the Cloudera Data Platform. Significantly, Cloudera enables the crafting of Retrieval Augmented Generation workflows, melding a powerful combination of retrieval and generation capabilities for enhanced AI applications. Cloudera Blog Page Glean Glean employs AI to enhance workplace search and knowledge discovery. It leverages vector search and deep learning-based large language models for semantic understanding of queries, continuously improving search relevance. It also offers a Generative AI assistant for answering queries and summarizing information across documents, tickets, and more. The platform provides personalized search results and suggests information based on user activity and trends, besides facilitating easy setup and integration with over 100 connectors to various apps. Glean Homepage Landbot Landbot offers a suite of tools for creating conversational experiences. It facilitates the generation of leads, customer engagement, and support via chatbots on websites or WhatsApp. Users can design, deploy, and scale chatbots with a no-code builder, and integrate them with popular platforms like Slack and Messenger. It also provides various templates for different use cases like lead generation, customer support, and product promotion Landbot.io Homepage Chatbase Chatbase provides a platform for customizing ChatGPT to align with a brand’s personality and website appearance. It allows for lead collection, daily conversation summaries, and integration with other tools like Zapier, Slack, and Messenger. The platform is designed to offer a personalized chatbot experience for businesses. Chatbase Product Page Scale AI Scale AI addresses the data bottleneck in AI application development by offering fine-tuning and RLHF for adapting foundation models to specific business needs. It integrates or partners with leading AI models, enabling enterprises to incorporate their data for strategic differentiation. Coupled with the ability to create RAG workflows and LLM agents, Scale AI provides a full-stack generative AI platform for accelerated AI application development. Scale AI Homepage Shakudo — LLM Solutions Shakudo offers a unified solution for deploying Large Language Models (LLMs), managing vector databases, and establishing robust data pipelines. It streamlines the transition from local demos to production-grade LLM services with real-time monitoring and automated orchestration. The platform supports flexible Generative AI operations, high-throughput vector databases, and provides a variety of specialized LLMOps tools, enhancing the functional richness of existing tech stacks. Shakundo RAG Workflows Product Page Each platform/business mentioned has its own set of unique features and capabilities, and could be explored further to understand how they could be leveraged for connecting enterprise data and implementing RAG workflows. Connect your data and apps with Nanonets AI Assistant to chat with data, deploy custom chatbots & agents, and create RAG workflows. Schedule a Demo RAG Workflows with Nanonets In the realm of augmenting language models to deliver more precise and insightful responses, Retrieval Augmented Generation (RAG) stands as a pivotal mechanism. This intricate process elevates the reliability and usefulness of AI systems, ensuring they aren’t merely operating in an information vacuum. At the heart of this, Nanonets AI Assistant emerges as a secure, multi-functional AI companion designed to bridge the gap between your organizational knowledge and Large Language Models (LLMs), all within a user-friendly interface. Here’s a glimpse into the seamless integration and workflow enhancement offered by Nanonets’ RAG capabilities: Data Connectivity: Nanonets facilitates seamless connections to over 100 popular workspace applications including Slack, Notion, Google Suite, Salesforce, and Zendesk, among others. It’s proficient in handling a wide spectrum of data types, be it unstructured like PDFs, TXTs, images, audio, and video files, or structured data such as CSVs, spreadsheets, MongoDB, and SQL databases. This broad-spectrum data connectivity ensures a robust knowledge base for the RAG mechanism to pull from. Trigger and Action Agents: With Nanonets, setting up trigger/action agents is a breeze. These agents are vigilant for events across your workspace apps, initiating actions as required. For instance, establish a workflow to monitor new emails at support@your_company.com, utilize your documentation and past email conversations as a knowledge base, draft an insightful email response, and send it out, all orchestrated seamlessly. Streamlined Data Ingestion and Indexing: Optimized data ingestion and indexing are part of the package, ensuring smooth data processing which is handled in the backdrop by the Nanonets AI Assistant. This optimization is crucial for the real-time sync with data sources, ensuring the RAG mechanism has the latest information to work with. To get started, you can get on a call with one of our AI experts and we can give you a personalized demo & trial of the Nanonets AI Assistant based on your use case. Once set up, you can use your Nanonets AI Assistant to - Create RAG Chat Workflows Empower your teams with comprehensive, real-time information from all your data sources. Create RAG Agent Workflows Use natural language to create and run complex workflows powered by LLMs that interact with all your apps and data. Deploy RAG based Chatbots Build and Deploy ready to use Custom AI Chatbots that know you within minutes. Propel Your Team’s Efficiency With Nanonets AI, you’re not just integrating data; you’re supercharging your team’s capabilities. By automating mundane tasks and providing insightful responses, your teams can reallocate their focus on strategic initiatives. Nanonets’ RAG-driven AI Assistant is more than just a tool; it’s a catalyst that streamlines operations, enhances data accessibility, and propels your organization towards a future of informed decision-making and automation. Connect your data and apps with Nanonets AI Assistant to chat with data, deploy custom chatbots & agents, and create RAG workflows. Schedule a Demo This article was originally published at https://nanonets.com/blog/retrieval-augmented-generation/ on 2023–10–25T01:02:43.000Z. -- -- Published in NanoNets 3.8K Followers ·Last published Jan 14, 2025 NanoNets: Machine Learning API Follow Written by Prithiv Sassisegarane 100 Followers ·6 Following Follow No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams

Source: https://www.analyticsvidhya.com/blog/2025/01/agentic-rag-system-architectures/
7 Agentic RAG System Architectures to Build AI Agents DeepSeek Learning Paths GenAI Pinnacle Program Agentic AI Pioneer Program New Login Switch Mode Logout Interview Prep Career GenAI Prompt Engg ChatGPT LLM Langchain RAG AI Agents Machine Learning Deep Learning GenAI Tools LLMOps Python NLP SQL AIML Projects Home AI Agents 7 Agentic RAG System Architectures to Build AI Agents 7 Agentic RAG System Architectures to Build AI Agents Pankaj Singh Last Updated : 07 Jan, 2025 27 min read For me, 2024 has been a year when I was not just using LLMs for content generation but also understanding their internal working. In this quest to learn about LLMs, RAG and more, I discovered the potential of AI Agents—autonomous systems capable of executing tasks and making decisions with minimal human intervention. Going back to 2023, Retrieval-Augmented Generation (RAG) was in the limelight, and 2024 advanced with Agentic RAG workflows, driving innovation across industries. Looking ahead, 2025 is set to be the “Year of AI Agents,” where autonomous systems will revolutionize productivity and reshape industries, unlocking unprecedented possibilities with the Agentic RAG Systems. These workflows, powered by autonomous AI agents capable of complex decision-making and task execution, enhance productivity and reshape how individuals and organisations tackle problems. The shift from static tools to dynamic, agent-driven processes has unlocked unprecedented efficiencies, laying the groundwork for an even more innovative 2025. Today, we will talk about the types of Agentic RAG systems. In this guide, we will go through the architecture of types of Agentic RAG and more. Table of contents Agentic RAG System: Combination of RAG and Agentic AI Systems Why Should We Care About Agentic RAG Systems? Agentic RAG: Merging RAG with AI Agents Agentic RAG Routers Query Planning Agentic RAG Adaptive RAG Agentic Corrective RAG Self-Reflective RAG Speculative RAG Self Route Agentic RAG Conclusion Agentic RAG System: Combination of RAG and Agentic AI Systems To simply understand Agentic RAG, let’s dissect the term: It is the amalgamation of RAG + AI Agents. If you don’t know these terms, don’t worry! We will be diving into them shortly. Now, I will shed light on both RAG and Agentic AI systems (AI Agents) What is RAG (Retrieval-Augmented Generation)? Source: Author RAG is a framework designed to enhance the performance of generative AI models by integrating external knowledge sources into the generative process. Here’s how it works: Retrieval Component: This part fetches relevant information from external knowledge bases, databases, or other data repositories. These sources can include structured or unstructured data, such as documents, APIs, or even live data streams. Augmentation: The retrieved information is used to inform and guide the generative model. This ensures the outputs are more factually accurate, grounded in external data, and contextually rich. Generation: The generative AI system (like GPT) synthesizes the retrieved knowledge with its own reasoning capabilities to produce final outputs. RAG is particularly valuable when working with complex queries or domains requiring up-to-date, domain-specific knowledge. What are AI Agents? Source: Dipanjan Sarkar Here’s the AI Agent Workflow responding to the query: “Who won the Euro in 2024? Tell me more details!”. Initial Instruction Prompt: The user inputs a query, such as “Who won the Euro in 2024? Tell me more details!”. LLM Processing and Tool Selection: The Large Language Model (LLM) interprets the query and decides if external tools (like web search) are needed. It initiates a function call for more details. Tool Execution and Context Retrieval: The selected tool (e.g., a search API) retrieves relevant information. Here, it fetches details about the Euro 2024 final. Response Generation: The new information is combined with the original query. The LLM generates a complete and final response: “Spain won the Euro 2024 against England with a score of 2–1 in the Final in Berlin on July 2024.” In a nutshell, an Agentic AI System has the following core components: Large Language Models (LLMs): The Brain of the Operation LLMs serve as the central processing unit, interpreting input and generating meaningful responses. Input Query: A user-provided question or command that initiates the AI’s operation. Understanding the Query: The AI analyzes the input to grasp its meaning and intent. Response Generation: Based on the query, the AI formulates an appropriate and coherent reply. Tools Integration: The Hands That Get Things Done External tools enhance the AI’s functionality to perform specific tasks beyond text-based interactions. Document Reader Tool: Processes and extracts insights from text documents. Analytics Tool: Performs data analysis to provide actionable insights. Conversational Tool: Facilitates interactive and dynamic dialogue capabilities. Memory Systems: The Key to Contextual Intelligence Memory allows the AI to retain and leverage past interactions for more context-aware responses. Short-term Memory: Holds recent interactions for immediate contextual use. Long-term Memory: Stores information over time for sustained reference. Semantic Memory: Maintains general knowledge and facts for informed interactions. This shows how AI integrates user prompts, tool outputs, and natural language generation. Here’s the definition of AI Agents: AI Agents are autonomous software systems designed to perform specific tasks or achieve certain objectives by interacting with their environment. Key traits of AI Agents include: Perception: They sense or retrieve data about their environment (e.g., from APIs or user inputs). Reasoning: They analyze the data to make informed decisions, often leveraging AI models like GPT for natural language understanding. Action: They perform actions in the real or virtual world, such as generating responses, triggering workflows, or modifying systems. Learning: Advanced agents often adapt and improve their performance over time based on feedback or new data. AI Agents can handle tasks across domains such as customer service, data analysis, workflow automation, and more. Why Should We Care About Agentic RAG Systems? Firstly, here are the limitations of basic Retrieval-Augmented Generation (RAG): When to Retrieve: The system might struggle to determine when retrieval is needed, potentially resulting in incomplete or less accurate answers. Document Quality: The retrieved documents might not align well with the user’s question, which can undermine the relevance of the response. Generation Errors: The model may “hallucinate,” adding inaccurate or unrelated information that isn’t supported by the retrieved content. Answer Precision: Even with relevant documents, the generated response might fail to directly or adequately address the user’s query, making the output less dependable. Reasoning Issues: The inability of the system to reason through complex queries hinders nuanced understanding. Limited Adaptability: Traditional systems can’t adapt strategies dynamically, like choosing API calls or web searches. Importance of Agentic RAG Understanding Agentic RAG systems, helps us deploy the right solutions for the above-given challenges, and specific tasks and ensures alignment with the intended use case. Here’s why it’s critical: Tailored Solutions: Different types of Agentic RAG systems are designed for varying levels of autonomy and complexity. For instance: Agentic RAG Router: Agentic RAG Routers is a modular framework that dynamically routes tasks to appropriate retrieval, generation, or action components based on the query’s intent and complexity. Self-Reflective RAG: Self-Reflective RAG integrates introspection mechanisms, enabling the system to evaluate and refine its responses by iteratively assessing retrieval relevance, generation quality, and decision-making accuracy before finalizing outputs. Knowing these types ensures optimal design and resource utilization. Risk Management: Agentic systems involve decision-making, which may introduce risks like incorrect actions, over-reliance, or misuse. Understanding the scope and limitations of each type mitigates these risks. Innovation & Scalability: Differentiating between types allows businesses to scale their systems from basic implementations to sophisticated agents capable of handling enterprise-level challenges. In a nutshell, the agentic RAG can plan, adapt, and iterate to find the right solution to the user. Agentic RAG: Merging RAG with AI Agents Combining the AI Agents and RAG workflow, here’s the architecture of Agentic RAG: Source: Author Agentic RAG combines the structured retrieval and knowledge integration capabilities of RAG with the autonomy and adaptability of AI agents. Here’s how it works: Dynamic Knowledge Retrieval: Agents equipped with RAG can retrieve specific information on the fly, ensuring they operate with the most current and contextually relevant data. Intelligent Decision-Making: The agent processes retrieved data, applying advanced reasoning to generate solutions, complete tasks, or answer questions with depth and accuracy. Task-Oriented Execution: Unlike a static RAG pipeline, Agentic RAG systems can execute multi-step tasks, adjust to changing objectives, or refine their approaches based on feedback loops. Continuous Improvement: Through learning, agents improve their retrieval strategies, reasoning capabilities, and task execution over time, becoming more efficient and effective. Applications of Agentic RAG Here are applications of Agentic RAG: Customer Support: Automatically retrieving and delivering accurate responses to user inquiries by accessing real-time data sources. Content Creation: Generating context-rich content for complex domains like legal or medical fields, supported by retrieved knowledge. Research Assistance: Helping researchers by autonomously gathering and synthesizing relevant materials from vast databases. Workflow Automation: Streamlining enterprise operations by integrating retrieval-driven decision-making into business processes. Agentic RAG represents a powerful synergy between Retrieval-Augmented Generation and autonomous AI agents, enabling systems to operate with unparalleled intelligence, adaptability, and relevance. It’s a significant step toward building AI systems that are not only informed but also capable of independently executing sophisticated, knowledge-intensive tasks. To understand this read this: RAG vs Agentic RAG: A Comprehensive Guide. I hope, now you are well versed with the Agentic RAG, in the next section I will tell you some important and popular types of Agentic RAG Systems along with their architectures. 1. Agentic RAG Routers As mentioned earlier, the term Agentic signifies that the system behaves like an intelligent agent, capable of reasoning and deciding which tools or methods to utilize for retrieving and processing data. By leveraging both retrieval (e.g., database search, web search, semantic search) and generation (e.g., LLM processing), this system ensures that the user’s query is answered in the most effective way possible. Similarly, Agentic RAG Routers are systems designed to dynamically route user queries to appropriate tools or data sources, enhancing the capabilities of Large Language Models (LLMs). The primary purpose of such routers is to combine retrieval mechanisms with the generative strengths of LLMs to deliver accurate and contextually rich responses. This approach bridges the gap between the static knowledge of LLMs (trained on pre-existing data) and the need for dynamic knowledge retrieval from live or domain-specific data sources. By combining retrieval and generation, Agentic RAG Routers enable applications such as: Question answering Data analysis Real-time information retrieval Recommendation generation Source: Author Architecture of Agentic RAG Routers The architecture shown in the diagram provides a detailed visualization of how Agentic RAG Routers operate. Let’s break down the components and flow: User Input and Query Processing User Input: A user submits a query, which is the entry point for the system. This could be a question, a command, or a request for specific data. Query: The user input is parsed and formatted into a query, which the system can interpret. Retrieval Agent The Retrieval Agent serves as the core processing unit. It acts as a coordinator, deciding how to handle the query. It evaluates: The intent of the query. The type of information required (structured, unstructured, real-time, recommendations). Router A Router determines the appropriate tool(s) to handle the query: Vector Search: Retrieves relevant documents or data using semantic embeddings. Web Search: Accesses live information from the internet. Recommendation System: Suggests content or results based on prior user interactions or contextual relevance. Text-to-SQL: Converts natural language queries into SQL commands for accessing structured databases. Tools: The tools listed here are modular and specialized: Vector Search A & B: Designed to search semantic embeddings for matching content in vectorized forms, ideal for unstructured data like documents, PDFs, or books. Web Search: Accesses external, real-time web data. Recommendation System: Leverages AI models to provide user-specific suggestions. Data Sources: The system connects to diverse data sources: Structured Databases: For well-organized information (e.g., SQL-based systems). Unstructured Sources: PDFs, books, research papers, etc. External Repositories: For semantic search, recommendations, and real-time web queries. LLM Integration: Once data is retrieved, it is fed into the LLM: The LLM synthesizes the retrieved information with its generative capabilities to create a coherent, human-readable response. Output: The final response is sent back to the user in a clear and actionable format. Types of Agentic RAG Routers Here are the types of Agentic Rag Routers: 1. Single Agentic RAG Router Source: Author In this setup, there is one unified agent responsible for all routing, retrieval, and decision-making tasks. Simpler and more centralized, ideal for systems with limited data sources or tools. Use Case: Applications with a single type of query, such as retrieving specific documents or processing SQL-based requests. In the Single Agentic RAG Router: Query Submission: The user submits a query, which is processed by a single Retrieval Agent. Routing via a Single Agent: The Retrieval Agent evaluates the query and passes it to a single router, which decides which tool to use (e.g., Vector Search, Web Search, Text-to-SQL, Recommendation System). Tool Access: The router connects the query to one or more tools, depending on the need. Each tool fetches data from its respective data source: Text-to-SQL interacts with databases like PostgreSQL or MySQL for structured queries. Semantic Search retrieves data from PDFs, books, or unstructured sources. Web Search fetches real-time online information. Recommendation Systems provide suggestions based on the context or user profile. LLM Integration: After retrieval, the data is passed to the LLM, which combines it with its generative capabilities to produce a response. Output: The response is delivered back to the user in a clear, actionable format. This approach is centralized and efficient for simple use cases with limited data sources and tools. 2. Multiple Agentic RAG Routers Source: Author This architecture involves multiple agents, each handling a specific type of task or query. More modular and scalable, suitable for complex systems with diverse tools and data sources. Use Case: Multi-functional systems that serve various user needs, such as research, analytics, and decision-making across multiple domains. In the Multiple Agentic RAG Routers: Query Submission: The user submits a query, which is initially processed by a Retrieval Agent. Distributed Retrieval Agents: Instead of a single router, the system employs multiple retrieval agents, each specializing in a specific type of task. For example: Retrieval Agent 1 might handle SQL-based queries. Retrieval Agent 2 might focus on semantic searches. Retrieval Agent 3 could prioritize recommendations or web searches. Individual Routers for Tools: Each Retrieval Agent routes the query to its assigned tool(s) from the shared pool (e.g., Vector Search, Web Search, etc.) based on its scope. Tool Access and Data Retrieval: Each tool fetches data from the respective sources as required by its retrieval agent. Multiple agents can operate in parallel, ensuring that diverse query types are processed efficiently. LLM Integration and Synthesis: All the retrieved data is passed to the LLM, which synthesizes the information and generates a coherent response. Output: The final, processed response is returned to the user. This approach is modular and scalable, suitable for complex systems with diverse tools and high query volume. Agentic RAG Routers combine intelligent decision-making, robust retrieval mechanisms, and LLMs to create a versatile query-response system. The architecture optimally routes user queries to appropriate tools and data sources, ensuring high relevance and accuracy. Whether using a single or multiple router setup, the design depends on the system’s complexity, scalability needs, and application requirements. 2. Query Planning Agentic RAG Query Planning Agentic RAG (Retrieval-Augmented Generation) is a methodology designed to handle complex queries efficiently by leveraging multiple parallelizable subqueries across diverse data sources. This approach combines intelligent query division, distributed processing, and response synthesis to deliver accurate and comprehensive results. Source: Author Core Components of Query Planning Agentic RAG Here are the core components: User Input and Query Submission User Input: The user submits a query or request into the system. The input query is processed and passed downstream for further handling. Query Planner: The Query Planner is the central component orchestrating the process. It: Interprets the query provided by the user. Generates appropriate prompts for the downstream components. Decide which tools (query engines) to invoke to answer specific parts of the query. Tools The tools are specialized pipelines (e.g., RAG pipelines) containing query engines, such as: Query Engine 1 Query Engine 2 These pipelines are responsible for retrieving relevant information or context from external knowledge sources (e.g., databases, documents, or APIs). The retrieved information is sent back to the Query Planner for integration. LLM (Large Language Model) The LLM serves as the synthesis engine for complex reasoning, natural language understanding, and response generation. It interacts bidirectionally with the Query Planner: Receives prompts from the planner. Provides context-aware responses or refined outputs based on the retrieved information. Synthesis and Output Synthesis: The system combines retrieved information from tools and the LLM’s response into a coherent answer or solution. Output: The final synthesized result is presented to the user. Key Highlights Modular Design: The architecture allows for flexibility in tool selection and integration. Efficient Query Planning: The Query Planner acts as an intelligent intermediary, optimizing which components are used and in what order. Retrieval-Augmented Generation: By leveraging RAG pipelines, the system enhances the LLM’s knowledge with up-to-date and domain-specific information. Iterative Interaction: The Query Planner ensures iterative collaboration between the tools and the LLM, refining the response progressively. 3. Adaptive RAG Adaptive Retrieval-Augmented Generation (Adaptive RAG) is a method that enhances the flexibility and efficiency of large language models (LLMs) by tailoring the query handling strategy to the complexity of the incoming query. Key Idea of Adaptive RAG Adaptive RAG dynamically chooses between different strategies for answering questions—ranging from simple single-step approaches to more complex multi-step or even no-retrieval processes—based on the complexity of the query. This selection is facilitated by a classifier, which analyzes the query’s nature and determines the optimal approach. Source: Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity Comparison with Other Methods Here’s the comparison with single-step, multi-step and adaptive approach: Single-Step Approach How it Works: For both simple and complex queries, a single round of retrieval is performed, and an answer is generated directly from the retrieved documents. Limitation: Works well for simple queries like “When is the birthday of Michael F. Phelps?” but fails for complex queries like “What currency is used in Billy Giles’ birthplace?” due to insufficient intermediate reasoning. This results in inaccurate answers for complex cases. Multi-Step Approach How it Works: Queries, whether simple or complex, go through multiple rounds of retrieval, generating intermediate answers iteratively to refine the final response. Limitation: Though powerful, it introduces unnecessary computational overhead for simple queries. For example, repeatedly processing “When is the birthday of Michael F. Phelps?” is inefficient and redundant. Adaptive Approach How it Works: This approach uses a classifier to determine the query’s complexity and choose the appropriate strategy: Straightforward Query: Directly generate an answer without retrieval (e.g., “Paris is the capital of what?”). Simple Query: Use a single-step retrieval process. Complex Query: Employ multi-step retrieval for iterative reasoning and answer refinement. Advantages Reduces unnecessary overhead for simple queries while ensuring high accuracy for complex ones. Adapts flexibly to a variety of query complexities. Source: Author Adaptive RAG Framework Classifier Role: A smaller language model predicts query complexity. It is trained using automatically labelled datasets, where the labels are derived from past model outcomes and inherent patterns in the data. Dynamic Strategy Selection: For simple or straightforward queries, the framework avoids wasting computational resources. For complex queries, it ensures sufficient iterative reasoning through multiple retrieval steps. RAG System Architecture Flow from LangGraph Here’s another example of an adaptive RAG System architecture flow from LangGraph: Source: Adaptive RAG 1. Query Analysis The process begins with analyzing the user query to determine the most appropriate pathway for retrieving and generating the answer. Step 1: Route Determination The query is classified into categories based on its relevance to the existing index (database or vector store). [Related to Index]: If the query is aligned with the indexed content, it is routed to the RAG module for retrieval and generation. [Unrelated to Index]: If the query is outside the scope of the index, it is routed for a web search or another external knowledge source. Optional Routes: Additional pathways can be added for more specialized scenarios, such as domain-specific tools or external APIs. 2. RAG + Self-Reflection If the query is routed through the RAG module, it undergoes an iterative, self-reflective process to ensure high-quality and accurate responses. Retrieve Node Retrieves documents from the indexed database based on the query. These documents are passed to the next stage for evaluation. Grade Node Assesses the relevance of the retrieved documents. Decision Point: If documents are relevant: Proceed to generate an answer. If documents are irrelevant: The query is rewritten for better retrieval and the process loops back to the retrieve node. Generate Node Generates a response based on the relevant documents. The generated response is evaluated further to ensure accuracy and relevance. Self-Reflection Steps Does it answer the question? If yes: The process ends, and the answer is returned to the user. If no: The query undergoes another iteration, potentially with additional refinements. Hallucinations Check If hallucinations are detected (inaccuracies or made-up facts): The query is rewritten, or additional retrieval is triggered for correction. Re-write Question Node Refines the query for better retrieval results and loops it back into the process. This ensures that the model adapts dynamically to handle edge cases or incomplete data. 3. Web Search for Unrelated Queries If the query is deemed unrelated to the indexed knowledge base during the Query Analysis stage: Generate Node with Web Search: The system directly performs a web search and uses the retrieved data to generate a response. Answer with Web Search: The generated response is delivered directly to the user. In essence, Adaptive RAG is an intelligent and resource-aware framework that improves response quality and computational efficiency by leveraging tailored query strategies. 4. Agentic Corrective RAG A low-quality retriever often introduces significant irrelevant information, hindering generators from accessing accurate knowledge and potentially leading them astray. Source: Corrective Retrieval Augmented Generation Likewise, here are some issues with RAG: Issues with Traditional RAG (Retrieval-Augmented Generation) Low-Quality Retrievers: These can introduce a substantial amount of irrelevant or misleading information. This not only impedes the model’s ability to acquire accurate knowledge but also increases the risk of hallucinations during generation. Undiscriminating Utilization: Many conventional RAG systems indiscriminately incorporate all retrieved documents, irrespective of their relevance. This leads to the integration of unnecessary or incorrect data. Inefficient Document Processing: Current RAG methods often treat complete documents as knowledge sources, even though large portions of retrieved text may be irrelevant, diluting the quality of generation. Dependency on Static Corpora: Retrieval systems that rely on fixed databases can only provide limited or suboptimal documents, failing to adapt to dynamic information needs. Corrective RAG (CRAG) CRAG aims to address the above issues by introducing mechanisms to self-correct retrieval results, enhancing document utilization, and improving generation quality. Key Features: Retrieval Evaluator: A lightweight component to assess the relevance and reliability of retrieved documents for a query. This evaluator assigns a confidence degree to the documents. Triggered Actions: Depending on the confidence score, different retrieval actions—Correct, Ambiguous, or Incorrect—are triggered. Web Searches for Augmentation: Recognizing the limitations of static databases, CRAG integrates large-scale web searches to supplement and improve retrieval results. Decompose-Then-Recompose Algorithm: This method selectively extracts key information from retrieved documents, discarding irrelevant sections to refine the input to the generator. Plug-and-Play Capability: CRAG can seamlessly integrate with existing RAG-based systems without requiring extensive modifications. Corrective RAG Workflow Source: Dipanjan Sarkar Step 1: Retrieval Retrieve context documents from a vector database using the input query. This is the initial step to gather potentially relevant information. Step 2: Relevance Check Use a Large Language Model (LLM) to evaluate whether the retrieved documents are relevant to the input query. This ensures the retrieved documents are appropriate for the question. Step 3: Validation of Relevance If all documents are relevant (Correct), no specific corrective action is required, and the process can proceed to generation. If ambiguity or incorrectness is detected, proceed to Step 4. Step 4: Query Rephrasing and Search If documents are ambiguous or incorrect: Rephrase the query based on insights from the LLM. Conduct a web search or alternative retrieval to fetch updated and accurate context information. Step 5: Response Generation Send the refined query and relevant context documents (corrected or original) to the LLM for generating the final response. The type of response depends on the quality of retrieved or corrected documents: Correct: Use the query with retrieved documents. Ambiguous: Combine original and new context documents. Incorrect: Use the corrected query and newly retrieved documents for generation. This workflow ensures high accuracy in responses through iterative correction and refinement. Agentic Corrective RAG System Workflow The idea is to couple a RAG system with a few checks in place and perform web searches if there is a lack of relevant context documents to the given user query as follows: Source: Dipanjan Sarkar Question: This is the input from the user, which starts the process. Retrieve (Node): The system queries a vector database to retrieve context documents that might answer the user’s question. Grade (Node): A Large Language Model (LLM) evaluates whether the retrieved documents are relevant to the query. If all documents are deemed relevant, the system proceeds to generate an answer. If any document is irrelevant, the system moves to rephrase the query and attempts a web search. Step 1 – Retrieve Node The system retrieves documents from a vector database based on the query, providing context or answers. Step 2 – Grade Node An LLM evaluates document relevance: All relevant: Proceeds to answer generation. Some irrelevant: Flags the issue and refines the query. Branching Scenarios After Grading Step 3A – Generate Answer Node: If all documents are relevant, the LLM generates a quick response. Step 3B – Rewrite Query Node: For irrelevant results, the query is rephrased for better retrieval. Step 3C – Web Search Node: A web search gathers additional context. Step 3D – Generate Answer Node: The refined query and new data are used to generate the answer. We can build this as an agentic RAG system by having a specific functionality step as a node in the graph and using LangGraph to implement it. Key steps in the node will include prompts being sent to LLMs to perform specific tasks as seen in the detailed workflow below: Source: A Comprehensive Guide to Building Agentic RAG Systems with LangGraph The Agentic Corrective RAG Architecture enhances Retrieval-Augmented Generation (RAG) with corrective steps for accurate answers: Query and Initial Retrieval: A user query retrieves context documents from a vector database. Document Evaluation: The LLM Grader Prompt evaluates each document’s relevance (yes or no). Decision Node: All Relevant: Directly proceed to generate the answer. Irrelevant Documents: Trigger corrective steps. Query Rephrasing: The LLM Rephrase Prompt rewrites the query for optimized web retrieval. Additional Retrieval: A web search retrieves improved context documents. Response Generation: The RAG Prompt generates an answer using validated context only. Here’s what the CRAG do in short: Error Correction: This architecture iteratively improves context accuracy by identifying irrelevant documents and retrieving better ones. Agentic Behavior: The system dynamically adjusts its actions (e.g., rephrasing queries, conducting web searches) based on the LLM’s evaluations. Factuality Assurance: By anchoring the generation step to validated context documents, the framework minimizes the risk of hallucinated or incorrect responses. 5. Self-Reflective RAG Self-reflective RAG (Retrieval-Augmented Generation) is an advanced approach in natural language processing (NLP) that combines the capabilities of retrieval-based methods with generative models while adding an additional layer of self-reflection and logical reasoning. For instance, self-reflective RAG helps in retrieval, re-writing questions, discarding irrelevant or hallucinated documents and re-try retrieval. In short, it was introduced to capture the idea of using an LLM to self-correct poor-quality retrieval and/or generations. Key Components of Self Route Decision-making by LLMs: Queries are evaluated to determine if they can be answered with the given retrieved context. Routing: If a query is answerable, response is generated immediately. Otherwise, it is routed to a long-context model with the full context documents to generate the response. Efficiency and Accuracy: This design balances cost-efficiency (avoiding unnecessary computation cost and time) and accuracy (leveraging long-context models only when needed). Key Features of Self-RAG On-Demand Adaptive Retrieval: Unlike traditional RAG methods, which retrieve a fixed set of passages beforehand, SELF-RAG dynamically decides whether retrieval is necessary based on the ongoing generation process. This decision is made using reflection tokens, which act as signals during the generation process. Source: SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION Reflection Tokens: These are special tokens integrated into the LLMs workflow, serving two purposes: Retrieval Tokens: Indicate whether more information is needed from external sources. Critique Tokens: Self-evaluate the generated text to assess quality, relevance, or completeness. By using these tokens, the LLMs can decide when to retrieve and ensure generated text aligns with cited sources. Self-Critique for Quality Assurance: The LLM critiques its own outputs using the generated critique tokens. These tokens validate aspects like relevance, support, or completeness of the generated segments. This mechanism ensures that the final output is not only coherent but also well-supported by retrieved evidence. Controllable and Flexible: Reflection tokens allow the model to adapt its behavior during inference, making it suitable for diverse tasks, such as answering questions requiring retrieval or generating self-contained outputs without retrieval. Improved Performance: By combining dynamic retrieval and self-critique, SELF-RAG surpasses standard RAG models and large language models (LLMs) in generating high-quality outputs that are better supported by evidence. Basic RAG flows involve an LLM generating outputs based on retrieved documents. Advanced RAG approaches, like routing, allow the LLM to select different retrievers based on the query. Self-reflective RAG adds feedback loops, re-generating queries or re-retrieving documents as needed. State machines, ideal for such iterative processes, define steps (e.g., retrieval, query refinement) and transitions, enabling dynamic adjustments like re-querying when retrieved documents are irrelevant. Source: LangGraph The Architecture of Self-reflective RAG Source: Author I have created a Self-Reflective RAG (Retrieval-Augmented Generation) architecture. Here’s the flow and components: The process starts with a Query (shown in green) First Decision Point: “Is Retrieval Needed?” If NO: The query goes directly to the LLM for processing If YES: The system proceeds to retrieval steps Knowledge Base Integration A Knowledge base (shown in purple) connects to the “Retrieval of Relevant Documents” step This retrieval process pulls potentially relevant information to answer the query Relevance Evaluation Retrieved documents go through an “Evaluate Relevance” step Documents are classified as either “Relevant” or “Irrelevant” Irrelevant documents trigger another retrieval attempt Relevant documents are passed to the LLM LLM Processing The LLM (shown in yellow) processes the query along with relevant retrieved information Produces an initial Answer (shown in green) Validation Process The system performs a Hallucination Check: Determines if the generated answer aligns with the provided context (avoiding unsupported or fabricated responses). Self-Reflection The “Critique Generated Response” step (shown in blue) evaluates the answer This is the “Self-Reflective” part of the architecture If the answer isn’t satisfactory, the system can trigger a query rewrite and restart the process Final Output: Once an “Accurate Answer” is generated, it becomes the final Output Grading and Generation Decisions Retrieve Node: Handles the initial retrieval of documents. Grade Documents: Assesses the quality and relevance of the retrieved documents. Transform Query: If no relevant documents are found, the query is adjusted for re-retrieval. Generation Process: Decides whether to generate an answer directly based on the retrieved documents. Uses conditional edges to iteratively refine the answer until it is deemed useful. Workflow of Traditional RAG and Self-Rag Source: SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION Here’s the workflow of both traditional RAG and Self-Rag using the example prompt “How did US states get their names?” Traditional RAG Workflow Step 1 – Retrieve K documents: Retrieve specific documents like: “Of the fifty states, eleven are named after an individual person” “Popular names by states. In Texas, Emma is a popular baby name” “California was named after a fictional island in a Spanish book” Step 2 – Generate with retrieved docs: Takes the original prompt (“How did US states get their names?”) + all retrieved documents The language model generates one response combining everything This can lead to contradictions or mixing unrelated information (like claiming California was named after Christopher Columbus) Self-RAG Workflow Step 1 – Retrieve on demand: Starts with the prompt “How did US states get their names?” Makes initial retrieval about state name sources Step 2 – Generate segments in parallel: Creates multiple independent segments, each with its own: Prompt + Retrieved information Fact verification Examples: Segment 1: Facts about states named after people Segment 2: Information about Texas’s naming Segment 3: Details about California’s name origin Step 3 – Critique and select: Evaluate all generated segments Pick the most accurate/relevant segment Can retrieve additional information if needed Combines verified information into the final response The key improvement is that Self-RAG Breaks down the response into smaller, verifiable pieces Verifies each piece independently Can dynamically retrieve more information when needed Assembles only the verified information into the final response As shown in the bottom example with “Write an essay of your best summer vacation”: Traditional RAG still tries to retrieve documents unnecessarily Self-RAG recognizes no retrieval is needed and generates directly from personal experience. 6. Speculative RAG Speculative RAG is a smart framework designed to make large language models (LLMs) both faster and more accurate when answering questions. It does this by splitting the work between two kinds of language models: A small, specialized model that drafts potential answers quickly. A large, general-purpose model that double-checks these drafts and picks the best one. Source: Author Why Do We Need Speculative RAG? When you ask a question, especially one that needs precise or up-to-date information (like “What are the latest features of the new iPhone?”), regular LLMs often struggle because: They can “hallucinate”: This means they might confidently give answers that are wrong or made up. They rely on outdated knowledge: If the model wasn’t trained on recent data, it can’t help with newer facts. Complex reasoning takes time: If there’s a lot of information to process (like long documents), the model might take forever to respond. That’s where Retrieval-Augmented Generation (RAG) steps in. RAG retrieves real-time, relevant documents (like from a database or search engine) and uses them to generate answers. But here’s the issue: RAG can still be slow and resource-heavy when handling lots of data. Speculative RAG fixes this by adding specialized teamwork: (1) a specialist RAG drafter, and (2) a generalist RAG verifier How Speculative RAG Works? Imagine Speculative RAG as a two-person team solving a puzzle: Step 1: Gather Clues A “retriever” goes out and fetches documents with information related to your question. For example, if you ask, “Who played Doralee Rhodes in the 1980 movie Nine to Five?”, it pulls articles about the movie and maybe the musical. Step 2: Drafting Answers (Small Model) A smaller, faster language model (the specialist drafter) works on these documents. Its job is to: Quickly create multiple drafts of possible answers. Include reasoning for each draft (like saying, “This answer is based on this source”). This model is like a junior detective who quickly sketches out ideas. Step 3: Verifying the Best Answer (Big Model) A larger, more powerful language model (the generalist verifier) steps in next. It: Check each draft for accuracy and relevance. Scores them based on confidence. Pick the best one as the final answer. Think of this model as the senior detective who carefully examines the junior’s work and makes the final call. An Example to Tie it Together Let’s go through an example query: “Who starred as Doralee Rhodes in the 1980 film Nine to Five?” Retrieve Documents: The system finds articles about both the movie (1980) and the musical (2010). Draft Answers (Specialist Drafter): Draft 1: “Dolly Parton played Doralee Rhodes in the 1980 movie Nine to Five.” Draft 2: “Doralee Rhodes is a character in the 2010 musical Nine to Five.” Verify Answers (Generalist Verifier): Draft 1 gets a high score because it matches the movie and the question. Draft 2 gets a low score because it’s about the musical, not the movie. Final Answer: The system confidently outputs: “Dolly Parton played Doralee Rhodes in the 1980 movie Nine to Five.” Why is this Approach Smart? Faster Responses: The smaller model handles the heavy lifting of generating drafts, which speeds things up. More Accurate Answers: The larger model focuses only on reviewing drafts, ensuring high-quality results. Efficient Resource Use: The larger model doesn’t waste time processing unnecessary details—it only verifies. Key Benefits of Speculative RAG Balanced Performance: It’s fast because the small model drafts, and it’s accurate because the big model verifies. Avoids Wasting Effort: Instead of reviewing everything, the big model only checks what the small model suggests. Real-World Applications: Great for answering tough questions that require both reasoning and real-time, up-to-date information. Speculative RAG is like having a smart assistant (the specialist drafter) and a careful editor (the generalist verifier) working together to make sure your answers are not just fast but also spot-on accurate! Standard RAG vs. Self-Reflective RAG vs. Corrective RAG vs. Speculative RAG Source: Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting 1. Standard RAG What it does: It retrieves documents from a knowledge base and directly incorporates them into the generalist LM’s input. Weakness: This approach burdens the generalist LM with both understanding the documents and generating the final answer. It doesn’t differentiate between relevant and irrelevant information. 2. Self-Reflective RAG What it adds: The generalist LM learns to classify whether the retrieved documents are relevant or irrelevant and can tune itself based on those classifications. Weakness: It requires additional instruction-tuning of the generalist LM to handle these classifications and may still produce answers that are less efficient. 3. Corrective RAG What it adds: Uses an external Natural Language Inference (NLI) model to classify documents as Correct, Ambiguous, or Incorrect before incorporating them into the generalist LM’s prompt. Weakness: This adds complexity by introducing an extra NLI step, slowing down the process. 4. Speculative RAG Key Innovation: It divides the task into two parts: A specialist RAG drafter (a smaller LM) rapidly generates multiple drafts and rationales for the answer. The generalist LM evaluates these drafts and selects the best one. Step-by-Step Process: Question Input: When the system receives a knowledge-intensive question, it retrieves relevant documents. Parallel Drafting: The specialist RAG drafter works on subsets of retrieved documents in parallel. Each subset generates: A draft answer (α) An accompanying rationale (β). Verification and Selection: The generalist LM evaluates all the drafts (α1,α2,α3) and their rationales to assign scores. It selects the most confident draft as the final answer. The Speculative RAG framework achieves a perfect balance of speed and accuracy: The small specialist LM does the heavy lifting (drafting answers based on retrieved documents). The large generalist LM ensures the final output is accurate and well-justified. This approach outperforms earlier methods by reducing latency while maintaining state-of-the-art accuracy. Approach How It Works Weakness Speculative RAG Improvement Standard RAG Passes all retrieved documents to the generalist LM directly. Inefficient and prone to irrelevant content. Offloads drafting to a specialist, reducing burden. Self-Reflective RAG LM learns to classify documents as relevant/irrelevant. Requires instruction-tuning, still slow. Specialist LM handles this in parallel without tuning. Corrective RAG Uses Natural Language Inference (NLI) models to classify document correctness. Adds complexity, slows response times. Avoids extra steps; uses drafts for fast evaluation. Speculative RAG Splits drafting (specialist LM) and verifying (generalist LM). None (faster and more accurate). Combines speed, accuracy, and parallel processing. 7. Self Route Agentic RAG Self Route is a design pattern in Agentic RAG systems where Large Language Models (LLMs) play an active role in deciding how a query should be processed. The approach relies on the LLM’s ability to self-reflect and determine whether it can generate an accurate response based on the context provided. If the model decides it cannot generate a reliable response, it routes the query to an alternative method, such as a long-context model, for further processing. This architecture leverages the LLM’s internal calibration for determining answerability to optimize performance and cost. Introduced in Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach, this method combines Retrieval-Augmented Generation (RAG) and Long Context (LC) to achieve cost efficiency while maintaining performance comparable to LC. Self Route utilizes the LLM itself to route queries through self-reflection, operating on the assumption that LLMs are well-calibrated in predicting whether a query is answerable given the provided context. Key components of Self Route: Decision-making by LLMs: Queries are evaluated to determine if they can be answered with the given context. Routing: If a query is answerable, it is processed immediately. Otherwise, it is routed to a long-context model with additional or full context. Efficiency and Accuracy: This design balances cost-efficiency (avoiding unnecessary computation) and accuracy (leveraging long-context models only when needed). Source: Dipanjan Sarkar 1. Standard RAG Flow Input Query and Context Retrieval: A user query is submitted. Relevant context documents are retrieved using a Vector Database, which matches the query with pre-indexed documents. Decision Node: A long context LLM like GPT-4o or Gemini receives the query and the context documents. It uses the LLM Judge Prompt: Prompt: Write UNANSWERABLE if the query cannot be answered based on the provided context else write ANSWERABLE.
Query: <query>
Context Doc: <context> This step determines whether the context is sufficient to answer the query. Outcome: If the query is judged ANSWERABLE, the flow proceeds with Standard RAG Prompt. If UNANSWERABLE, the flow moves to the Long-Context LLM Flow. RAG Prompt (for ANSWERABLE queries): If sufficient context is available, the following prompt is used to generate the response: Given a query and context documents, use only the provided information to answer the query, do not make up answers.
Query: <query>
Context: <context> Answer Generation: The GPT 4o model processes the RAG Prompt and generates the answer based on the provided context. 2. Long-Context LLM Flow Trigger Condition: If the query is judged UNANSWERABLE by the Decision Node, the process switches to the Long-Context LLM Flow. Merging Context Documents: The LLM Judge Prompt identifies the insufficiency in the context, so a merge operation combines multiple related context documents into a single long-context document for better context continuity. Long Context Prompt: The merged document is then used as input to the GPT-4o model with the following prompt: Given a query and this context document, use only the provided information to answer the query, do not makeup answers.
Query: <query>
Context: <long_context> Answer Generation: The GPT 4o model processes the Long Context Prompt and generates a response based on the enriched, merged context. Key Features and Workflow Here are key features and workflow: Dynamic Decision-Making: The architecture evaluates whether the context is sufficient to answer a query dynamically, ensuring that the system adapts based on the input complexity. Two-Tiered Answer Generation: Standard RAG Flow: Handles straightforward queries with sufficient context. Long-Context LLM Flow: Addresses complex queries requiring extensive or combined context. Prompts for Fine-Grained Control: Explicit instructions in the RAG Prompt and Long Context Prompt ensure factuality by restricting the model to the provided context, avoiding hallucination. Scalability with Vector Database: The system scales efficiently by retrieving relevant context from a vector database before making decisions about query processing. Summary The Standard RAG Flow efficiently handles queries with available and sufficient context. The Long-Context LLM Flow extends the capability to handle complex queries by merging multiple documents into a coherent long context. Carefully designed prompts and decision nodes ensure accuracy, context adherence, and adaptability to varying query requirements. Conclusion As the field of Retrieval-Augmented Generation (RAG) advances, Agentic RAG system has emerged as a transformative innovation, blending traditional RAG workflows with the autonomy and adaptability of AI agents. This fusion allows systems to retrieve relevant knowledge dynamically, refine context intelligently, and execute multi-step tasks with precision. From Agentic RAG Routers and Self-Reflective RAG to advanced architectures like Speculative RAG and Self-Route RAG, each approach addresses specific challenges, such as irrelevant retrievals, reasoning errors, or computational inefficiencies. These systems demonstrate significant progress in enhancing accuracy, adaptability, and scalability across diverse applications, including customer support, workflow automation, and research assistance. By integrating generative AI with advanced retrieval mechanisms, Agentic RAG not only enhances efficiency but also sets the stage for future AI innovations. As we move toward 2025, these technologies are poised to redefine how we harness data, automate workflows, and tackle complex problem-solving, making them an essential toolkit for businesses and developers alike. Also, if you are looking for a comprehensive program on AI Agents online, then explore: Agentic AI Pioneer Program Pankaj Singh Hi, I am Pankaj Singh Negi - Senior Content Editor | Passionate about storytelling and crafting compelling narratives that transform ideas into impactful content. I love reading about technology revolutionizing our lifestyle. AdvancedAI AgentsBest of TechRAG Free Courses 4.7 Generative AI - A Way of Life Explore Generative AI for beginners: create text and images, use top AI tools, learn practical skills, and ethics. 4.5 Getting Started with Large Language Models Master Large Language Models (LLMs) with this course, offering clear guidance in NLP and model training made simple. 4.6 Building LLM Applications using Prompt Engineering This free course guides you on building LLM apps, mastering prompt engineering, and developing chatbots with enterprise data. 4.8 Improving Real World RAG Systems: Key Challenges & Practical Solutions Explore practical solutions, advanced retrieval strategies, and agentic RAG systems to improve context, relevance, and accuracy in AI-driven applications. 4.7 Microsoft Excel: Formulas & Functions Master MS Excel for data analysis with key formulas, functions, and LookUp tools in this comprehensive course. Responses From Readers Cancel reply Clear Submit reply Δ Write for us Write, captivate, and earn accolades and rewards for your work Reach a Global Audience Get Expert Feedback Build Your Brand & Audience Cash In on Your Knowledge Join a Thriving Community Level Up Your Data Science Game We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy & Cookies Policy. Show details Accept all cookies Use necessary cookies Powered By Consent Details About Cookies This site uses cookies to ensure that you get the best experience possible. To learn more about how we use cookies, please refer to our Privacy Policy & Cookies Policy. Necessary (2) Necessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies. Analytics Vidhya (4) learn more about analytics vidhya privacy brahmaid It is needed for personalizing the website. Expiry: Session Type: HTTP csrftoken This cookie is used to prevent Cross-site request forgery (often abbreviated as CSRF) attacks of the website Expiry: Session Type: HTTPS Identityid Preserves the login/logout state of users across the whole site. Expiry: Session Type: HTTPS sessionid Preserves users' states across page requests. Expiry: Session Type: HTTPS Google (1) learn more about google privacy g_state Google One-Tap login adds this g_state cookie to set the user status on how they interact with the One-Tap modal. Expiry: 365 days Type: HTTP Statistics (4) Statistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously. Microsoft (7) learn more about microsoft policy MUID Used by Microsoft Clarity, to store and track visits across websites. Expiry: 1 Year Type: HTTP _clck Used by Microsoft Clarity, Persists the Clarity User ID and preferences, unique to that site, on the browser. This ensures that behavior in subsequent visits to the same site will be attributed to the same user ID. Expiry: 1 Year Type: HTTP _clsk Used by Microsoft Clarity, Connects multiple page views by a user into a single Clarity session recording. Expiry: 1 Day Type: HTTP SRM_I Collects user data is specifically adapted to the user or device. The user can also be followed outside of the loaded website, creating a picture of the visitor's behavior. Expiry: 2 Years Type: HTTP SM Use to measure the use of the website for internal analytics Expiry: 1 Years Type: HTTP CLID The cookie is set by embedded Microsoft Clarity scripts. The purpose of this cookie is for heatmap and session recording. Expiry: 1 Year Type: HTTP SRM_B Collected user data is specifically adapted to the user or device. The user can also be followed outside of the loaded website, creating a picture of the visitor's behavior. Expiry: 2 Months Type: HTTP Google (7) learn more about google privacy _gid This cookie is installed by Google Analytics. The cookie is used to store information of how visitors use a website and helps in creating an analytics report of how the website is doing. The data collected includes the number of visitors, the source where they have come from, and the pages visited in an anonymous form. Expiry: 399 Days Type: HTTP _ga_# Used by Google Analytics, to store and count pageviews. Expiry: 399 Days Type: HTTP _gat_# Used by Google Analytics to collect data on the number of times a user has visited the website as well as dates for the first and most recent visit. Expiry: 1 Day Type: HTTP collect Used to send data to Google Analytics about the visitor's device and behavior. Tracks the visitor across devices and marketing channels. Expiry: Session Type: PIXEL AEC cookies ensure that requests within a browsing session are made by the user, and not by other sites. Expiry: 6 Months Type: HTTP G_ENABLED_IDPS use the cookie when customers want to make a referral from their gmail contacts; it helps auth the gmail account. Expiry: 2 Years Type: HTTP test_cookie This cookie is set by DoubleClick (which is owned by Google) to determine if the website visitor's browser supports cookies. Expiry: 1 Year Type: HTTP Webengage (2) Learn more about webengage privacy _we_us this is used to send push notification using webengage. Expiry: 1 Year Type: HTTP WebKlipperAuth used by webenage to track auth of webenagage. Expiry: Session Type: HTTP LinkedIn (16) learn more about linkedin privacy ln_or Linkedin sets this cookie to registers statistical data on users' behavior on the website for internal analytics. Expiry: 1 Day Type: HTTP JSESSIONID Use to maintain an anonymous user session by the server. Expiry: 1 Year Type: HTTP li_rm Used as part of the LinkedIn Remember Me feature and is set when a user clicks Remember Me on the device to make it easier for him or her to sign in to that device. Expiry: 1 Year Type: HTTP AnalyticsSyncHistory Used to store information about the time a sync with the lms_analytics cookie took place for users in the Designated Countries. Expiry: 6 Months Type: HTTP lms_analytics Used to store information about the time a sync with the AnalyticsSyncHistory cookie took place for users in the Designated Countries. Expiry: 6 Months Type: HTTP liap Cookie used for Sign-in with Linkedin and/or to allow for the Linkedin follow feature. Expiry: 6 Months Type: HTTP visit allow for the Linkedin follow feature. Expiry: 1 Year Type: HTTP li_at often used to identify you, including your name, interests, and previous activity. Expiry: 2 Months Type: HTTP s_plt Tracks the time that the previous page took to load Expiry: Session Type: HTTP lang Used to remember a user's language setting to ensure LinkedIn.com displays in the language selected by the user in their settings Expiry: Session Type: HTTP s_tp Tracks percent of page viewed Expiry: Session Type: HTTP AMCV_14215E3D5995C57C0A495C55%40AdobeOrg Indicates the start of a session for Adobe Experience Cloud Expiry: Session Type: HTTP s_pltp Provides page name value (URL) for use by Adobe Analytics Expiry: Session Type: HTTP s_tslv Used to retain and fetch time since last visit in Adobe Analytics Expiry: 6 Months Type: HTTP li_theme Remembers a user's display preference/theme setting Expiry: 6 Months Type: HTTP li_theme_set Remembers which users have updated their display / theme preferences Expiry: 6 Months Type: HTTP Preferences (0) Preference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in. We do not use cookies of this type. Marketing (4) Marketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers. Google (11) learn more about google privacy _gcl_au Used by Google Adsense, to store and track conversions. Expiry: 3 Months Type: HTTP SID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP SAPISID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP __Secure-# Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP APISID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP SSID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP HSID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP DV These cookies are used for the purpose of targeted advertising. Expiry: 6 Hours Type: HTTP NID These cookies are used for the purpose of targeted advertising. Expiry: 1 Month Type: HTTP 1P_JAR These cookies are used to gather website statistics, and track conversion rates. Expiry: 1 Month Type: HTTP OTZ Aggregate analysis of website visitors Expiry: 6 Months Type: HTTP Facebook (2) learn more about facebook privacy _fbp This cookie is set by Facebook to deliver advertisements when they are on Facebook or a digital platform powered by Facebook advertising after visiting this website. Expiry: 4 Months Type: HTTP fr Contains a unique browser and user ID, used for targeted advertising. Expiry: 2 Months Type: HTTP LinkedIn (6) Learn about linkedin policy bscookie Used by LinkedIn to track the use of embedded services. Expiry: 1 Year Type: HTTP lidc Used by LinkedIn for tracking the use of embedded services. Expiry: 1 Day Type: HTTP bcookie Used by LinkedIn to track the use of embedded services. Expiry: 6 Months Type: HTTP aam_uuid Use these cookies to assign a unique ID when users visit a website. Expiry: 6 Months Type: HTTP UserMatchHistory These cookies are set by LinkedIn for advertising purposes, including: tracking visitors so that more relevant ads can be presented, allowing users to use the 'Apply with LinkedIn' or the 'Sign-in with LinkedIn' functions, collecting information about how visitors use the site, etc. Expiry: 6 Months Type: HTTP li_sugr Used to make a probabilistic match of a user's identity outside the Designated Countries Expiry: 90 Days Type: HTTP Microsoft (2) Learn more about microsoft privacy. MR Used to collect information for analytics purposes. Expiry: 1 year Type: HTTP ANONCHK Used to store session ID for a users session to ensure that clicks from adverts on the Bing search engine are verified for reporting purposes and for personalisation Expiry: 1 Day Type: HTTP UnclassNameified (0) UnclassNameified cookies are cookies that we are in the process of classNameifying, together with the providers of individual cookies. We do not use cookies of this type. Cookie declaration last updated on 24/03/2023 by Analytics Vidhya. Cookies are small text files that can be used by websites to make a user's experience more efficient. The law states that we can store cookies on your device if they are strictly necessary for the operation of this site. For all other types of cookies, we need your permission. This site uses different types of cookies. Some cookies are placed by third-party services that appear on our pages. Learn more about who we are, how you can contact us, and how we process personal data in our Privacy Policy. Accept all cookies Use necessary cookies Flagship Courses GenAI Pinnacle Program| AI/ML BlackBelt Courses Free Courses Generative AI| Large Language Models| Building LLM Applications using Prompt Engineering| Building Your first RAG System using LlamaIndex| Stability.AI| MidJourney| Building Production Ready RAG systems using LlamaIndex| Building LLMs for Code| Deep Learning| Python| Microsoft Excel| Machine Learning| Decision Trees| Pandas for Data Analysis| Ensemble Learning| NLP| NLP using Deep Learning| Neural Networks| Loan Prediction Practice Problem| Time Series Forecasting| Tableau| Business Analytics Popular Categories Generative AI| Prompt Engineering| Generative AI Application| News| Technical Guides| AI Tools| Interview Preparation| Research Papers| Success Stories| Quiz| Use Cases| Listicles Generative AI Tools and Techniques GANs| VAEs| Transformers| StyleGAN| Pix2Pix| Autoencoders| GPT| BERT| Word2Vec| LSTM| Attention Mechanisms| Diffusion Models| LLMs| SLMs| StyleGAN| Encoder Decoder Models| Prompt Engineering| LangChain| LlamaIndex| RAG| Fine-tuning| LangChain AI Agent| Multimodal Models| RNNs| DCGAN| ProGAN| Text-to-Image Models| DDPM| Document Question Answering| Imagen| T5 (Text-to-Text Transfer Transformer)| Seq2seq Models| WaveNet| Attention Is All You Need (Transformer Architecture) Popular GenAI Models Llama 3.1| Llama 3| Llama 2| GPT 4o Mini| GPT 4o| GPT 3| Claude 3 Haiku| Claude 3.5 Sonnet| Phi 3.5| Phi 3| Mistral Large 2| Mistral NeMo| Mistral-7b| Gemini 1.5 Pro| Gemini Flash 1.5| Bedrock| Vertex AI| DALL.E| Midjourney| Stable Diffusion Data Science Tools and Techniques Python| R| SQL| Jupyter Notebooks| TensorFlow| Scikit-learn| PyTorch| Tableau| Apache Spark| Matplotlib| Seaborn| Pandas| Hadoop| Docker| Git| Keras| Apache Kafka| AWS| NLP| Random Forest| Computer Vision| Data Visualization| Data Exploration| Big Data| Common Machine Learning Algorithms| Machine Learning Company About Us Contact Us Careers Discover Blogs Expert Sessions Learning Paths Comprehensive Guides Learn Free Courses AI&ML Program GenAI Program Agentic AI Program Engage Community Hackathons Events Podcasts Contribute Become an Author Become a Speaker Become a Mentor Become an Instructor Enterprise Our Offerings Trainings Data Culture AI Newsletter Terms & conditions Refund Policy Privacy Policy Cookies Policy © Analytics Vidhya 2025.All rights reserved. SKIP Continue your learning for FREE Login with Google Login with Email Forgot your password? I accept the Terms and Conditions Receive updates on WhatsApp Enter email address to continue Email address Get OTP Enter OTP sent to Edit Enter the OTP Resend OTP Resend OTP in 45s Verify OTP

Source: https://github.com/cloudbring/ast-kg-rag-code-gen
GitHub - cloudbring/ast-kg-rag-code-gen: Research project leveraging Abstract Syntax Trees and Knowledge Graphs with Retrieval-Augmented Generation to develop an advanced, context-aware code generation system using Language Server Protocol and Large Language Models. Skip to content Navigation Menu Toggle navigation Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} cloudbring / ast-kg-rag-code-gen Public Notifications You must be signed in to change notification settings Fork 1 Star 6 Research project leveraging Abstract Syntax Trees and Knowledge Graphs with Retrieval-Augmented Generation to develop an advanced, context-aware code generation system using Language Server Protocol and Large Language Models. License MIT license 6 stars 1 fork Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 0 Pull requests 0 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights cloudbring/ast-kg-rag-code-gen master Branches Tags Go to file Code Folders and files Name Name Last commit message Last commit date Latest commit History 4 Commits .devcontainer .devcontainer .github .github .vscode .vscode docs docs notebooks notebooks research research src src tests tests .gitignore .gitignore CONTRIBUTING.md CONTRIBUTING.md LICENSE LICENSE README.md README.md requirements.txt requirements.txt View all files Repository files navigation README MIT license AST-KG-RAG Code Generation Project Project Overview This research project aims to leverage Language Servers and Abstract Syntax Trees (ASTs) to create a Knowledge Graph, which will then be used as the basis for a Retrieval-Augmented Generation (RAG) system to power LLM-based code generation. The project combines cutting-edge techniques in natural language processing, code analysis, and machine learning to create a novel approach to AI-assisted programming. Table of Contents Project Goals Technology Stack Project Phases Installation Usage Contributing License Project Goals Develop a system to parse and analyze code using Language Servers and ASTs Create a Knowledge Graph representation of code structures and relationships Implement a RAG system that utilizes the Knowledge Graph for context-aware retrieval Integrate the RAG system with an LLM to generate high-quality, context-aware code Evaluate the system's performance and demonstrate improvements over existing code generation techniques Research Our ongoing research is documented in the research folder. For an overview of our research organization and current topics, please see the research README . Technology Stack 💡 This will evolve over time Programming Language: Python 3.8+ AST Parsing: ast module (Python standard library) Language Server: Python Language Server Knowledge Graph: Neo4j or NetworkX Machine Learning Framework: PyTorch LLM Integration: Hugging Face Transformers Data Processing: Pandas, NumPy Visualization: Matplotlib, Plotly Project Phases Phase 1: Research and Preparation (4-6 weeks) Literature review on Language Servers, ASTs, and Knowledge Graphs Study of RAG systems and LLM-based code generation Tool and library selection Deliverables: Research summary report Annotated bibliography Initial project setup and environment configuration Phase 2: Design and Prototyping (6-8 weeks) Architecture design for AST parsing and Knowledge Graph construction Prototype implementation of AST to Knowledge Graph conversion RAG system architecture design Basic RAG prototype development Deliverables: System architecture document Proof-of-concept implementations Initial test results and performance metrics Phase 3: Implementation (8-10 weeks) Language Server integration and AST parsing implementation Knowledge Graph construction module development RAG system implementation with Knowledge Graph integration LLM integration for code generation Deliverables: Functional AST parsing and Knowledge Graph construction pipeline Integrated RAG system with LLM-based code generation capabilities Comprehensive test suite Phase 4: Testing and Evaluation (4-6 weeks) Development of test cases and evaluation metrics Thorough testing and performance evaluation Results analysis and implementation iteration Deliverables: Test results report Performance analysis document Refined and optimized system implementation Phase 5: Documentation and Presentation (2-3 weeks) Comprehensive documentation writing Final report preparation System demonstration or presentation creation Deliverables: User and developer documentation Final project report Project presentation or demo Installation (To be updated with specific installation instructions as the project progresses) Clone the repository: git clone https://github.com/cloudbring/ast-kg-rag-code-gen.git
cd ast-kg-rag-code-gen
 Set up a virtual environment: python -m venv venv
source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
 Install dependencies: pip install -r requirements.txt
 Set up additional components (e.g., Neo4j database) as required. Usage (To be updated with usage instructions as the project develops) Contributing This is a research project, and contributions are welcome. Please read the CONTRIBUTING.md file for details on our code of conduct and the process for submitting pull requests. License This project is licensed under the MIT License - see the LICENSE.md file for details. About Research project leveraging Abstract Syntax Trees and Knowledge Graphs with Retrieval-Augmented Generation to develop an advanced, context-aware code generation system using Language Server Protocol and Large Language Models. Resources Readme License MIT license Activity Stars 6 stars Watchers 4 watching Forks 1 fork Report repository Releases No releases published Packages 0 No packages published Footer © 2025 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can’t perform that action at this time.

Results for query: How can Abstract Syntax Trees (AST) be used to analyze and improve code search?

Search Engine: Google
Source: https://softwareengineering.stackexchange.com/questions/331253/how-is-a-abstract-syntax-tree-used-to-execute-source-code
programming languages - How is a abstract syntax tree used to execute source code? - Software Engineering Stack Exchange Skip to main content Stack Exchange Network Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers. Visit Stack Exchange Loading… Tour Start here for a quick overview of the site Help Center Detailed answers to any questions you might have Meta Discuss the workings and policies of this site About Us Learn more about Stack Overflow the company, and our products current community Software Engineering help chat Software Engineering Meta your communities Sign up or log in to customize your list. more stack exchange communities company blog Log in Sign up Home Questions Tags Users Companies Unanswered Teams Ask questions, find answers and collaborate at work with Stack Overflow for Teams. Try Teams for free Explore Teams Teams Ask questions, find answers and collaborate at work with Stack Overflow for Teams. Explore Teams Teams Q&A for work Connect and share knowledge within a single location that is structured and easy to search. Learn more about Teams How is a abstract syntax tree used to execute source code? Ask Question Asked 8 years, 4 months ago Modified 8 years, 4 months ago Viewed 5k times 8 After researching how a parser generates an AST, I believe that I can know attempt to create one. Before I started on this project, I began to ponder what I should be done next after creating a AST that represented my language grammar. Despite my research into this topic, I have not surfaced any quality resources explaining what should be done with the AST to execute the source code. Take this example for instance: var = 10 + 2. A parser might create an AST that is similar to this:      =
    / \
  var  +
      / \
     10  2
 What would be done next with the example AST above. Would the parser record the variable and its value? Or does the parser simply generate the AST, and its up to some other program to evaluate the AST. It seems to me that creating a AST is making more work for the rest of the program. Whatever reads over the AST has to keep track of all kinds of statements and scopes. Would it not make more sense to just group your tokens into statements, and execute each statement individually without a AST? Note: My question is not a duplicate of Is an AST enough to build any translator?. The OP of that question is asking if a AST is enough to implement any language feature?. I'm asking how one would execute the source code of a language from an AST?. some parts of the post may be similar, but the overall questions of each one are very different. programming-languages parsing Share Improve this question Follow edited Apr 12, 2017 at 7:31 CommunityBot 1 asked Sep 16, 2016 at 17:33 ChrisChris 2,84011 gold badge2424 silver badges3838 bronze badges 0 Add a comment | 2 Answers 2 Sorted by: Reset to default Highest score (default) Date modified (newest first) Date created (oldest first) 4 The parser just dissects the code into logically meaningful parts. It is then up to a following processing step to do something with the parse result. The following step (in your case a code interpreter) can focus on the semantics. The parser did all the dirty work, its result is clean and simple. So it does make things easier. Note that you start out with a piece of text (one string or a collection of lines) and the parser gives you an object model. It seems you have no clear picture of an AST, of what that means in terms of an implementation in code. So let's get into that. An execution engine would read the tree and find one object: an assignnent. The assignment class contains two properties: left and right. Left is a variable object, right is an expression object. The expression object is a tree too which is basically a bunch of objects that contain other objects. These are typically processed recursively, as the processor hits the first object in the tree it does not know or care how deep the tree is, it just processes that one object and delegates the processing of contained objects to the same code. When that code hits a leaf (an object that is not complex, that has no contained objects) it performs an action and/or returns a result. Then everything bubbles back up the call stack until the top object gets its result and can do its thing, in your example add 10 and 2. So what makes it easy is the ability to walk down a nested collection of objects and solve it recursively. Parsing text and acting on it in one go can get messy soon and may produce half-baked results because there may be syntax errors further down and you already started executing. Share Improve this answer Follow edited Sep 18, 2016 at 16:48 answered Sep 17, 2016 at 6:16 Martin MaatMartin Maat 18.6k33 gold badges3232 silver badges5858 bronze badges Add a comment | 3 Would the parser record the variable and its value? No. The parser just takes the tokens and arranges them into an AST (possibly returning errors). Would it not make more sense to just group your tokens into statements, and execute each statement individually without a AST? Not really. The AST lets you break down your program into its constituent parts, and then lets you implement those tiny, simple parts in a clear, error-free manner. Better yet, it does it in a very agnostic way. If you want to take that AST and do some static analysis on it, you can. If you want to take that AST and run it through an interpreter, the interpreter doesn't have to worry about associativity, or if the statements parse properly, or what that identifier really refers to. If you want to take that AST and compile it against some new processor, you can since the tiny nodes of an AST likely align with the new assembly of that processor. You can avoid building an AST when making an interpreter or compiler, but I would not recommend it. And how you turn that into something that is runnable is up to you, but you will find it far easier to implement that if your parser output is in a simple tree form. Share Improve this answer Follow answered Sep 16, 2016 at 17:42 TelastynTelastyn 110k2929 gold badges245245 silver badges373373 bronze badges 1 1 Also, an optimizing compiler can perform transformations on the AST that result in fast/smaller/less-memory-intensive code. Performing those transformations directly on source code, generated assembly or machine code is possible, but generally speaking harder and more resource intensive. – Iker Commented Sep 17, 2016 at 10:04 Add a comment | Your Answer Reminder: Answers generated by artificial intelligence tools are not allowed on Software Engineering Stack Exchange. Learn more Thanks for contributing an answer to Software Engineering Stack Exchange! Please be sure to answer the question. Provide details and share your research! But avoid … Asking for help, clarification, or responding to other answers. Making statements based on opinion; back them up with references or personal experience. To learn more, see our tips on writing great answers. Draft saved Draft discarded Sign up or log in Sign up using Google Sign up using Email and Password Submit Post as a guest Name Email Required, but never shown Post as a guest Name Email Required, but never shown Post Your Answer Discard By clicking “Post Your Answer”, you agree to our terms of service and acknowledge you have read our privacy policy. Not the answer you're looking for? Browse other questions tagged programming-languages parsing or ask your own question. The Overflow Blog Community Products Roadmap Update, January 2025 Will the web ever be the primary delivery system for 3D games? Featured on Meta Announcing Jody Bailey as Chief Product and Technology Officer (Feb 2025) AI-generated Answers experiment on Stack Exchange sites that volunteered to... Linked 2 Do bytecode compilers compile syntax directly or intermediate assembly language? 1 Is an AST enough to build any translator? Related 13 How should I specify a grammar for a parser? 5 Programming Language Parser (in Java) - What would be a better design alternative for a special case? 26 Is it possible to statically predict when to deallocate memory---from source code only? 24 In which process does syntax error occur? (tokenizing or parsing) 5 Right way to read from token list to implement parse tree 7 Proper separation between lexing and parsing Hot Network Questions Local accuracy of the daily precipitation forecast How can I slice multiple items at once on a mandolin An AsyncStreamWriter that allows to asynchronously write to a Stream How to respond to evaluation comment that I have a brash personality? Bad letter of recommendation although good thesis Why was I not allowed to board a flight to Cyprus? Performance consideration for cascading style sheet implementation How would a spacecraft figure out its own orbit, without help from earth? Do any official rules govern the construction/creation of, or improving of, a wizard's tower? Unable to import a polynomial from Mathematica to Maple Capacitors on Input Side of Two Regulators Terminology used with fire-fighting aircraft Why `\allowbreak` not working The uses of Transmutation magic in the Metalworking Industry "The server you are trying to connect to does not seem to be a WMS server. Please check the URL" error when using Ordnance Survey's API in QGIS Why would a near future, interstellar empire still have military units/divisions using ‘antique’ weapons (Cavalry, bolt-action rifles, long bayonets)? How do you use small details (like birthdays and names) to flesh out your characters? Why does the ninth amendment carry so much less weight than the other bill of rights amendments? Film about a woman who is murdered and rebuilt as a cyborg What is this large-leaf plant in the Philippines? Make 2026 using digits: 2, 0, 2, 5 Do Emmanuel Goldstein and The Brotherhood exist? Where to publish unsuccessful research projects? Using savings to buy a house to move into before divorce process is initiated more hot questions Question feed Subscribe to RSS Question feed To subscribe to this RSS feed, copy and paste this URL into your RSS reader. Software Engineering Tour Help Chat Contact Feedback Company Stack Overflow Teams Advertising Talent About Press Legal Privacy Policy Terms of Service Cookie Settings Cookie Policy Stack Exchange Network Technology Culture & recreation Life & arts Science Professional Business API Data Blog Facebook Twitter LinkedIn Instagram Site design / logo © 2025 Stack Exchange Inc; user contributions licensed under CC BY-SA . rev 2025.2.4.22037

Source: https://dbshisode.medium.com/unlocking-code-improvement-with-abstract-syntax-trees-asts-8b63de0b5d3d
Unlocking Code Improvement with Abstract Syntax Trees (ASTs) | by Deepak Shisode | Medium Open in app Sign up Sign in Write Sign up Sign in Unlocking Code Improvement with Abstract Syntax Trees (ASTs) Deepak Shisode · Follow 4 min read · Oct 10, 2024 -- Listen Share An Abstract Syntax Tree (AST) is a tree representation of the abstract syntactic structure of source code written in a programming language. Each node of the tree denotes a construct occurring in the source code. ASTs are compelling tools for code analysis, transformation, and optimization. By manipulating the AST of code, developers can automate code improvements, enforce coding standards, and even refactor code programmatically. In this answer, we’ll explore practical examples of using ASTs to improve code, focusing on: Code Analysis and Linting Code Refactoring Code Optimization Automated Testing and Debugging We’ll use Python for code examples, leveraging its built-in ast module, but the concepts apply to other programming languages with AST support. 1. Code Analysis and Linting Use Case: Detecting anti-patterns or enforcing code style guidelines. Example: Identifying unused imports or variables in Python code. Solution: Here’s how you can use AST to analyze code and find unused imports. import ast from collections import defaultdict    class ImportUsageVisitor(ast.NodeVisitor):      def __init__(self):          self.imports = {}          self.used_names = set()        def visit_Import(self, node):          for alias in node.names:              self.imports[alias.asname or alias.name] = node.lineno        def visit_ImportFrom(self, node):          module = node.module          for alias in node.names:              name = alias.asname or alias.name              full_name = f"{module}.{name}" if module else name              self.imports[name] = node.lineno        def visit_Name(self, node):          self.used_names.add(node.id)    def find_unused_imports(code):      tree = ast.parse(code)      visitor = ImportUsageVisitor()      visitor.visit(tree)      unused = set(visitor.imports.keys()) - visitor.used_names      for name in unused:          lineno = visitor.imports[name]          print(f"Unused import '{name}' at line {lineno}")    # Example usage:  code = """  import os  import sys  from collections import defaultdict    def foo():      print(os.name)  """    find_unused_imports(code) Output: Unused import 'defaultdict' at line 4 Unused import 'sys' at line 3 Explanation: We parse the code into an AST. ImportUsageVisitor collects all imported names and tracks where they are used. After traversal, we compare the imported names with the used names to find unused imports. 2. Code Refactoring Use Case: Automating repetitive refactoring tasks. Example: Renaming variables across a codebase while ensuring scope correctness. Solution: pip install astor import ast  import astor  # Third-party library to convert AST back to code    class VariableRenamer(ast.NodeTransformer):      def __init__(self, old_name, new_name):          self.old_name = old_name          self.new_name = new_name        def visit_Name(self, node):          if node.id == self.old_name:              return ast.copy_location(ast.Name(id=self.new_name, ctx=node.ctx), node)          return node    code = """  def compute_area(radius):      pi = 3.1415      area = pi * radius ** 2      return area  """    tree = ast.parse(code)  renamer = VariableRenamer('radius', 'r')  modified_tree = renamer.visit(tree)  modified_code = astor.to_source(modified_tree)  print(modified_code) Output: def compute_area(radius):     pi = 3.1415     area = pi * r ** 2     return area Explanation: VariableRenamer walks the AST and replaces instances of old_name with new_name. By using AST, we ensure that only variable names are changed, not strings or comments. 3. Code Optimization Use Case: Simplifying constant expressions to improve performance. Example: Folding constant expressions at compile time. Solution: import ast    class ConstantFolder(ast.NodeTransformer):      def visit_BinOp(self, node):          self.generic_visit(node)          if (isinstance(node.left, ast.Constant) and              isinstance(node.right, ast.Constant)):              result = eval(compile(ast.Expression(node), '', 'eval'))              return ast.copy_location(ast.Constant(value=result), node)          return node    code = """  def calculations():      x = 2 * 3      y = 4 + x      return y  """    tree = ast.parse(code)  folder = ConstantFolder()  optimized_tree = folder.visit(tree)  optimized_code = ast.unparse(optimized_tree)  print(optimized_code) Output: def calculations():     x = 6     y = 4 + x     return y Explanation: ConstantFolder evaluates binary operations with constant operands and replaces them with the result. This can reduce runtime computation, shifting it to compile time. 4. Automated Testing and Debugging Use Case: Instrumenting code to track execution or inject test hooks. Example: Adding logging statements before function calls. Solution: import ast    class FunctionCallLogger(ast.NodeTransformer):      def visit_Call(self, node):          self.generic_visit(node)          log_call = ast.Expr(value=ast.Call(              func=ast.Name(id='print', ctx=ast.Load()),              args=[ast.Constant(value=f"Calling function {ast.unparse(node.func)}")],              keywords=[]          ))          return [log_call, node]    code = """  def add(a, b):      return a + b    def multiply(x, y):      return x * y    result = multiply(add(2, 3), 4)  """    tree = ast.parse(code)  logger = FunctionCallLogger()  instrumented_tree = logger.visit(tree) instrumented_code = ast.unparse(instrumented_tree) print(instrumented_code) Output: def add(a, b):     return a + b def multiply(x, y):     return x * y result = print('Calling function multiply')multiply( print('Calling function add'), add(2, 3), 4) Explanation: FunctionCallLogger inserts a print statement before each function call. This can help in debugging by tracing function calls during execution. Additional Applications Security Analysis: Detecting vulnerable patterns, such as SQL injections or insecure function use. Code Formatters: Tools like Black (for Python) use ASTs to format code consistently. Language Transpilation: Converting code from one programming language to another by translating ASTs. Benefits of Using AST Language-Aware Transformation: AST manipulations understand the syntax and structure, reducing errors that occur with regex or string-based transformations. Safety and Accuracy: By operating on the AST, you avoid altering code in comments, strings, or unintended places. Automation: ASTs enable programmatic code transformations, which are scalable across large codebases. Differences Between AST Manipulation and Reflection Does AST Use Reflection Techniques? No, working with an Abstract Syntax Tree (AST) does not inherently involve using reflection techniques. While both AST manipulation and reflection allow a program to examine aspects of its own code, they operate at different stages and have different purposes in programming. Abstract Syntax Trees provide a powerful way to introspect and manipulate code programmatically. By leveraging ASTs, developers can automate code improvements, enforce standards, and optimize performance. Whether you’re building a linter, an automatic refactoring tool, or performing static analysis, ASTs are an indispensable tool in a developer’s toolkit Refection Abstract Syntax Tree Code Improvement Code Analysis Linting -- -- Follow Written by Deepak Shisode 1 Follower ·1 Following Follow No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams

Source: https://www.twilio.com/en-us/blog/abstract-syntax-trees
ASTs - What are they and how to use them | Twilio English English Support Help Center Talk to Support Training Support Plans Partner Solutions Login Console Twilio logo Products Communications Twilio Messaging logo Messaging Send and receive multichannel text and media messages in 180+ countries SMS WhatsApp Conversations RCS Voice Make, receive, and monitor calls around the world PSTN SIP Trunking SDK Twilio Verify logo Verify Globally-managed multichannel 2FA and passwordless authentication Twilio Lookup logo Lookup Know your customer and assess identity risk with real-time phone intelligence Twilio SendGrid Email API logo Email Deliver email messages reliably at scale with SendGrid Twilio Task Router logo Serverless Build, deploy, and run apps with Twilio’s serverless environment and visual builder Twilio Phone Numbers logo Phone Numbers Access local, national, and toll-free phone numbers Toll-Free 10DLC Short Codes Twilio Video logo Video Connect virtually with secure, high-quality video calling at global scale Applications Twilio Flex logo Twilio Flex Digital engagement center for customer service and sales Contact center Twilio Engage logo Twilio Engage Create multi-channel campaigns with a unified data-first platform Twilio Marketing Campaigns logo Marketing Campaigns Create and manage email marketing campaigns with SendGrid Customer Data Twilio Segment logo Twilio Segment ↗ Unify your customer data with Segment CDP to power personalized engagement Connections ↗ Integrate web and mobile app data with a single API Reverse ETL Cloud with an upload download symbol Protocols ↗ Protect the integrity of your data Unify ↗ Unify customer data across every channel Profiles Sync Unified Profiles Journeys ↗ Orchestrate multichannel customer journeys See all products Solutions Use Cases Verification & identity Add user verification to protect against fraud. Fraud prevention Alerts & notifications Deliver notifications across text, email, voice. Appointment reminders Lead alerts Mass texting Marketing & promotions Create multichannel marketing campaigns. SMS marketing Customer surveys Contact Center Provide personalized, data-driven service. IVR & self-service Set up a modern IVR with AI-enabled support. Call tracking Teams Marketing Keep customers engaged on any channel Product & Engineering Connect with target users reliably and securely Customer Experience Deliver personalized sales and support experiences at scale Developers Tutorials and docs to start building on Twilio Industries Financial Services Healthcare Hospitality Public Sector Real Estate Retail See all solutions People shaking hands Partners Find a partner Work with a Twilio partner to buy or build the right solution Partner with us Join our Partner Program to build or sell Twilio solutions to your customers. Developers An Open book Documentation Twilio Flex SMS Voice Email WhatsApp API Elastic SIP Trunking Verify Functions Studio Twilio CLI See all docs > Two users in a circle Community Ahoy! Get inspired by the latest from our developer community Developer Blog Read tutorials, community projects, and product updates Stack Overflow Ask the Twilio community for help Champions Become a Twilio community Champion Wrench and screwdriver Tools & Resources CodeExchange Usable code samples for faster builds Changelog See updates and additions to Twilio products API Status Check real-time monitoring of APIs and all services Beta Products and Features Explore new launches and request access to early releases Why Twilio Company Why Twilio The trusted platform for data-driven customer engagement across any channel. About Us Twilio company page Twilio Magic Twilio core values Diversity Diversity, equity, and inclusion Investor Relations Information for Twilio investors Press & Media Latest announcements and launches Our Platform Customer Engagement Platform Build durable customer relationships with personalized, data-driven engagement Communication Customer Data Trust Center Twilio’s security, data privacy, regulatory compliance, and resiliency policies. Super Network Simplify, strengthen and unite carrier communication networks using software. Resources Resource Center Browse our content library for more resources on how you can create lasting customer relationships Customer Stories See what customers are building with Twilio Blog Explore the latest product updates, technical tutorials, and best practices State of Customer Engagement See the top trends shaping customer engagement in our annual report Twilio Events Join upcoming group events Pricing Communications Twilio Messaging logo Messaging SMS Whatsapp Conversations Twilio SendGrid Email API logo Email ↗ Twilio Programmable Voice logo Voice Conferencing SIP trunking Twilio Verify logo Verify Twilio Lookup logo Lookup Serverless Twilio Video logo Video Applications Twilio Flex logo Twilio Flex Twilio Engage logo Twilio Segment ↗ Twilio Marketing Campaigns logo Marketing Campaigns See all pricing Data is collected based on search terms. Privacy Policy Contact sales Start for free Menu Data is collected based on search terms. Privacy Policy Products Communications Twilio Messaging logo Messaging Send and receive multichannel text and media messages in 180+ countries SMS WhatsApp Conversations RCS Voice Make, receive, and monitor calls around the world PSTN SIP Trunking SDK Twilio Verify logo Verify Globally-managed multichannel 2FA and passwordless authentication Twilio Lookup logo Lookup Know your customer and assess identity risk with real-time phone intelligence Twilio SendGrid Email API logo Email Deliver email messages reliably at scale with SendGrid Twilio Task Router logo Serverless Build, deploy, and run apps with Twilio’s serverless environment and visual builder Twilio Phone Numbers logo Phone Numbers Access local, national, and toll-free phone numbers Toll-Free 10DLC Short Codes Twilio Video logo Video Connect virtually with secure, high-quality video calling at global scale Applications Twilio Flex logo Twilio Flex Digital engagement center for customer service and sales Contact center Twilio Engage logo Twilio Engage Create multi-channel campaigns with a unified data-first platform Twilio Marketing Campaigns logo Marketing Campaigns Create and manage email marketing campaigns with SendGrid Customer Data Twilio Segment logo Twilio Segment ↗ Unify your customer data with Segment CDP to power personalized engagement Connections ↗ Integrate web and mobile app data with a single API Reverse ETL Cloud with an upload download symbol Protocols ↗ Protect the integrity of your data Unify ↗ Unify customer data across every channel Profiles Sync Unified Profiles Journeys ↗ Orchestrate multichannel customer journeys See all products Solutions Use Cases Verification & identity Add user verification to protect against fraud. Fraud prevention Alerts & notifications Deliver notifications across text, email, voice. Appointment reminders Lead alerts Mass texting Marketing & promotions Create multichannel marketing campaigns. SMS marketing Customer surveys Contact Center Provide personalized, data-driven service. IVR & self-service Set up a modern IVR with AI-enabled support. Call tracking Teams Marketing Keep customers engaged on any channel Product & Engineering Connect with target users reliably and securely Customer Experience Deliver personalized sales and support experiences at scale Developers Tutorials and docs to start building on Twilio Industries Financial Services Healthcare Hospitality Public Sector Real Estate Retail See all solutions People shaking hands Partners Find a partner Work with a Twilio partner to buy or build the right solution Partner with us Join our Partner Program to build or sell Twilio solutions to your customers. Developers An Open book Documentation Twilio Flex SMS Voice Email WhatsApp API Elastic SIP Trunking Verify Functions Studio Twilio CLI See all docs > Two users in a circle Community Ahoy! Get inspired by the latest from our developer community Developer Blog Read tutorials, community projects, and product updates Stack Overflow Ask the Twilio community for help Champions Become a Twilio community Champion Wrench and screwdriver Tools & Resources CodeExchange Usable code samples for faster builds Changelog See updates and additions to Twilio products API Status Check real-time monitoring of APIs and all services Beta Products and Features Explore new launches and request access to early releases Why Twilio Company Why Twilio The trusted platform for data-driven customer engagement across any channel. About Us Twilio company page Twilio Magic Twilio core values Diversity Diversity, equity, and inclusion Investor Relations Information for Twilio investors Press & Media Latest announcements and launches Our Platform Customer Engagement Platform Build durable customer relationships with personalized, data-driven engagement Communication Customer Data Trust Center Twilio’s security, data privacy, regulatory compliance, and resiliency policies. Super Network Simplify, strengthen and unite carrier communication networks using software. Resources Resource Center Browse our content library for more resources on how you can create lasting customer relationships Customer Stories See what customers are building with Twilio Blog Explore the latest product updates, technical tutorials, and best practices State of Customer Engagement See the top trends shaping customer engagement in our annual report Twilio Events Join upcoming group events Pricing Communications Twilio Messaging logo Messaging SMS Whatsapp Conversations Twilio SendGrid Email API logo Email ↗ Twilio Programmable Voice logo Voice Conferencing SIP trunking Twilio Verify logo Verify Twilio Lookup logo Lookup Serverless Twilio Video logo Video Applications Twilio Flex logo Twilio Flex Twilio Engage logo Twilio Segment ↗ Twilio Marketing Campaigns logo Marketing Campaigns See all pricing Start for free Login Console Contact sales Twilio Blog Overview Developers Digital Insights Company ASTs - What are they and how to use them Blog / Company / ASTs - What are they and how to use them Tags JavaScript Start for free Time to read: 8 minutes Share: Facebook logo Twitter Logo Follow us on Twitter LinkedIn logo June 11, 2020 Written by Dominik Kundel Twilion Reviewed by Tilde Thurium Twilion Diane Phan Twilion If you are writing code, chances are that ASTs are involved in your development flow. AST stands for Abstract Syntax Tree and they power a lot of parts of your development flow. Some people might have heard about them in the context of compilers but they are being used in a variety of tools. Even if you don't write general development tools, ASTs can be a useful thing to have in your toolbelt. In this post we'll talk about what ASTs are, where they are used and how you can leverage them. What is AST? Abstract Syntax Trees or ASTs are tree representations of code. They are a fundamental part of the way a compiler works. When a compiler transforms some code there are fundamentally the following steps: Lexical Analysis Syntax Analysis Code Generation Visualizsation by https://twitter.com/@_lrlna Lexical Analysis aka Tokenization During this step, code that you wrote is going to be converted into a set of tokens describing the different parts of your code. This is fundamentally the same method that basic syntax highlighting is using. Tokens don't understand how things fit together and purely focuses on components of a file. You can imagine it as a list or array of different types of tokens. You can imagine this as if you'd take a text and you break it into words. You might be able to make a differentiation between punctuation, verbs, nouns, numbers, etc. but at this stage you don't have any deeper understanding of what's part of a sentence or how sentences fit together. Syntax Analysis aka Parsing This is the step where we turn our list of tokens into an Abstract Syntax Tree. It converts our tokens into a tree that represents the actual structure of the code. Where previously in the tokens we only had a pair of () we now have an idea of whether it's a function call, a function definition, a grouping or something else. The equivalent here would be turning our list of words into a data structure representing things such as sentences, what role a certain noun plays in a sentence or whether we are in a listing. Another example that this can be compared to is the DOM. Where the previous step would just break down the HTML into "tags" and "text", this step would generate the hierarchy represented as the DOM. One thing to note is that there is no "one" AST format. They might differ both depending on the language you are turning into an AST as well as the tool you are using for the parsing. In JavaScript a common standard is ESTree but you'll see that different tools might add additional properties. Json Copy Code {
 "type": "Program",
 "start": 0,
 "end": 14,
 "body": [
   {
     "type": "ExpressionStatement",
     "start": 0,
     "end": 14,
     "expression": {
       "type": "CallExpression",
       "start": 0,
       "end": 13,
       "callee": {
         "type": "Identifier",
         "start": 0,
         "end": 7,
         "name": "isPanda"
       },
       "arguments": [
         {
           "type": "Literal",
           "start": 8,
           "end": 12,
           "value": "🐼",
           "raw": "'🐼'"
         }
       ]
     }
   }
 ],
 "sourceType": "module"
} In general an AST is a tree structure where every node has at least a type specifying what it is representing. For example a type could be a Literal that represents an actual value or a CallExpression that represents a function call. The Literal node might only contain a value while the CallExpression node contains a lot of additional information that might be relevant like "what is being called" (callee) or what are the arguments that are being passed in. Code Generation This step can be multiple steps by itself. Once we have an Abstract Syntax Tree we can both manipulate it as well as "print" it into a different type of code. Using ASTs to manipulate code is safer than doing those operations directly on the code as text or on a list of tokens. Manipulating text is always dangerous; it shows the least amount of context. If you ever tried to manipulate text using string replacements or regular expressions you probably noticed how easy it is to get it wrong. Even manipulating tokens isn't easy. While we might know what a variable is, if we would want to rename it we would have no insight into things like the scope of the variable or any variables it might clash with. The AST provides enough information about the structure of code that we can modify it with more confidence. We could for example determine where a variable is declared and know exactly which part of the program this affects due to the tree structure. Once we have manipulated the tree we can then print the tree to output whatever code output is expected. For example if we would build a compiler like the TypeScript compiler we'd output JavaScript while another compiler might output machine code. Again this is more easily achieved with an AST because different outputs might have different formats for the same structure. It would be more difficult to generate the output with a more linear input like text or a list of tokens. What to do with ASTs? With the theory covered what are actually some real life use cases for ASTs? We talked about compilers but we are not all building compilers all day. The use cases for ASTs are wide and can generally be split into three overarching actions: reading, modifying and printing. They are sort of additive, meaning if you are printing ASTs, chances are high that you previously also read an AST and modified it. But we'll cover examples for each of these that primarily focuses on one use case. In each of these sections we'll also cover ways how you can perform the respective action. Since I'm a JavaScript developer all of the examples will be in JavaScript but basically every programming language has similar tools and libraries. Some of the examples covered below have a demo link included that links to astexplorer.net. It's a great tool to dip your feet into working ASTs. The tool supports a variety of different languages as well as parsing and transformation tools that might differ between the different examples. Both the parser and the transformation tool will be listed at the top of the screen. Reading/Traversing ASTs Technically the first step of working with ASTs is parsing text to create an AST but in most cases the libraries that offer the parsing step also offer a way to traverse the AST. Traversing an AST means visiting the different nodes of the tree to gain insights or perform actions. One of the most common use cases for this is linting. ESLint for example uses espree to generate an AST and if you want to write any custom rules you'll write those based on the different AST nodes. The ESLint docs have extensive documentation on how you can build custom rules, plugins and formatters. If you want to see an abbreviated example here is a code example in the AST explorer that establishes a rule that you can only have one class in your file. JavaScript Copy Code let numberOfClasses = 0;

export default function (context) {
  return {
    ClassDeclaration(node) {
      numberOfClasses = numberOfClasses + 1;
      if (numberOfClasses > 1) {
        context.report({
          node,
          message: "You shouldn't use more than one class",
        });
      }
    },
  };
} In this code snippet we are looking for ClassDeclaration nodes and every time increase a global counter. Once we reach a certain limit we use ESLint's reporter API to raise a complaint. Now this is a very ESLint specific syntax but you could build a similar script without building an ESLint plugin. We could for example use the underlying espree library to do the parsing and traversing of nodes manually using a basic Node.js script. JavaScript Copy Code const fs = require('fs').promises;
const path = require('path');
const espree = require('espree');

function checkTopLevelClasses(ast) {
 let topLevelClassCounter = ast.body.reduce((counter, node) => {
   if (node.type === 'ClassDeclaration') {
     counter++;
   }
   return counter;
 }, 0);

 if (topLevelClassCounter > 1) {
   throw new Error(
     `Found ${topLevelClassCounter} top level classes. Expected not more than one.`
   );
 }
}

async function run() {
 const fileName = path.resolve(process.cwd(), process.argv[2]);
 const content = await fs.readFile(fileName, 'utf8');
 console.log(fileName);

 const ast = espree.parse(content, { ecmaVersion: 2019 });
 checkTopLevelClasses(ast);
}

run().catch(console.error); This script manually reads a file, parses it using espree and then checks every top-level node and whether it is a ClassDeclaration at which point it will increase a local counter. Once it's done it checks if the count is bigger than expected and will throw an error. As you can see it involves quite the additional code compared to our previous example. Plugins for existing linters are certainly the more code efficient solutions but sometimes you want to have more control or want to ship a linter independent tool. If you search npm you'll also find a collection of other tools to parse and traverse ASTs. They typically differ in API design and occasionally in JavaScript parsing capabilities. Some common examples are acorn and esprima for parsing or estree-walker for traversing ESTree compatible trees. Modifying/Transforming ASTs As previously covered, having an AST makes modifying said tree easier and safer than modifying the code as tokens or a raw string. There's a wide variety of reasons why you might want to modify some code using ASTs. Babel for example modifies ASTs to down-transpile newer features or to turn JSX into function calls. Which is what happens when you compile your React or Preact code for example. Another use case would be bundling code. In the world of modules, bundling code is often much more complex than just appending files together. Having a better understanding of the structure of the respective files makes it easier to merge those files and adjust imports and function calls where necessary. If you check out the codebases of tools such as webpack, parcel or rollup you'll find that all of them use ASTs as part of their bundling workflow. A use case that might seem less obvious is test coverage. Have you ever wondered how code coverage tools such as Istanbul determine your test coverage? I certainly did. The tl;dr is that they inject additional code that is increasing different counters for every line, function and statement. That after all your tests ran they can inspect said counters and give you a detailed insight in what has been executed and what hasn't. Doing this without ASTs is both incredibly difficult as well as less predictable. I found this article explaining what's happening under the hood of test coverage tools that you can check out if you want to learn more. Now all of these are tools that you likely won't write a lot yourself. But there is one use case that can be beneficial for your regular development flow. And that's doing code modifications for optimization, macros or to update larger parts of your code base all at once. The React team for example maintains a collection of scripts called react-codemod that can perform common operations related to updating your React version. The tool they use under the hood is called jscodeshift and we can use that to write our own transformation scripts as well. Let's say we for example love debugging using alert() but we want to avoid shipping that to our customers. We could write a script such as the following to replace all calls to alert with console.error without worrying that we might override something like myalert(). JavaScript Copy Code export default function transformer(file, api) {
 const j = api.jscodeshift;

 return j(file.source)
   .find(j.CallExpression)
   .forEach((path) => {
     const callee = path.node.callee;
     console.log(callee);
     if (callee.type !== 'Identifier' || callee.name !== 'alert') {
       return;
     }
     j(path).replaceWith(
       j.callExpression(j.identifier('console.error'), path.node.arguments)
     );
   })
   .toSource();
} If you want to check out this example, head over to AST Explorer. You can also use the same skill set to build for example your own Babel or ESLint plugins that will perform code fixes for you. Printing ASTs In most cases printing and modifying ASTs go hand in hand since you'll have to output the AST you just modified. But while some libraries like recast explicitly focus on printing the AST in the same code style as the original, there's also a variety of use cases where you want to explicitly print your AST differently. Prettier for example uses ASTs to reformat your code according to your configuration without changing the content/meaning of your code. The way they do it is by converting your code into an AST which is completely formatting agnostic and then rewriting it based on your rules. Common other use cases would be to print code in a different target language or build your own minification tool. There's a couple of different tools you can use to print ASTs such as escodegen or astring. You could also go all in and build your own formatter depending on your use case or build a plugin for Prettier. What's next? We barely scratched the surface of awesome things you can build with ASTs. One of my favorite examples we didn't cover is Esper.JS. It was developed by the folks at CodeCombat to allow people to write Python and other languages in the browser and execute them on a step-by-step basis for educational purposes. All of that is heavily powered by parsing the different languages into an AST that's compatible with a JavaScript AST and then modifying it accordingly for execution. While ASTs might be things most developers don't work with on a daily basis I believe they are a useful tool to have in your toolbelt. What are some use cases of ASTs that you have seen that are not covered here? I'd love to hear about them! Twitter: @dkundel Email: dkundel@twilio.com GitHub: dkundel Web: dkundel.com Related Posts Creating a Recipe Recommendation Chatbot with Ollama and Twilio Carlos Mucuho Building a Voice-Based Pizza Ordering Service with Twilio, OpenAI, and Google Maps Eluda Laaroussi Integrate Twilio ConversationRelay with Twilio Flex for Contextual Escalations Ricardo Bangueses Related Resources A newspaper article Twilio Docs From APIs to SDKs to sample apps API reference documentation, SDKs, helper libraries, quickstarts, and tutorials for your language and platform. An Open book Resource Center The latest ebooks, industry reports, and webinars Learn from customer engagement experts to improve your own communication. User group reactions Ahoy Twilio's developer community hub Best practices, code samples, and inspiration to build communications and digital engagement experiences. We can't wait to see what you build. Products Twilio Engage Flex User Authentication & Identity Lookup Verify Voice Video Messaging Beta products and features All products Solutions Customer Data Platform SMS Marketing Commerce Communications Contact Center All solutions Docs Communications Customer Data Engagement Applications Twilio Essentials Helper Libraries Developer Resources All Docs Company About Twilio Why Twilio Customer Engagement Platform Trusted Communications Customers Get Help Contact Sales Press & Media Public Policy Investor Relations Events Jobs at Twilio Facebook logo Twitter Logo Follow us on Twitter LinkedIn logo Youtube Logo Legal Privacy Twilio.org Press & Media Signal Investors Jobs Copyright © 2025 Twilio Inc. All Rights Reserved.

Source: https://www.google.com/search?sca_esv=392315896b6345ed&q=How+can+Abstract+Syntax+Trees+(AST)+be+used+to+analyze+and+improve+code+%22search?%22&sa=X&ved=2ahUKEwie4oSttK2LAxUudqQEHfVlCFoQ5t4CegQIGBAB
Voordat je verdergaat naar Google Zoeken Inloggen Inloggen Voordat je verdergaat naar Google We gebruiken cookies en gegevens voor het volgende: Google-services leveren en onderhouden Uitval bijhouden en bescherming bieden tegen spam, fraude en misbruik Doelgroepbetrokkenheid en sitestatistieken meten om inzicht te krijgen in hoe onze services worden gebruikt en de kwaliteit van die services te verbeteren Als je Alles accepteren kiest, gebruiken we cookies en gegevens ook voor het volgende: Nieuwe services ontwikkelen en verbeteren Advertenties laten zien en de effectiviteit ervan meten Gepersonaliseerde content laten zien (afhankelijk van je instellingen) Gepersonaliseerde advertenties laten zien (afhankelijk van je instellingen) Als je Alles afwijzen kiest, gebruiken we cookies niet voor deze aanvullende doeleinden. Niet-gepersonaliseerde content wordt beïnvloed door factoren zoals de content die je op dat moment bekijkt, activiteit in je actieve zoeksessie en je locatie. Niet-gepersonaliseerde advertenties worden beïnvloed door de content die je op dat moment bekijkt en je algemene locatie. Gepersonaliseerde content en advertenties kunnen ook relevantere resultaten, aanbevelingen en op jou toegespitste advertenties omvatten die zijn gebaseerd op eerdere activiteit van deze browser, zoals uitgevoerde Google-zoekopdrachten. We gebruiken cookies en gegevens ook om te zorgen dat de functionaliteit geschikt is voor je leeftijd, als dit relevant is. Selecteer Meer opties om meer informatie te bekijken, waaronder informatie over hoe je je privacyinstellingen beheert. Je kunt ook altijd naar g.co/privacytools gaan. Meer opties AfrikaansazərbaycanbosanskicatalàČeštinaCymraegDanskDeutscheestiEnglish (United Kingdom)English (United States)Español (España)Español (Latinoamérica)euskaraFilipinoFrançais (Canada)Français (France)GaeilgegalegoHrvatskiIndonesiaisiZuluíslenskaItalianoKiswahililatviešulietuviųmagyarMelayuNederlandsnorsko‘zbekpolskiPortuguês (Brasil)Português (Portugal)românăshqipSlovenčinaslovenščinasrpski (latinica)SuomiSvenskaTiếng ViệtTürkçeΕλληνικάбеларускаябългарскикыргызчақазақ тілімакедонскимонголРусскийсрпскиУкраїнськаქართულიհայերենעבריתاردوالعربيةفارسیአማርኛनेपालीमराठीहिन्दीঅসমীয়াবাংলাਪੰਜਾਬੀગુજરાતીଓଡ଼ିଆதமிழ்తెలుగుಕನ್ನಡമലയാളംසිංහලไทยລາວမြန်မာខ្មែរ한국어日本語简体中文繁體中文繁體中文 (香港) Privacybeleid Servicevoorwaarden

Source: https://www.nearform.com/insights/what-is-an-abstract-syntax-tree
What is an Abstract Syntax Tree? | Nearform Skip to content About Work Insights Careers Contact About Work Insights Careers Contact What is an Abstract Syntax Tree? 05 Jan 2023 Share Editor's note: This is a cross-post written by Senior Software Developer, Manuel Spigolon. Manuel has his own blog at backend.cafe where you can subscribe for updates and find more great posts. Some of the links in the article point to Manuel’s personal GitHub account. Abstract Syntax Tree You may have seen the term "AST" in multiple contexts without digging into what it actually means. It is a common term in compiler theory, and it is also used in the context of static analysis. In this post, we will see what an AST is, and we will focus on how it is a key part of the JavaScript language and ecosystem. The AST acronym stands for Abstract Syntax Tree . It is a tree data structure representing any structured text file, so every standardized syntax can be represented through an AST. Since the AST is "abstract", it does not have a standard representation, because every language may have its own specific dictionary. However, the common concept shared across all the ASTs, is the tree representation where the first node describes the document/program's entry point. Generally, the first step to run a piece of software is to parse the source code and build an AST. This operation is called parsing and is performed by the parser component. ???? Source ➞ ⚙️ Parser ➞ ???? AST If the parser is unable to parse the source code, it will throw an error. The source code is invalid and the AST cannot be generated. How the AST is used The AST is generated to process source code files, but it can be generated from any text file such as Markdown, JSON, or even a GraphQL Document. When the parser builds a valid AST, the following step is the transform : ???? Source ➞ ⚙️ Parser ➞ ???? AST ➞ ⚙️ Transform ➞ ???? AST (Transformed) The AST is manipulated and transformed during this phase to generate a new AST. Some examples of transformations are: the babel tool that transforms the AST to generate a new AST that is compatible with the target environment source code minification, where the AST is transformed to remove unnecessary characters and to reduce the file size code formatting tools, where the AST is transformed to add/remove spaces and new lines to improve the code readability Finally, the new AST is then passed to the compiler : ???? Source ➞ ⚙️ Parser ➞ ???? AST ➞ ⚙️ Transform ➞ ???? AST (Transformed) ➞ ⚙️ Compiler ➞ ???? Output The compiler generates output such as: bytecode to be executed a new source code file derived from the original source code some console output such as a warning or an error or suggestions All the tools we use in our daily work are based on the AST and are used to improve developer experience. Code completion, refactoring, linting and formatting are all powered by the source code tree representation! This is how our IDE and powerful editors implement extraordinary features! How to generate a JavaScript AST Let's focus on the Node.js runtime and see how the AST is built. JavaScript is a dynamic language, but the first step for every high-level programming language is to parse the source code and build an AST. Without completing this step, our code will never be executed. In fact, if you try to run a JS file that contains invalid JavaScript code, the runtime will throw a Syntax error even if the function is never called. text Copy to clipboard ~ this is not javascript code! ~
         ^^

SyntaxError: Unexpected identifier Under the hood, Node.js relies on the Google V8 engine to parse the source code and build the AST. The tree representation is then passed to the Ignition interpreter that builds the final bytecode. The AST generated by V8 is not accessible from the JavaScript code, but it is possible to generate an AST from JavaScript using one of the many available libraries: acorn: the core module of many parsers espree: the eslint's parser @typescript-eslint/typescript-estree: used by prettier ...and a lot more For example, the following code: js Copy to clipboard function hello(name) {
  console.log('Hello ' + name)
}

hello('foo') The following AST can represent it: json Copy to clipboard {
  "type": "Program",
  "start": 0,
  "end": 70,
  "body": [
    {
      "type": "FunctionDeclaration",
      "start": 0,
      "end": 56,
      "id": {
        "type": "Identifier",
        "start": 9,
        "end": 14,
        "name": "hello"
      },
      "expression": false,
      "generator": false,
      "async": false,
      "params": [
        {
          "type": "Identifier",
          "start": 16,
          "end": 20,
          "name": "name"
        }
      ],
      "body": {
        "type": "BlockStatement",
        "start": 22,
        "end": 56,
        "body": [
          {
            "type": "ExpressionStatement",
            "start": 26,
            "end": 54,
            "expression": {
              "type": "CallExpression",
              "start": 26,
              "end": 54,
              "callee": {
                "type": "MemberExpression",
                "start": 26,
                "end": 37,
                "object": {
                  "type": "Identifier",
                  "start": 26,
                  "end": 33,
                  "name": "console"
                },
                "property": {
                  "type": "Identifier",
                  "start": 34,
                  "end": 37,
                  "name": "log"
                },
                "computed": false,
                "optional": false
              },
              "arguments": [
                {
                  "type": "BinaryExpression",
                  "start": 38,
                  "end": 53,
                  "left": {
                    "type": "Literal",
                    "start": 38,
                    "end": 46,
                    "value": "Hello ",
                    "raw": "'Hello '"
                  },
                  "operator": "+",
                  "right": {
                    "type": "Identifier",
                    "start": 49,
                    "end": 53,
                    "name": "name"
                  }
                }
              ],
              "optional": false
            }
          }
        ]
      }
    },
    {
      "type": "ExpressionStatement",
      "start": 58,
      "end": 70,
      "expression": {
        "type": "CallExpression",
        "start": 58,
        "end": 70,
        "callee": {
          "type": "Identifier",
          "start": 58,
          "end": 63,
          "name": "hello"
        },
        "arguments": [
          {
            "type": "Literal",
            "start": 64,
            "end": 69,
            "value": "foo",
            "raw": "'foo'"
          }
        ],
        "optional": false
      }
    }
  ],
  "sourceType": "module"
} The AST standard initiative In the initial section, we said that the AST is not standardized. Moreover, the AST generated by v8 is a tuned and optimized AST designed for its engine, so it is a tailor-made AST . As mentioned before, many libraries can generate an AST from a JavaScript source code. At this point, the ESTree community organization defines an (unofficial) standard for JavaScript ASTs. However, there is a nice story behind the JavaScript AST baseline structure: “Once upon a time, an unsuspecting Mozilla engineer created an API in Firefox that exposed the SpiderMonkey engine's JavaScript parser as a JavaScript API. Said engineer documented the format it produced, and this format caught on as a lingua franca for tools that manipulate JavaScript source code.” The specification maintained by the ESTree community follows the ECMAScript standard and its naming conventions. Resources A handy tool to explore the AST is AST Explorer . You can copy-paste the source code and see the AST generated by the selected parser. “ℹ️ Navigate the AST I find it very useful to explore the AST generated by a GraphQL schema when I need to implement a mercurius plugin!” Of course, an awesome list ???? of AST-related projects exists! Summary I hope this AST post was helpful to understand how the AST is used in the JavaScript ecosystem. Now you should see your IDE and the eslint tool with different eyes! There was no magic, just a tree data structure! If you are willing to build an eslint or graphql plugin, the information in this post will help you to start your journey! If you enjoyed this article please share and follow me on Twitter @ManuEomm ! Insight, imagination and expertly engineered solutions to accelerate and sustain progress. Contact Keep in touch Get our newsletter, event invites, plus developer insights and research. Subscribe Check out our retail offerings Nearform_Commerce Company About Work Careers Sustainability Contact Updates Insights News Events Developers Open Source Hashnode Follow LinkedIn X Facebook Instagram YouTube © 2025 Nearform Cookie policy Legal requirements Privacy statement Site Motion ON

Source: https://cole.codes/posts/abstract-syntax-trees-ast-and-codemods
Supercharge your code with AST: Abstract Syntax Trees | Cole Turner Cole Turner Supercharge your code with AST: Abstract Syntax Trees Cole Turner July 20, 20207 min read Supercharge your code with AST: Abstract Syntax Trees Blog LIKE LOVE WOW LOL Abstract Syntax Trees (AST) is a fancy technical term to describe how programming language parsers understand our code. So for example, when we take the following code: console.log("JavaScript ASTs are fun!"); Copy A JavaScript parser will take this code and turn it into an Abstract Syntax Tree (AST). The tree appears as a deep nested object: {
  "type": "CallExpression",
  "callee": {
    "type": "MemberExpression",
    "computed": false,
    "object": {
      "type": "Identifier",
      "name": "console"
    },
    "property": {
      "type": "Identifier",
      "name": "log"
    }
  },
  "arguments": [
    {
      "type": "Literal",
      "value": "JavaScript ASTs are fun!",
      "raw": "\"JavaScript ASTs are fun!\""
    }
  ]
} Copy Included in the tree will also be tokens and numbers to identify where in the source code the tree elements will appear. Pretty neat! Codemods What's great about Abstract Syntax Trees (AST) is that we can make changes to the tree to create new source code. Using JavaScript, we can write a codemod to transform the input code using a set of pre-programmed instructions. ELI5: AST vs Codemod To recap, AST is the representation of our source code's syntax. A codemod makes changes to the AST to transform the input into a new output. Codemods are given a set of instructions to modify properties within the tree, which contains deeply nested objects that represent the contents of the source code. AST: Types and Properties A node in the Abstract Syntax Tree is any object that can standalone to represent a piece of source code. The tree is made up of many nodes, which can nest within each other, and contain the recipe for how to print the syntax. Every node has the following properties: Type Kind Location (start, end) Tokens - used to identify syntactical choices Every kind of node has its own set of additional properties. These additional properties help add some additional information to the recipe for printing that node. Example 1: Function Declaration A Function Declaration is the name given to the AST Node type that represents how we declare functions in JavaScript. function petTheDog(dog) {
} Copy When we print out the AST for this function declaration, that looks like this: {
    "type": "FunctionDeclaration",
    "id": {
      "type": "Identifier",
      "name": "petTheDog"
    },
    "params": [
      {
        "type": "Identifier",
        "name": "dog"
      }
    ],
    "body": {
      "type": "BlockStatement",
      "body": []
    },
    "generator": false,
    "expression": false,
    "async": false
  } Copy In the example above you will see a few types: Function Declaration Identifier Block Statement The Function Declaration type is the ancestor node (the whole source code). The name of the function is represented as an Identifier, which is used for variables and function names. The first parameter in the function signature, dog, is also represented as an Identifier. Lastly there is the Block Statement, which represents the source code that would appear within the curly brackets. The body attribute would normally be filled with other kinds of nodes. Isn't that really neat? Let's look at another example to compare. Example 2: Variable Declaration A Variable Declaration is composed of many types, and is a way to represent assigning a value to an identifier. const truths = {
  blackLives: "Matter"
}; Copy This variable declaration assigns an object as the value to the truths variable. Here's how that would appear in AST: {
  "type": "VariableDeclaration",
  "declarations": [
    {
      "type": "VariableDeclarator",
      "id": {
        "type": "Identifier",
        "name": "truths",
        "range": [6, 12]
      },
      "init": {
        "type": "ObjectExpression",
        "properties": [
          {
            "type": "Property",
            "key": {
              "type": "Identifier",
              "name": "blackLives"
            },
            "computed": false,
            "value": {
              "type": "Literal",
              "value": "Matter",
              "raw": "\"Matter\""
            },
            "kind": "init",
            "method": false,
            "shorthand": false
          }
        ]
      }
    }
  ],
  "kind": "const"
}

 Copy In the example above you will see a few types: Variable Declaration Object Expression Property Literal The Variable Declaration type represents the entire source code. The Object Expression represents the value that is assigned to the input Identifier. The object expression is a node that contains multiple properties, which represents the key/value pair. The property in this example uses a Property node type, containing instructions how to describe the key/value pair. They key is represented as an Identifer and the value is represented by the Literal node type. This example also demonstrates some of the additional properties (method, shorthand, computed) that make up the source code recipe. AST Types: All the Things! To represent every element of source code, we need tens to hundreds of types. Here is just the short list of some of the types you can use to identify JavaScript source code: For a more comprehensive list of all the types, checkout the Esprima Abtract Syntax Tree Format guide. Supercharging your source code with JSCodeshift Now that we understand Abstract Syntax Trees, and how we can use codemods, it's time to introduce JSCodeshift. jscodeshift is a toolkit for running codemods over multiple JavaScript or TypeScript files. It provides: - A runner, which executes the provided transform for each file passed to it. It also outputs a summary of how many files have (not) been transformed. - A wrapper around recast, providing a different API. Recast is an AST-to-AST transform tool and also tries to preserve the style of original code as much as possible. via JSCodeshift repository. The Basics: JSCodeshift 101 This utility is great for writing and running codemods that can be re-used across your codebase. In addition to the types we've learned from the Abstract Syntax Tree, there are two new object types that jscodeshift provides: Node Path: an object that contains a node, with additional metadata for manipulating that node or navigating through the tree. Collection: an iterable used to navigate through many Node Paths. These objects will help you structure your codemod to easily walk through the source code to transform the input. There are also some additional helper methods that make it easier to transverse the syntax tree: // Reads string into a `Collection`
const j = api.jscodeshift(sourceCode);

// Finds any node in a collection
const collection = j.find(j.Identifier, { name: 'dog' });

// Searches a collection for variable declarations
collection.findVariableDeclarators('truths');

// Renames variable declarators
collection.findVariableDeclarators('facts').renameTo('truths');

// Replace a node
j.find(j.Identifier, { name: 'foo' }).forEach(nodePath -> {
  j(nodePath).replaceWith(j.identifier('bar');
});

// Prints the output code from the syntax tree
collection.toSource({
  /* print options */
}); Copy In the example above, you will notice that the jscodeshift API provides objects for any of the AST node types. For example, for identifiers, the API provides: // Uppercase is used for searching/identifying types
j.find(j.CallExpression)

// Lowercase is used for composing nodes of a type
j.callExpression(callee, arguments) Copy Finding and Filtering In the examples above we saw that jscodeshift has an API for finding nodes by a certain type. Here's another example: const j = api.jscodeshift(sourceCode);

// Find types by a kind, returning a collection
j.find(j.FunctionDeclaration); Copy In a larger codebase, this will return all of the function declarations. However, if we don't want to transform all of the functions, we will need to add additional filters. const j = api.jscodeshift(sourceCode);

// Filter the find operation with additional metadata
// Only match functions with the name "petTheDog"
j.find(j.FunctionDeclaration, { id: { name: 'petTheDog' } }); Copy Now our transform will only run against the function declarations matched by the filter. In some cases, we will need to be very precise with our filters, and use deep matching to target only the code we want to change. What if I need to filter by an item in an array? The jscodeshift API provides mechanisms for deep filtering, such as only matching a specific item in an array: const j = api.jscodeshift(sourceCode);

// Filter the find operation with additional metadata
// Only match functions with the name "petTheDog"
j.find(j.FunctionDeclaration, {
  id: { name: 'petTheDog' },
  params: [, /* arg0 */ { name: 'dogBreed' }],
}); Copy In the example above, now our find operation will only match function declarations where the name is petTheDog and the second parameter is an identifier with the name dogBreed. That's super useful for avoiding false matches. What if I need to filter with some custom logic? The find operation also has the ability to specify functions as a matcher, so you can filter deeply with custom logic. const j = api.jscodeshift(sourceCode);

// Filter the find operation with additional metadata
// Only match functions with the name "petTheDog"
j.find(j.FunctionDeclaration, {
  id: { name: 'petTheDog' },
  params: [, /* arg0 */ { name: (value) => value.startsWith('dog') }],
});

 Copy Now our find operation will perform a similar match, however instead of matching function declarations with the second argument dogBreed, it will instead match any function where the second argument starts with the string "dog." This would match: function petTheDog(dog, dogBreed) and function petTheDog(dog, dogColor). This makes finding and filtering easy to use, and flexible enough to change only the code you want! Leveling up with jscodeshift Now you're ready to get started writing codemods. To learn more about JSCodeshift, checkout the repository. It's also worth getting familiar with recast, which is the transformer and printer that is used under the hood. Checkout this awesome, curated list of codemods - for inspiration! Starting your AST journey with ASTExplorer.net In all the examples above, we've parsed the source code and printed the represenation using JSON. To do this, I've used a tool called ASTExplorer.net. This tool is really useful for parsing, printing, and transforming all kinds of source code. It is absolutely essential for getting started and writing codemods. ASTexplorer allows you to input source code, write your codemod, browse the syntax tree, and view the transform output on the fly! Ready, Set, Go! That's all you need to get started with Abstract Syntax Trees, Codemods, and JSCodeshift. These tools are really useful for large scale refactors, or making repeated changes with ease. Here are some ideas for your next codemod: Refactoring API or making codebase platform changes. Migrating or removing deprecated packages (underscore/ lodash -> native prototypes). Upgrading from ES6 to ES6. LIKE LOVE WOW LOL Keep reading... Standing Out LOUDER in the Technical Interview March 10, 20215 min read If you want to stand out in the technical interview, you need to demonstrate not only your technical skills but also your communication and collaboration skills. You need to think LOUDER. Search Engine Optimization - Essentials for Web Developers August 16, 20208 min read Search Engine Optimization (SEO) can boost your page in search engines and boost your traffic. Every web developer should know about these basic optimizations. See everything

Source: https://medium.com/@ragav208/summarizing-source-code-with-abstract-syntax-trees-e7a468d9966e
Summarizing source code with Abstract Syntax Trees | by Ragav Sridharan | Medium Open in app Sign up Sign in Write Sign up Sign in Summarizing source code with Abstract Syntax Trees Ragav Sridharan · Follow 9 min read · Dec 14, 2021 -- 2 Listen Share There's been significant progress in the space of ML on source code in the last couple of years. The recent emergence of models like OpenAI and GitHub's copilot, GPT-J, and GPT-Neo allows software engineers to generate lines of code with minimal effort. Even though GPT based models help write code from comments, a significant part of a Software developer's work-life involves understanding code. Especially if the code is authored by someone else. In this blog post, we will be going through ML models that leverage Abstract Syntax Trees (ASTs) to help understand source code better: code2vec and code2seq. By the end of this blog, you will understand the workings of AST-based models to generate docstrings and summarize a piece of code in English. Motivation Like with codex and GPT3, one inspiration behind AST-based models is automating repetitive tasks that software engineers do daily. Software engineers often spend a lot of time understanding someone else's code or searching for why a code is written a certain way. While comments and code conventions help to a certain degree, you often come across bad commits or code snippets that make no sense whatsoever. The reality is that writing meaningful code is not the most important KPI of a software engineer, and business goals often trump good coding practices. Hence there is a need to automate certain practices to help software engineers focus on their creative flairs and business goals. By translating source code to natural language, AST-based ML models help software engineers search, navigate and understand code. However, understanding code is not straightforward. A human may not easily understand a code understood by a machine. So if a function is named 'f' or 'merge_sort' a computer would not care, but a software engineer would be grateful if the variables and functions have meaningful names. Though let's face it, it is not always straightforward to come up with meaningful names all the time, especially when we have 100s of tasks to finish in a sprint. Why ASTs? Following NLP trends, one approach to summarizing code snippets is to represent code snippets as a stream of tokens fed to an NLP model to learn relationships. However, this approach ignores code snippets' structured nature and syntax, which is essential to understand and summarize a code snippet. e.g., in the python snippet below, we know the first print statement is executed ten times because it's within a for-loop. for i in 1 to 10:   print("Print run 10 times in loop") Furthermore, performing no program analysis and learning from just code tokens requires a significant learning effort and is heavily reliant on the amount of available data. The considerable effort required is because the learning model has to re-learn the syntax and semantics of the programming language from the data. Hence, code generation models like GPT3 and codex are humongous in size. On the other end of the spectrum, deep program analysis before learning would make a learned model language-specific or even task-specific. There is a need to balance the learning and analysis effort required to represent code snippets effectively. Figure 1 shows that looking at AST paths as representations for code-snippets could be a potential sweet spot because: ASTs helped to capture structure and regularities that reflect common code patterns. Training on AST paths significantly lowers the learning effort. It is scalable: ASTs can be applied to a wide range of problems and large amounts of code. Figure 1: Learning effort vs. Programming analysis effort. (source: https://urialon.cswp.cs.technion.ac.il/wp-content/uploads/sites/83/2019/01/code2vec_popl19_slides.pdf) But how do we get AST paths from a code snippet, and how do we learn what a code-snippet does from a bunch of AST paths? Let's break it down! The Breakdown Step 1: Choosing a parser To convert a code-snippet into an AST, you need a parser. A parser is a software component that takes a code snippet and converts it into an AST. But choosing the correct parser is tricky as different languages have different parsing constraints. Like ANTLR and bblfshd, some universal parsers return standard ASTs for code-snippets in multiple languages. However, their coverage is limited. You could increase the coverage by adding more grammar rules to ANTLR, but the process is time-consuming. Step 2: Converting a code snippet to a bag of path contexts Once we have figured out which parser to use, let's look at the AST. Consider the code snippet below to check if a string is in a set of strings in JAVA while ignoring the case: boolean f(Set<String> set, String value) {     for (String entry : set) {         if (entry.equalsIgnoreCase(value)) {             return true;         }     }     return false; } On parsing the code, we get an AST shown in figure 2: Figure 2: AST for code to check if a string is present in a set of strings. (Reference for Figure: https://code2seq.org/ ) From the AST in figure 2, we can now extract a bag of path contexts. A path context is a triplet of: a) start terminal node: A terminal node in the AST b) AST path: a path between nodes in the AST, starting from one terminal, ending in another terminal, passing through an intermediate nonterminal in the path, which is a common ancestor of both terminals. c) end terminal node: A terminal node with an AST path connecting to the starting terminal node. The start terminal node, AST path, and the end terminal node are comma-separated, and the AST path is pipe-separated. The terminal nodes are also sub-tokenized in code2seq. (e.g., camelCase would be sub-tokenized to camel and case.) So, an example of a path context in figure 2 is: set,Parameter|Method Declaration|Parameter,string Step 3: Aggregating path contexts to a function name Now that we have a bunch of path contexts from a given code snippet, we need to figure out how to aggregate them so that the context vectors can be mapped to the target variable (function name in this case). Though both code2vec and code2seq use attention for aggregating a bag of paths, their architecture for aggregating a bag of paths and summarizing code is different. code2vec: Figure 3: Code2vec architecture (Reference: self) Figure 3 explains how AST path contexts are converted to a code summary in code2vec. The start terminal node, AST path, and the end terminal node of each path context are then embedded using their vocabularies. The three embeddings are then concatenated to get a single 3D vector called the context vector. Every context vector is passed through a fully connected layer that learns to combine components in the context vector (i.e., start terminal node, AST path, end terminal path). The fully connected layer learns to compress the information from 3d context vectors to a 1d vector by multiplying the context vectors by a weight matrix and then applying the tanh function to each vector element. After passing through the fully connected layers, the attention layer is applied to the condensed context vector. During training, the attention layer learns which path contexts are important in understanding a given code snippet by optimizing the weights for each path context. The attention layer is randomly initialized when the training begins. Figure 4 shows important path contexts that the attention layer gives importance to. Figure 4: Attended context vectors (Reference for figure: https://code2vec.org/) 5. The code vector for the given code snippet is then calculated by the formula below: $code\_vector = softmax(c_i.a_i)$ where $c_i$ is the condensed context vector and $a_i$ is the attention weight for that context vector. Code vector is the vector representation of the code snippet. We can search for similar code-snippets, cluster code-snippets by using code vectors. 6. To summarize the code snippet, the code vector is passed through a softmax dense layer to predict the probabilities of targets learned from the target vocabulary. The target vocabulary is a vocabulary of sub-tokenized function names and docstrings. This layer helps to categorize AST paths and predict the function name or summary of a given code snippet in English. So code2vec predicts the function name for Code snippet 1 as containsIgnoreCase code2seq Code2seq applies seq2seq based encoder-decoder architecture on path contexts to summarize code. That means the input to the model is a sequence of path contexts, and the output is the summary of the code snippet in English. To understand seq2seq encoder-decoder models, check out this very clear explanation by Jay Alammar: http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ Figure 5: Architecture Diagram for code2seq (Reference: Self) Figure 5 explains how code2seq works. Let us break down the process: To get a sequence of path-context, after parsing a code snippet and extracting path contexts, k path contexts are randomly selected as the representation of the snippet. To ensure no bias, k new paths are sampled afresh in every training iteration. The optimal number for k varies with the language. In general, k = 220 works well for Java snippets, while k=120 gives better results for javascript snippets in our dataset. This is because the average number of AST paths in js snippets in our dataset was around 122, whereas in Java it was around 220. After we have k randomly selected path contexts, we create path representations and token representations. Creating path representation: Each AST path consists of nodes and their child indices from a limited vocabulary of 364 symbols. Every node in an AST path is represented using a learned embedding matrix and then the entire sequence of AST paths is encoded using a bi-directional LSTM to create path representations. Creating token representation: The path context's start and end terminal nodes are split into subtokens. e.g., a token with value containsCase would be split into contains and case. Once split, the subtokens are represented using a learned embedding matrix to form token representations. After we have the path representation and token representations for each AST path, like in code2vec, all the representations are combined and passed through a fully-condensed layer to get a 1 D condensed vector (z) for a given path context. Steps 1 to 4 are included in the encoder phase. Next, we initiate the decoder by taking the mean of all the combined vectors of k paths. Once we have the decoder initial state, the attention layer attends over all k condensed vectors using the initialized decoder state to form the context vector (c_{t}). The context vector is calculated by: \alpha^{t} = softmax(h_{t}.W_{a}.z) , c_{t} = \sum_{i}^{n}\alpha_{i}^{t}.z_{i} Finally, the context vector c_{t} and the decoding state are then combined to predict the target token or the function name. Comparing the performance Figure 6: Comparing the F1 score of code2seq and code2vec on predicting docstrings for the java-med dataset. (Reference: self) Figure 6. shows how the F1 score of code2seq and code2vec vary with the increase in the lines of code in a snippet. We note that: Code2seq outperforms code2vec. Unlike code2vec, code2seq uses subtokens to encode the terminal nodes. This fact and using seq2seq architecture helped improve the F1 score for code-captioning. As the length of the code snippet increases, the F1 of the code snippet decreases. But the performance somewhat stabilizes once the number of lines in a code snippet is greater than nine. It's easier for models to understand shorter code snippets. Therefore, writing short and precise code snippets is generally good practice. So if you have been patient with me until now, I would imagine you have an idea about how you can automatically summarize code snippets. You could also give it a shot and summarize code snippets using code2vec on their site and code2seq here. Mloncode Abstract Syntax Tree Code2seq Code2vec Code -- -- 2 Follow Written by Ragav Sridharan 6 Followers ·36 Following Follow Responses (2) See all responses Help Status About Careers Press Blog Privacy Terms Text to speech Teams

Source: https://en.wikipedia.org/wiki/Abstract_syntax_tree#:~:text=An%20abstract%20syntax%20tree%20(AST,written%20in%20a%20formal%20language.
Abstract syntax tree - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Application in compilers Toggle Application in compilers subsection 1.1 Motivation 1.2 Design 2 Other usages Toggle Other usages subsection 2.1 AST differencing 2.2 Clone detection 3 See also 4 References 5 Further reading 6 External links Toggle the table of contents Abstract syntax tree 21 languages العربية Bosanski Čeština Deutsch Español فارسی Français 한국어 Հայերեն Hrvatski Ido Italiano Lombard 日本語 Polski Português Русский Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Special pages Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Tree representation of the abstract syntactic structure of source code For the trees used in linguistics, see parse tree. This article includes a list of references, related reading, or external links, but its sources remain unclear because it lacks inline citations. Please help improve this article by introducing more precise citations. (February 2013) (Learn how and when to remove this message) An abstract syntax tree for the following code for the Euclidean algorithm: while b ≠ 0:
    if a > b:
        a := a - b
    else:
        b := b - a
return a
 An abstract syntax tree (AST) is a data structure used in computer science to represent the structure of a program or code snippet. It is a tree representation of the abstract syntactic structure of text (often source code) written in a formal language. Each node of the tree denotes a construct occurring in the text. It is sometimes called just a syntax tree. The syntax is "abstract" in the sense that it does not represent every detail appearing in the real syntax, but rather just the structural or content-related details. For instance, grouping parentheses are implicit in the tree structure, so these do not have to be represented as separate nodes. Likewise, a syntactic construct like an if-condition-then statement may be denoted by means of a single node with three branches. This distinguishes abstract syntax trees from concrete syntax trees, traditionally designated parse trees. Parse trees are typically built by a parser during the source code translation and compiling process. Once built, additional information is added to the AST by means of subsequent processing, e.g., contextual analysis. Abstract syntax trees are also used in program analysis and program transformation systems. Application in compilers [edit] Abstract syntax trees are data structures widely used in compilers to represent the structure of program code. An AST is usually the result of the syntax analysis phase of a compiler. It often serves as an intermediate representation of the program through several stages that the compiler requires, and has a strong impact on the final output of the compiler. Motivation [edit] An AST has several properties that aid the further steps of the compilation process: An AST can be edited and enhanced with information such as properties and annotations for every element it contains. Such editing and annotation is impossible with the source code of a program, since it would imply changing it. Compared to the source code, an AST does not include inessential punctuation and delimiters (braces, semicolons, parentheses, etc.). An AST usually contains extra information about the program, due to the consecutive stages of analysis by the compiler. For example, it may store the position of each element in the source code, allowing the compiler to print useful error messages. Languages are often ambiguous by nature. In order to avoid this ambiguity, programming languages are often specified as a context-free grammar (CFG). However, there are often aspects of programming languages that a CFG can't express, but are part of the language and are documented in its specification. These are details that require a context to determine their validity and behaviour. For example, if a language allows new types to be declared, a CFG cannot predict the names of such types nor the way in which they should be used. Even if a language has a predefined set of types, enforcing proper usage usually requires some context. Another example is duck typing, where the type of an element can change depending on context. Operator overloading is yet another case where correct usage and final function are context-dependent. Design [edit] The design of an AST is often closely linked with the design of a compiler and its expected features. Core requirements include the following: Variable types must be preserved, as well as the location of each declaration in source code. The order of executable statements must be explicitly represented and well defined. Left and right components of binary operations must be stored and correctly identified. Identifiers and their assigned values must be stored for assignment statements. These requirements can be used to design the data structure for the AST. Some operations will always require two elements, such as the two terms for addition. However, some language constructs require an arbitrarily large number of children, such as argument lists passed to programs from the command shell. As a result, an AST used to represent code written in such a language has to also be flexible enough to allow for quick addition of an unknown quantity of children. To support compiler verification it should be possible to unparse an AST into source code form. The source code produced should be sufficiently similar to the original in appearance and identical in execution, upon recompilation. The AST is used intensively during semantic analysis, where the compiler checks for correct usage of the elements of the program and the language. The compiler also generates symbol tables based on the AST during semantic analysis. A complete traversal of the tree allows verification of the correctness of the program. After verifying correctness, the AST serves as the base for code generation. The AST is often used to generate an intermediate representation (IR), sometimes called an intermediate language, for the code generation. Other usages [edit] AST differencing [edit] AST differencing, or for short tree differencing, consists of computing the list of differences between two ASTs.[1] This list of differences is typically called an edit script. The edit script directly refers to the AST of the code. For instance, an edit action may result in the addition of a new AST node representing a function. Clone detection [edit] An AST is a powerful abstraction to perform code clone detection.[2] See also [edit] Abstract semantic graph (ASG), also called term graph Composite pattern Control-flow graph Directed acyclic graph (DAG) Document Object Model (DOM) Expression tree Extended Backus–Naur form Lisp, a family of languages written in trees, with macros to manipulate code trees Parse tree, also known as concrete syntax tree Semantic resolution tree (SRT) Shunting-yard algorithm Symbol table TreeDL Abstract Syntax Tree Interpreters References [edit] ^ Fluri, Beat; Wursch, Michael; PInzger, Martin; Gall, Harald (2007). "Change Distilling:Tree Differencing for Fine-Grained Source Code Change Extraction". IEEE Transactions on Software Engineering. 33 (11): 725–743. doi:10.1109/tse.2007.70731. ISSN 0098-5589. S2CID 13659557. ^ Koschke, Rainer; Falke, Raimar; Frenzel, Pierre (2006). "Clone Detection Using Abstract Syntax Suffix Trees". 2006 13th Working Conference on Reverse Engineering. IEEE. pp. 253–262. doi:10.1109/wcre.2006.18. ISBN 0-7695-2719-1. S2CID 6985484. Further reading [edit] Jones, Joel. "Abstract Syntax Tree Implementation Idioms" (PDF). Archived from the original (PDF) on 21 July 2024. (overview of AST implementation in various language families) Neamtiu, Iulian; Foster, Jeffrey S.; Hicks, Michael (May 17, 2005). Understanding Source Code Evolution Using Abstract Syntax Tree Matching. MSR'05. Saint Louis, Missouri: ACM. CiteSeerX 10.1.1.88.5815. Würsch, Michael. Improving Abstract Syntax Tree based Source Code Change Detection (Diploma thesis). Lucas, Jason (16 August 2006). "Thoughts on the Visual C++ Abstract Syntax Tree (AST)". External links [edit] Wikimedia Commons has media related to Abstract syntax trees. AST View: an Eclipse plugin to visualize a Java abstract syntax tree "Abstract Syntax Tree and Java Code Manipulation in the Eclipse IDE". eclipse.org. "CAST representation". cs.utah.edu. eli project: Abstract Syntax Tree Unparsing "Architecture‑Driven Modernization — ADM: Abstract Syntax Tree Metamodeling — ASTM". (OMG standard). JavaParser: The JavaParser library provides you with an Abstract Syntax Tree of your Java code. The AST structure then allows you to work with your Java code in an easy programmatic way. Spoon: A library to analyze, transform, rewrite, and transpile Java source code. It parses source files to build a well-designed AST with powerful analysis and transformation API. AST Explorer: A website to help visualize ASTs in several popular languages such as Go, Python, Java, and JavaScript. v t e Parsing algorithms Top-down Earley LL Recursive descent Tail recursive Bottom-up Precedence Simple Operator Shunting-yard LR Simple Look-ahead Canonical Generalized CYK Recursive ascent Shift-reduce Mixed, other Combinator Chart Left corner Statistical Related topics PEG Definite clause grammar Deterministic parsing Dynamic programming Memoization Parser generator LALR Parse tree AST Scannerless parsing History of compiler construction Comparison of parser generators Operator-precedence grammar Authority control databases: National Germany Retrieved from "https://en.wikipedia.org/w/index.php?title=Abstract_syntax_tree&oldid=1272101389" Categories: Trees (data structures) Formal languages Hidden categories: Articles with short description Short description matches Wikidata Articles lacking in-text citations from February 2013 All articles lacking in-text citations Commons category link from Wikidata This page was last edited on 27 January 2025, at 04:31 (UTC). Text is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Abstract syntax tree 21 languages Add topic

Source: https://www.educative.io/answers/what-is-the-astparse-method-in-python#:~:text=In%20Python%2C%20the%20ast.,representation%20of%20a%20source%20code.
What is the ast.parse() method in Python? Explore EXPLORE THE CATALOGSupercharge your career with 700+ hands-on courses View All Courses Python Java JavaScript C React Docker Vue JS R Web Dev DevOps AWS C# LEARNING TOOLSExplore the industry's most complete learning platform CoursesLevel up your skills Skill PathsAchieve learning goals ProjectsBuild real-world applications Mock InterviewsNewAI-Powered interviews Personalized PathsGet the right resources for your goals LEARN TO CODE Check out our beginner friendly courses. Pricing For Business Resources BlogFor developers, By developers CheatsheetsDownload handy guides for tech topics WebinarsSessions with our global developer community AnswersTrusted answers to developer questions Search Courses Log In Join for free What is the ast.parse() method in Python? Overview In Python, the ast.parse() function splits the source code into tokens based on the grammar. These tokens are then transformed to build an Abstract Syntax Tree (AST). It is a tree representation of a source code. In other words, the ast.parse() method allows the conversion of the source code into a collection of AST nodes linked together as a tree. Syntax ast.parse(source, filename='<unknown>', mode='exec', *, type_comments=False, feature_version=None)
 Parameters source: The source code in a string, which is to be converted into an AST. filename: The file from which the code is read. mode: The mode in which the code is compiled. It is an exec, which takes a code block that has statements, class, functions, and so on. It can also be an exec that only accepts a single expression. type_comments: The default value is False. If True is returned, the parser is modified to check. It returns type comments specified by PEP 484 , and PEP 526. feature_version: The default value is None. If it is set to a tuple, say (3,4), it attempts to parse using the Python 3.4 version’s grammar. Return type The ast.parse() method converts the source code to return a collection of AST nodes. Code import ast from pprint import pprint tree = ast.parse(""" def add(a, b):   return a + b """) pprint(ast.dump(tree)) Run Explanation Line 5: We write a function add(a,b) that returns the sum of the input numbers. Line 4: The add function is passed as a string to the ast.parse() method. It transforms the source code into a collection of AST nodes. The output has several AST nodes, such as: FunctionDef: Function definition. BinOp: Binary operation. We can also observe arguments inside these nodes. For instance, the value of the Op argument as Add() in the BinOp node. Relevant Answers Explore Courses Free Resources License: Creative Commons-Attribution-ShareAlike 4.0 (CC-BY-SA 4.0) Learn in-demand tech skills in half the time PRODUCTS Mock Interview New Courses Skill Paths Projects Assessments TRENDING TOPICS Learn to Code Tech Interview Prep Generative AI Data Science Machine Learning GitHub Students Scholarship Early Access Courses Blind 75 Layoffs Pricing For Individuals Try for Free Gift a Subscription CONTRIBUTE Become an Author Become an Affiliate Earn Referral Credits RESOURCES Blog Cheatsheets Webinars Answers ABOUT US Our Team Careers Hiring Frequently Asked Questions Contact Us Press LEGAL Privacy Policy Cookie Policy Terms of Service Business Terms of Service Data Processing Agreement INTERVIEW PREP COURSES Grokking the Modern System Design Interview Grokking the Product Architecture Design Interview Grokking the Coding Interview Patterns Machine Learning System Design Tiktok Streamline Icon: https://streamlinehq.com Copyright ©2025 Educative, Inc. All rights reserved.

Source: https://en.wikipedia.org/wiki/Abstract_syntax_tree
Abstract syntax tree - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Application in compilers Toggle Application in compilers subsection 1.1 Motivation 1.2 Design 2 Other usages Toggle Other usages subsection 2.1 AST differencing 2.2 Clone detection 3 See also 4 References 5 Further reading 6 External links Toggle the table of contents Abstract syntax tree 21 languages العربية Bosanski Čeština Deutsch Español فارسی Français 한국어 Հայերեն Hrvatski Ido Italiano Lombard 日本語 Polski Português Русский Українська Tiếng Việt 粵語 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Special pages Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Tree representation of the abstract syntactic structure of source code For the trees used in linguistics, see parse tree. This article includes a list of references, related reading, or external links, but its sources remain unclear because it lacks inline citations. Please help improve this article by introducing more precise citations. (February 2013) (Learn how and when to remove this message) An abstract syntax tree for the following code for the Euclidean algorithm: while b ≠ 0:
    if a > b:
        a := a - b
    else:
        b := b - a
return a
 An abstract syntax tree (AST) is a data structure used in computer science to represent the structure of a program or code snippet. It is a tree representation of the abstract syntactic structure of text (often source code) written in a formal language. Each node of the tree denotes a construct occurring in the text. It is sometimes called just a syntax tree. The syntax is "abstract" in the sense that it does not represent every detail appearing in the real syntax, but rather just the structural or content-related details. For instance, grouping parentheses are implicit in the tree structure, so these do not have to be represented as separate nodes. Likewise, a syntactic construct like an if-condition-then statement may be denoted by means of a single node with three branches. This distinguishes abstract syntax trees from concrete syntax trees, traditionally designated parse trees. Parse trees are typically built by a parser during the source code translation and compiling process. Once built, additional information is added to the AST by means of subsequent processing, e.g., contextual analysis. Abstract syntax trees are also used in program analysis and program transformation systems. Application in compilers [edit] Abstract syntax trees are data structures widely used in compilers to represent the structure of program code. An AST is usually the result of the syntax analysis phase of a compiler. It often serves as an intermediate representation of the program through several stages that the compiler requires, and has a strong impact on the final output of the compiler. Motivation [edit] An AST has several properties that aid the further steps of the compilation process: An AST can be edited and enhanced with information such as properties and annotations for every element it contains. Such editing and annotation is impossible with the source code of a program, since it would imply changing it. Compared to the source code, an AST does not include inessential punctuation and delimiters (braces, semicolons, parentheses, etc.). An AST usually contains extra information about the program, due to the consecutive stages of analysis by the compiler. For example, it may store the position of each element in the source code, allowing the compiler to print useful error messages. Languages are often ambiguous by nature. In order to avoid this ambiguity, programming languages are often specified as a context-free grammar (CFG). However, there are often aspects of programming languages that a CFG can't express, but are part of the language and are documented in its specification. These are details that require a context to determine their validity and behaviour. For example, if a language allows new types to be declared, a CFG cannot predict the names of such types nor the way in which they should be used. Even if a language has a predefined set of types, enforcing proper usage usually requires some context. Another example is duck typing, where the type of an element can change depending on context. Operator overloading is yet another case where correct usage and final function are context-dependent. Design [edit] The design of an AST is often closely linked with the design of a compiler and its expected features. Core requirements include the following: Variable types must be preserved, as well as the location of each declaration in source code. The order of executable statements must be explicitly represented and well defined. Left and right components of binary operations must be stored and correctly identified. Identifiers and their assigned values must be stored for assignment statements. These requirements can be used to design the data structure for the AST. Some operations will always require two elements, such as the two terms for addition. However, some language constructs require an arbitrarily large number of children, such as argument lists passed to programs from the command shell. As a result, an AST used to represent code written in such a language has to also be flexible enough to allow for quick addition of an unknown quantity of children. To support compiler verification it should be possible to unparse an AST into source code form. The source code produced should be sufficiently similar to the original in appearance and identical in execution, upon recompilation. The AST is used intensively during semantic analysis, where the compiler checks for correct usage of the elements of the program and the language. The compiler also generates symbol tables based on the AST during semantic analysis. A complete traversal of the tree allows verification of the correctness of the program. After verifying correctness, the AST serves as the base for code generation. The AST is often used to generate an intermediate representation (IR), sometimes called an intermediate language, for the code generation. Other usages [edit] AST differencing [edit] AST differencing, or for short tree differencing, consists of computing the list of differences between two ASTs.[1] This list of differences is typically called an edit script. The edit script directly refers to the AST of the code. For instance, an edit action may result in the addition of a new AST node representing a function. Clone detection [edit] An AST is a powerful abstraction to perform code clone detection.[2] See also [edit] Abstract semantic graph (ASG), also called term graph Composite pattern Control-flow graph Directed acyclic graph (DAG) Document Object Model (DOM) Expression tree Extended Backus–Naur form Lisp, a family of languages written in trees, with macros to manipulate code trees Parse tree, also known as concrete syntax tree Semantic resolution tree (SRT) Shunting-yard algorithm Symbol table TreeDL Abstract Syntax Tree Interpreters References [edit] ^ Fluri, Beat; Wursch, Michael; PInzger, Martin; Gall, Harald (2007). "Change Distilling:Tree Differencing for Fine-Grained Source Code Change Extraction". IEEE Transactions on Software Engineering. 33 (11): 725–743. doi:10.1109/tse.2007.70731. ISSN 0098-5589. S2CID 13659557. ^ Koschke, Rainer; Falke, Raimar; Frenzel, Pierre (2006). "Clone Detection Using Abstract Syntax Suffix Trees". 2006 13th Working Conference on Reverse Engineering. IEEE. pp. 253–262. doi:10.1109/wcre.2006.18. ISBN 0-7695-2719-1. S2CID 6985484. Further reading [edit] Jones, Joel. "Abstract Syntax Tree Implementation Idioms" (PDF). Archived from the original (PDF) on 21 July 2024. (overview of AST implementation in various language families) Neamtiu, Iulian; Foster, Jeffrey S.; Hicks, Michael (May 17, 2005). Understanding Source Code Evolution Using Abstract Syntax Tree Matching. MSR'05. Saint Louis, Missouri: ACM. CiteSeerX 10.1.1.88.5815. Würsch, Michael. Improving Abstract Syntax Tree based Source Code Change Detection (Diploma thesis). Lucas, Jason (16 August 2006). "Thoughts on the Visual C++ Abstract Syntax Tree (AST)". External links [edit] Wikimedia Commons has media related to Abstract syntax trees. AST View: an Eclipse plugin to visualize a Java abstract syntax tree "Abstract Syntax Tree and Java Code Manipulation in the Eclipse IDE". eclipse.org. "CAST representation". cs.utah.edu. eli project: Abstract Syntax Tree Unparsing "Architecture‑Driven Modernization — ADM: Abstract Syntax Tree Metamodeling — ASTM". (OMG standard). JavaParser: The JavaParser library provides you with an Abstract Syntax Tree of your Java code. The AST structure then allows you to work with your Java code in an easy programmatic way. Spoon: A library to analyze, transform, rewrite, and transpile Java source code. It parses source files to build a well-designed AST with powerful analysis and transformation API. AST Explorer: A website to help visualize ASTs in several popular languages such as Go, Python, Java, and JavaScript. v t e Parsing algorithms Top-down Earley LL Recursive descent Tail recursive Bottom-up Precedence Simple Operator Shunting-yard LR Simple Look-ahead Canonical Generalized CYK Recursive ascent Shift-reduce Mixed, other Combinator Chart Left corner Statistical Related topics PEG Definite clause grammar Deterministic parsing Dynamic programming Memoization Parser generator LALR Parse tree AST Scannerless parsing History of compiler construction Comparison of parser generators Operator-precedence grammar Authority control databases: National Germany Retrieved from "https://en.wikipedia.org/w/index.php?title=Abstract_syntax_tree&oldid=1272101389" Categories: Trees (data structures) Formal languages Hidden categories: Articles with short description Short description matches Wikidata Articles lacking in-text citations from February 2013 All articles lacking in-text citations Commons category link from Wikidata This page was last edited on 27 January 2025, at 04:31 (UTC). Text is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Abstract syntax tree 21 languages Add topic

Source: https://earthly.dev/blog/python-ast/
Introduction to Abstract Syntax Trees in Python - Earthly Blog <img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=259843109045285&amp;ev=PageView&amp;noscript=1"/> Introducing Earthly Lunar. Streamline your development process with an all-in-one governance platform. Learn More. Introducing Earthly Lunar. Streamline your development process with an all-in-one governance platform. Learn More. Products Earthly Lunar Earthfile Login Earthfile Docs Earthly Satellites Satellites Pricing Blog Get a Demo Products Products Earthly Lunar Earthfile Login Earthfile Docs Earthly Satellites Satellites Pricing Blog Get a Demo Blog / Python Table of contents Understanding AST Importance of AST Anatomy of an AST Node Generating Code with AST in Python Visualizing the Graph Analyzing Code With AST Real-World Example: Static Code Analysis Transforming Code With AST Conclusion Related ‣ Python Abstract Base Classes ‣ New Features in Python 3.11 ‣ Classes and Objects in Python Popular Posts ‣ Working with YAML in Python ‣ Python's Subprocess Module ‣ Python Makefile Learn More About Earthly Earthly makes builds super simple. Learn More Introduction to Abstract Syntax Trees in Python 36 minute read Updated: September 11, 2023 Daniel Boadzie Table of Contents Understanding AST Importance of AST Anatomy of an AST Node Generating Code with AST in Python Visualizing the Graph Analyzing Code With AST Real-World Example: Static Code Analysis Transforming Code With AST Conclusion This article explores Python Abstract Syntax Trees (ASTs). Python developers like that Earthly provides consistent and fast build environments for running Python code. Check it out. When working with Python, it is essential to have a clear understanding of the code’s structure and how it works. This is where Abstract Syntax Trees (AST) come into play. An AST is a tree-like structure that represents the syntactic structure of a program, making it a powerful tool for Python programmers who want to take their coding skills to the next level. By understanding how Python code is parsed, developers can efficiently analyze thier code leading to more effective and efficient programming. In this guide, we will introduce you to the concept of AST and provide an overview of its importance in Python programming. We will explore how to use AST for analyzing, transforming, and optimizing your code, as well as provide a real-world example of how AST can be used for static code analysis. By the end of this article, you will have a solid understanding of how to leverage an AST to improve your Python coding skills and create more efficient and effective programs. Understanding AST An Abstract Syntax Tree (AST) is a hierarchical, tree-like data structure that represents the syntactic structure of a program. It serves as an intermediate representation of the code and is generated by a compiler or interpreter. The AST captures the essential elements of the program’s syntax, providing a structured representation of operators, functions, statements, and expressions. The AST is constructed by parsing the source code of a program. During the parsing process, the code is analyzed and broken down into its constituent parts, such as keywords, identifiers, literals, and operators. These parts are then organized hierarchically in the form of a tree, with the main program as the root node and the various code elements as its child nodes. The AST effectively captures the structure of the code by representing relationships between different code elements. For example, function calls are represented as child nodes of the corresponding function definitions, and expressions within statements are represented as child nodes of those statements. This hierarchical representation allows for easy traversal and analysis of the code’s structure. To obtain an AST from source code, a parser is used. The parser takes the source code as input and performs lexical analysis and syntactic parsing to identify the different elements and their relationships. The parser follows the grammar rules of the programming language to generate the corresponding AST. The AST can be thought of as a simplified version of the program’s source code, making it easier to analyze, manipulate, and optimize. This is particularly useful for tools that need to analyze the structure of the program, such as linters, code formatters, and optimization tools. By understanding how to work with ASTs, developers can gain insights into how their code works and identify areas where improvements can be made. Importance of AST The importance of Abstract Syntax Trees (AST) in Python programming cannot be overstated. AST serves as a powerful and efficient tool for code analysis and manipulation. By representing the program’s syntax in a structured form, AST empowers developers to perform a wide range of code transformations and optimizations, including bug detection, performance improvement, and memory usage optimization. The significance of AST is particularly evident in the context of Python due to the language’s dynamic nature. Python’s syntax is known for its complexity and flexibility, which can pose challenges when it comes to effectively analyzing and optimizing code. However, AST provides a structured and standardized representation of Python code, offering developers a unified framework to reason about and manipulate the code. This structured approach facilitates the navigation and transformation of Python code, ultimately enhancing the overall quality and efficiency of Python programs. Some of the way you might use an AST in Python programming (besides parsing) include: Code analysis and optimization: AST can be used to analyze the structure of code and identify potential issues such as syntax errors, security vulnerabilities, and performance bottlenecks. It can also be used to optimize code by identifying redundant or inefficient operations. Code transformation: AST can be used to transform code by replacing, adding, or removing code elements such as functions, classes, and statements. This is particularly useful for code refactoring, where large codebases need to be updated or modernized. Tooling and automation: AST is used extensively in Python tooling and automation, such as linters, code formatters, and static analyzers. These tools use AST to analyze and manipulate code automatically, improving developer productivity and code quality. Anatomy of an AST Node Understanding the structure of an AST node is crucial for working with ASTs effectively. Each node in the AST represents a specific element of the program’s syntax, such as functions, statements, and expressions. AST nodes have a defined structure, consisting of a type, attributes, and child nodes. AST tree The type of an AST node indicates the kind of syntax element that it represents. For example, a “FunctionDef” node represents a function definition in Python, while an “If” node represents an if statement. Other types of AST nodes include “Assign” for variable assignments, “Call” for function calls, and “Expr” for expression statements. The attributes of an AST node provide additional information about the node’s type. For example, a “FunctionDef” node will have attributes such as the function name, arguments, and body. An “Assign” node will have attributes such as the variable name and the value being assigned. The child nodes of an AST node represent the sub-elements of the syntax element it represents. For example, the body of a FunctionDef node will be a list of child nodes representing the statements that make up the function body. Here’s an example of an AST node for a simple Python function: FunctionDef(
    name='my_function',
    args=arguments(
        args=[arg(arg='arg1'), arg(arg='arg2')],
        vararg=None,
        kwarg=None,
        defaults=[]
    ),
    body=[Expr(value=Num(n=42))],
    decorator_list=[]
) In this example, the AST node represents a function definition with the name my_function and two arguments. The body of the function contains a single expression statement, which is a number literal with the value 42. The node also has an empty decorator list. Generating Code with AST in Python In Python, an AST is generated using the built-in ast module, which provides a range of functions for parsing Python code and constructing an AST tree. The ast.parse() function is the primary method for generating an AST tree, which takes a string of Python code as input and returns an AST object that represents the code’s structure. For example, consider the following Python code: def greet(name):
    print("Hello, " + name + "!")
    
greet("John") To generate an AST tree for this code, we can use the ast.parse() function as follows: import ast

code = '''
def greet(name):
    print("Hello, " + name + "!")
    
greet("John")
'''

tree = ast.parse(code) This will create an AST tree that represents the structure of the code, with each node in the tree corresponding to a specific Python language construct, such as a function definition or a call expression. We can also use the ast.dump() function to visualize the AST tree in a more readable format, as follows: import pprint

pprint.pprint(ast.dump(tree)) This will output the following representation of the AST tree: 
("Module(body=[FunctionDef(name='greet', args=arguments(posonlyargs=[], "
 "args=[arg(arg='name')], kwonlyargs=[], kw_defaults=[], defaults=[]), "
 "body=[Expr(value=Call(func=Name(id='print', ctx=Load()), "
 "args=[BinOp(left=BinOp(left=Constant(value='Hello, '), op=Add(), "
 "right=Name(id='name', ctx=Load())), op=Add(), right=Constant(value='!'))], "
 "keywords=[]))], decorator_list=[]), Expr(value=Call(func=Name(id='greet', "
 "ctx=Load()), args=[Constant(value='John')], keywords=[]))], type_ignores=[])") The output above shows the structure of a program’s syntax in a hierarchical manner. Each element of the code is represented by a node in the AST. In this specific example, the AST represents a Python module that contains a function definition named greet. The greet function takes an argument called name. The body of the function consists of an expression that calls the print function, passing it a formatted string that includes the value of the “name” argument. To make sense of this AST representation, you can break it down as follows: The outermost node is the Module node, which represents the entire module. Within the Module node, there is a FunctionDef node representing the greet function definition. The FunctionDef node has various attributes, such as the name of the function (name) and the arguments it takes (args). The body of the greet function is represented by a list of nodes, in this case, containing a single “Expr” node. The Expr node represents an expression, and in this case, it contains a function call to print. The function call is represented by the “Call” node, which has the function name (“Name”) as print and the arguments it takes. The arguments of the print function call include a formatted string that concatenates the name argument with other strings. Visualizing the Graph There are several ways to visualize an AST in Python. One popular library for this purpose is the graphviz library, which can generate visual representations of graphs and trees. To use graphviz for rendering graphs to PNG, you first need to install it on your system. You can do this by following the installation instructions on the graphviz website for your specific operating system. Once installed, you can then use pip to install the Python bindings for graphviz by running the command pip install graphviz. Here’s an example of how to use graphviz to visualize an AST: import ast
from graphviz import Digraph

...
# Create a Graphviz Digraph object
dot = Digraph()

# Define a function to recursively add nodes to the Digraph
def add_node(node, parent=None):
    node_name = str(node.__class__.__name__)
    dot.node(str(id(node)), node_name)
    if parent:
        dot.edge(str(id(parent)), str(id(node)))
    for child in ast.iter_child_nodes(node):
        add_node(child, node)

# Add nodes to the Digraph
add_node(tree)

# Render the Digraph as a PNG file
dot.format = 'png'
dot.render('my_ast', view=True) In this example, we start by defining a Python code string that contains a simple function. We then use the ast module to generate an AST from the code. Next, we create a graphviz Digraph object to hold the nodes and edges of the AST. We define a recursive function called add_node to add nodes to the Digraph, along with their parent nodes and edges. Finally, we call add_node on the root node of the AST (in this case, the Module node), and then render the Digraph as a PNG file after specifying the format and using the render method. When we open the PNG file, we see a visual representation of the AST, with nodes representing different elements of the code (such as functions, statements, and expressions) and edges representing the relationships between them. By executing the code and rendering the Digraph as a PNG file, a graphical representation of the AST will be generated. The resulting visualization will help visualize the code’s structure, including functions, statements, and expressions, along with their relationships and hierarchy. Opening the generated PNG file (my_ast.png in this case) will display the graphical representation of the AST, allowing for a more intuitive understanding of the code’s structure and organization. Here is an example of how the graph generated by graphviz may look like: Graph Analyzing Code With AST Real-World Example: Static Code Analysis Static code analysis involves analyzing a program’s code without executing it to identify potential issues before deployment. To demonstrate the use of AST for static code analysis, we will use a simple example of a Python program that contains some common coding errors, such as unused variables and functions, as well as code smells that could potentially lead to bugs and security vulnerabilities. We will generate an AST tree from the program’s code and analyze its structure to identify and fix these issues. By using AST for static code analysis, we can quickly identify and fix potential issues, ensuring that the code is optimized, secure, and bug-free before deployment. This can save developers time and resources by minimizing the need for manual debugging and testing, leading to more efficient and effective programming. To perform static code analysis on the given code using an AST, we can generate an AST tree and traverse it to identify potential issues. Let’s consider an example where we want to analyze a Python codebase for inefficient string concatenation. We can use the ast.NodeVisitor class to visit each node in the AST tree and check if any string concatenations are being performed inefficiently. Here’s a suitable example: import ast

class FunctionCallVisitor(ast.NodeVisitor):
    def visit_Call(self, node):
        if isinstance(node.func, ast.Name) and node.func.id == "print":
            args = [arg for arg in node.args if isinstance(arg, ast.Str)]
            if args:
                print("Detected print statements with string literals:")
                for arg in args:
                    print(arg.s)  # Print the string literal directly
        self.generic_visit(node) The code defines a custom FunctionCallVisitor class that inherits from ast.NodeVisitor. This class is used to visit and traverse the nodes in the AST. Inside the FunctionCallVisitor class, there is a method called visit_Call, which is called when a Call node is encountered in the AST. The visit_Call method takes the node as an argument. The code checks if the Call node represents a function call to the print function by verifying that node.func is an instance of ast.Name and its id attribute is equal to “print”. If the function call is indeed to print, the code further checks if any of the arguments passed to the print function are string literals by using a list comprehension and checking if each argument is an instance of ast.Str. If there are string literals among the arguments, the code prints the message “Detected print statements with string literals:” and then iterates over the string literals (arg.s) and prints each one directly. Finally, the code invokes self.generic_visit(node) to continue visiting other nodes in the AST. This code essentially traverses the AST and detects print statements with string literals, printing those literals directly. To demonstrate static code analysis using AST in Python, let’s create a function called perform_static_analysis. This function takes a code snippet as input and performs the analysis. First, it parses the code using ast.parse to generate the corresponding Abstract Syntax Tree (AST). Then, it creates an instance of the FunctionCallVisitor class and visits each node in the AST to identify specific function calls. The function call visitor can be customized to detect and analyze specific patterns or behaviors. In our case, we will focus on identifying print statements with string literals. Let’s take a look at the code implementation below: def perform_static_analysis(code):
    tree = ast.parse(code)
    visitor = FunctionCallVisitor()
    visitor.visit(tree) The code defines a function called perform_static_analysis that takes the code as an input. Inside the function, it uses ast.parse(code) to parse the given code and generate an AST (Abstract Syntax Tree) representation of the code. The resulting AST tree is stored in the tree variable. It then creates an instance of the FunctionCallVisitor class, named visitor. Finally, it calls visitor.visit(tree) to start traversing the AST by invoking the visit method of the visitor object and passing the tree as the argument. This code performs the static code analysis by parsing the given code into an AST and then utilizing the FunctionCallVisitor to traverse the AST and identify print statements with string literals. Now to observe the output of the code and understand how the static analysis works, let’s consider the following example: def main():
    code = '''
def calculate_average(numbers):
        total = sum(numbers)
        average = total / len(numbers)
        print("Average:", average)
   
data = [1, 2, 3, 4, 5]
calculate_average(data)
print("End of program")
    '''
    perform_static_analysis(code) In this code, we have a main function that defines a multiline string code. The code represents a Python code snippet that calculates the average of a list of numbers and prints the result. The perform_static_analysis function is then called with the code as an argument to analyze the code statically. Inside the code, there is a function called calculate_average that takes a list of numbers as input. It calculates the total sum of the numbers and then divides it by the length of the list to compute the average. The average value is then printed using the print statement. After defining the calculate_average function, the code snippet creates a list data with some numbers and calls the calculate_average function with data as an argument. Finally, it prints “End of program” to indicate the end of the program execution. By calling perform_static_analysis(code), the code is analyzed using the FunctionCallVisitor to detect any print statements that use string literals. This analysis helps identify any potential issues or inefficiencies in the code related to printing values. Now when we call our main function, we will see the following output: main() Detected print statements with string literals:
Average:
Detected print statements with string literals:
End of program In conclusion, static code analysis using AST offers developers a powerful tool to analyze their code without executing it. By leveraging the abstract syntax tree representation of the code, developers can detect potential issues before deployment, saving time and resources while ensuring code quality. Transforming Code With AST Transforming code with AST involves modifying the structure of the code without changing its behavior. This can be useful for addressing issues related to code smells and technical debt, such as redundant code, unnecessary statements, and outdated syntax. By using AST to transform code, developers can improve code readability, maintainability, and efficiency. One benefit of using AST for code transformation is that it allows developers to modify code in a precise and controlled manner. For example, instead of searching through the code manually for a specific pattern to replace, developers can use an AST tool to target only the relevant nodes in the code tree. This can save a lot of time and reduce the risk of introducing errors. To demonstrate the effectiveness of using AST for code transformation, let’s consider a practical scenario. Imagine we have a complex Python codebase consisting of multiple modules and classes. We want to introduce a logging mechanism by adding a logger instance to all relevant functions and methods in the codebase. Manually modifying each function would be tedious and error-prone, especially in a large-scale project. However, by leveraging the power of AST and AST manipulation techniques, we can programmatically traverse the codebase, identify function and method definitions, and automatically inject the necessary logging statements. This approach saves developers significant time and effort while ensuring consistent and reliable modifications across the entire codebase. class LoggingTransformer(ast.NodeTransformer):
    def visit_FunctionDef(self, node):
        # Check if the function requires logging
        if self.requires_logging(node):
            # Create the logger instance
            logger_stmt = ast.Assign(
                targets=[ast.Name(id='logger', ctx=ast.Store())],
                value=ast.Call(
                    func=ast.Attribute(
                        value=ast.Name(id='logging', ctx=ast.Load()),
                        attr='getLogger',
                        ctx=ast.Load()
                    ),
                    args=[ast.Str(s=node.name)],
                    keywords=[]
                ),
                lineno=node.lineno  # Set the lineno attribute
            )
            # Insert the logger instance at the beginning of the function body
            node.body.insert(0, logger_stmt)
        return node

    def requires_logging(self, node):
        # Add your logic here to determine if the function requires logging
        # For example, you can check function attributes, docstrings, 
        # or function names
        # Return True if logging is required, False otherwise
        return True The code snippet provided defines a class called LoggingTransformer which inherits from ast.NodeTransformer. This class is responsible for transforming the Abstract Syntax Tree (AST) of a Python code snippet by adding a logger statement at the beginning of each function definition. The visit_FunctionDef method is an override of the default behavior in ast.NodeTransformer specifically for FunctionDef nodes in the AST. It is called when a FunctionDef node is encountered during the traversal of the AST. Inside visit_FunctionDef, there is a check to determine if the function requires logging by calling the requires_logging method. This method can be customized according to specific requirements to determine if a function should have logging added to it. If logging is required, a logger statement is created using an ast.Assign node. This assigns a new variable logger to the result of the logging.getLogger function call. The targets attribute of the ast.Assign node specifies the target of the assignment, which is an ast.Name node representing the variable logger. The value attribute of the ast.Assign node specifies the value being assigned, which is an ast.Call node representing the call to logging.getLogger with the function name as an argument. The lineno attribute of the ast.Assign node is set to the line number of the original function definition to ensure the transformed code maintains accurate line numbers. Finally, the logger statement is inserted at the beginning of the function body by using the insert method of the node.body list. This effectively adds the logger statement as the first statement in the function. The requires_logging method is defined to determine if a function requires logging. It can be customized by adding specific logic to check function attributes, docstrings, or function names. In the given example, the method simply returns True, indicating that logging is required for all functions. To actually transform the code and add logging statements, we can use the following approach: def add_logging(code):
    # Parse the code into an AST
    tree = ast.parse(code)

    # Transform the AST to add logging
    transformer = LoggingTransformer()
    transformed_tree = transformer.visit(tree)

    # Convert the transformed AST back to code
    transformed_code = ast.unparse(transformed_tree)

    return transformed_code The add_logging function takes a code snippet as input and performs the following steps: Parsing the code into an AST: The ast.parse function is used to parse the code and generate an Abstract Syntax Tree (AST) representation of the code. Transforming the AST to add logging: An instance of the LoggingTransformer class is created, which is responsible for transforming the AST by adding logging statements. The visit method of the LoggingTransformer is called on the AST, which traverses the tree and applies the necessary transformations. Converting the transformed AST back to code: The ast.unparse function is used to convert the transformed AST back into code. This function generates Python code from the AST representation. Returning the transformed code: The transformed code is returned as the result of the add_logging function. Let’s add a useful example to demonstrate the output of the add_logging function: def main():
    code = '''
def example_function():
    # Function body
    pass

class ExampleClass:
    def __init__(self):
        # Constructor body
        pass

    def example_method(self):
        # Method body
        pass
'''
    modified_code = add_logging(code)
    print(modified_code) The code above demonstrates the usage of the add_logging function on a specific code snippet. Let’s break it down step by step: The main function is defined as the entry point of the program. Inside the main function, a multi-line string code is defined, which contains a sample code snippet. This code snippet includes a function named example_function, a class named ExampleClass, and its associated constructor and method. The add_logging function is called with the code as an argument, which performs the transformation by adding logging statements to the code. The result of the transformation is stored in the modified_code variable. Finally, the modified code is printed to the console using the print function. Code transformation using Abstract Syntax Trees (AST) offers powerful capabilities for analyzing and modifying code at a structural level. With the help of the ast module in Python, developers can parse code into AST, traverse and manipulate its nodes, and convert it back to executable code. AST-based transformations enable tasks such as static code analysis, code generation, and refactoring. By leveraging AST, developers can gain deeper insights into their code, automate repetitive tasks, and improve code quality and maintainability. Conclusion In conclusion, using Python’s abstract syntax trees you can analyze and transform code effectively. It generates a tree representation of a program’s code, which can help identify potential issues and improve code quality and security. Earthly Cloud: Consistent, Fast Builds, Any CI Consistent, repeatable builds across all environments. Advanced caching for faster builds. Easy integration with any CI. 6,000 build minutes per month included. Get Started Free Daniel Boadzie As a passionate data scientist and full-stack web developer, I am constantly inspired by the power of data to drive meaningful insights and build robust solutions that can make a real impact. Whether it's working with complex data sets, developing machine learning models, or building intuitive web interfaces, I am dedicated to creating solutions that are both technically sound and user-friendly. Updated: September 11, 2023 Published: August 30, 2023 Get notified about new articles! We won't send you spam. Unsubscribe at any time. Subscribe to the Newsletter Subscribe You May Also Enjoy Abstract Base Classes in Python 43 minute read Learn how to create Abstract Base Classes (ABCs) in Python to enforce the implementation of certain methods or attributes in subclasses. ABCs promote code re... Introducing the New Features in Python 3.11 24 minute read Learn about the new features in Python 3.11, including improved error handling, enhanced type annotation, a new library for working with TOML files, and perf... Lunar Get a Demo Overview Earthfiles Docs About Earthfiles Earthly Satellites Satellites Pricing Check Status Resources Blog Newsletter Newsroom Videos & Webinars FAQ About Earthly Made with 🩵 on Planet Earth | We're hiring! Terms of Service | Privacy Policy | Security Made with 🩵 on Planet Earth | We're hiring! Terms of Service | Privacy Policy | Security

Source: https://dev.to/balapriya/abstract-syntax-tree-ast-explained-in-plain-english-1h38
Abstract Syntax Tree (AST) - Explained in Plain English - DEV Community Skip to content Navigation menu Search Powered by Search Algolia Log in Create account DEV Community Close Add reaction Like Unicorn Exploding Head Raised Hands Fire Jump to Comments Save Boost More... Moderate Copy link Copy link Copied to Clipboard Share to X Share to LinkedIn Share to Facebook Share to Mastodon Share Post via... Report Abuse Bala Priya C Posted on Dec 6, 2021 • Edited on Dec 28, 2022 Abstract Syntax Tree (AST) - Explained in Plain English #programming #codequality #compilers #codereview As a developer, the source code that you write is all so concise and elegant. And other developers can read and understand your code with ease. However, a compiler has to do much more to make sense of your code. In this post, you’ll learn how compilers try to understand what your code does with a focus on what an Abstract Syntax Tree (AST) is, and its relevance to static analysis. How Does the Compiler Make Sense of Your Code? The steps involved in the compiler’s processing of source code are illustrated below: Let’s expand on this a bit. The source code is first split into smaller chunks called tokens through the process of lexical analysis. Lexical analysis is also called tokenization. The tokens are then parsed into a tree called the syntax tree by the parser. An Abstract Syntax Tree (AST) abstracts away certain details and retains just enough information to help the compiler understand the structure of the code. Therefore, an AST is a tree data structure that best represents the syntactic structure of the source code. Don’t worry if things aren’t clear just yet! Over the next few minutes, you’ll draw a relatable analogy. And understand how the compiler works very similarly to how you'd try to understand a sentence. Lexical Analysis In this section, you’ll learn about lexical analysis. Suppose you’re learning a new language—not a programming language though😄. And you’re given the task of inferring the meaning of a sentence in that language. So how would you do it? As a first step, you’ll try identifying the nouns, verbs, or more generally, words that matter. Lexical analysis is very similar to this step. Given the source code, the compiler tries to first identify the different types of tokens that your code is composed of. A token could be any valid entity in the programming language—a literal that takes a fixed value, a variable, an operator, or a function call. As lexical analysis breaks down the source code into tokens, it is also called tokenization. Syntactic Analysis So far, you've learned that tokenization leaves you with tokens or entities—just the way you’d identify entities in a sentence. Let's go back to the analogy again. After you’ve identified the nouns and verbs in the sentence, how’ll you go about inferring its meaning? Well, you’ll now try to parse the relationship between the nouns, verbs and the like—to see how they fit together—how they conform to the language’s grammar. This leads to the step of syntax analysis or parsing. And to perform syntactic analysis, there’s a parser that processes these tokens and parses them into a tree called the Abstract Syntax Tree (AST). More on AST Representation The different entities and their relationships are often language-specific. For example, the syntactic structure of a sentence in German may be very different from its syntactic structure in Hindi. Similarly, there’s no one common AST representation and the actual AST structure may depend on the specific language. In general, the AST models the relationship between tokens in the source code as a tree comprising of nodes and the nodes containing children. And each node contains information about the type of token, and related data. For example, if your node represents a function call, the arguments and the return values will be the associated data. Let's draw the AST corresponding to the following equation: 2+(z−1) 2 + (z - 1) 2+(z−1) In the above AST representation, the nodes + and - are operators, z is a variable, and 1 and 2 are just literals. Notice how the parentheses are discarded in the AST; they're subsumed in the representation of (z - 1): z and 1 are both children of the - operator node. Want to explore more on ASTs? The AST Explorer helps in visualizing ASTs in several popular languages such as Go, Python, Java, and JavaScript. Here's a simple example in Python: 

my_str = "code more!"


 Enter fullscreen mode Exit fullscreen mode Here, the type of node is VariableDeclaration as we declare the variable my_str. Relevance of AST in Development So where do AST show up in the development process? In most languages, parsers that give you the AST will also give you the methods to traverse an AST. This would enable you to visit different nodes of the AST to understand the functionality of each node, and additionally perform analysis. So you can: define rules, traverse the parse tree by visiting each node, and check for violation of rules. And this is where ASTs are relevant in static code analysis. Static code analysis involves parsing of the source code into an intermediary representation—on which you can run analysis—without actually running the code. The intermediary representation is often the AST. This analysis then returns potential security issues, bugs, and performance issues in your code, which you can fix almost immediately. For more information on static analysis, consider reading the following post. What Is Static Code Analysis? Bala Priya C ・ Dec 6 '21 #beginners #programming #codequality #codereview Conclusion In this tutorial, you’ve learned: how lexical and syntactic analyses work, lexical analysis: identifies the tokens in the source code, and syntactic analysis: parses the relationship between these tokens to see how they fit together. how ASTs are tree representations of the source code, and how ASTs can help in static analysis Thank you for reading. Hope you found this post helpful! Top comments (3) Subscribe 
 Personal Trusted User Create template Templates let you quickly answer FAQs or store snippets for re-use. Submit Preview Dismiss Collapse Expand Sameerthe-Big-O Sameerthe-Big-O Sameerthe-Big-O Follow Software engineer and computer science student Joined Apr 30, 2024 • Jun 20 '24 Dropdown menu Copy link Hide Simple and clear i like that i love the analogy you use Like comment: Like comment: 2 likes Like Comment button Reply Collapse Expand Matheus Damião Matheus Damião Matheus Damião Follow Creative Front-end developer Location Rio de Janeiro, Brazil Joined Sep 7, 2021 • Sep 2 '24 Dropdown menu Copy link Hide This article was absolutely amazing! Perfect examples and pace! Thanks for it! Like comment: Like comment: 1 like Like Comment button Reply Collapse Expand Jacob W Runge Jacob W Runge Jacob W Runge Follow I've a Software Engineer at Precision Planting, working primarily with the front-end of the Radicle Agronomics hybrid-native app. I like Svelte and Capacitor and Rust. Location Illinois, US Work Software Engineer at Precision Planting Joined Nov 27, 2023 • Feb 1 '24 Dropdown menu Copy link Hide This helped me get my head on straight, thanks! I've started Nora Sandler's blog series on writing a compiler, and just couldn't quite wrap my head around ASTs. Like comment: Like comment: 1 like Like Comment button Reply Code of Conduct • Report abuse Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's permalink. Hide child comments as well Confirm For further actions, you may consider blocking this person and/or reporting abuse Read next How to use generics in pipe-and-combine Wolfgang Rathgeb - Dec 5 '24 Brain debugging. Interview with Anders Schau Knatten, author of "C++ Brain Teasers: Exercise Your Mind" Anastasiia Vorobeva - Dec 5 '24 Advanced Entity Extraction with Azure OpenAI: Harnessing Structured Outputs Umesh Tharuka Malaviarachchi - Dec 5 '24 Upload to S3 Shubhainder Singh - Dec 5 '24 Bala Priya C Follow Enjoys reading, writing, coding, and coffee.☕✨ Location India Joined Feb 28, 2021 More from Bala Priya C String Slicing in Python, Explained #python #programming #codenewbie #tutorial A 2021 Reflection Journal: My Tech Writing Journey, Learning, and More #beginners #codenewbie #programming #career PyDP: A Python Differential Privacy Library #python #tutorial #programming Thank you to our Diamond Sponsor Neon for supporting our community. DEV Community — A constructive and inclusive social network for software developers. With you every step of your journey. Home DEV++ Podcasts Videos Tags DEV Help Forem Shop Advertise on DEV DEV Challenges DEV Showcase About Contact Free Postgres Database Software comparisons Code of Conduct Privacy Policy Terms of use Built on Forem — the open source software that powers DEV and other inclusive communities. Made with love and Ruby on Rails. DEV Community © 2016 - 2025. We're a place where coders share, stay up-to-date and grow their careers. Log in Create account

Source: https://ieeexplore.ieee.org/document/4299919/#:~:text=Abstract%3A%20Abstract%20syntax%20trees%20(ASTs,model%20of%20the%20source%20code.
Abstract Syntax Trees - and their Role in Model Driven Software Development | IEEE Conference Publication | IEEE Xplore IEEE Account Change Username/Password Update Address Purchase Details Payment Options Order History View Purchased Documents Profile Information Communications Preferences Profession and Education Technical Interests Need Help? US & Canada: +1 800 678 4333 Worldwide: +1 732 981 0060 Contact & Support About IEEE Xplore Contact Us Help Accessibility Terms of Use Nondiscrimination Policy Sitemap Privacy & Opting Out of Cookies A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. © Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.

Results for query: Real-world case studies of using RAG to enhance productivity in codebases

Search Engine: Bing
Source: https://pixion.co/blog/designing-rag-application-a-case-study
Designing RAG Application: A Case Study Blog Designing RAG Application: A Case Study Are you interested in building a Retrieval-Augmented Generation (RAG) application? In previous articles, we introduced fundamental concepts such as retrieval strategies, database choices, vector search indices, and the Ragas evaluation framework. Now let's tie all this together into a comprehensive solution. Development15 min read StipanPetrovicStipan Petrovic2024-07-03 2024-07-03 AIAI Blog SeriesRetrieval Augmented Generation AI AI Blog Series Retrieval Augmented Generation Goals of the study Our primary goal was to evaluate various retrieval strategies and determine which ones yield the best performance in different contexts. We sought to develop a general-purpose RAG application that could be fine-tuned for specific use cases in a production environment. This involved selecting suitable strategies, an evaluation framework, and the right database and index configurations. Strategy selection and evaluation framework To ensure a thorough evaluation, we needed to select a range of retrieval strategies to test. These included basic index retrieval, hierarchical index retrieval, hypothetical questions, and context enrichment strategies such as sentence window retrieval and auto-merging retrieval. Each of these, as already mentioned, we have covered in previous articles, so if you do not know what they are, go check them out. Each strategy has its own strengths and weaknesses, making it essential to evaluate them against each other under controlled conditions. Our evaluation framework of choice was the Ragas framework. Ragas provided us with a robust set of metrics to measure performance, including context recall, faithfulness, relevance, and context precision. By using Ragas, we could objectively compare the effectiveness of each retrieval strategy and make data-driven decisions. If you want to know more, go check this out, we have a couple of articles on Ragas as well. Database selection Choosing the right database is extremely important when building any application, but even more so with RAG applications due to the high level of memory usage indices used in vector search incur. We explored both traditional and native vector databases, weighing their pros and cons, and we even wrote a short article about that. Traditional databases offered maturity and reliability, while native vector databases provided specialised environments for handling vectors efficiently. Postgres stood out to us as a vector database option due to its versatility and extensive indexing capabilities, including Flat, HNSW, and IVFFlat indices. These indices accommodate a range of requirements, from precise searches for smaller datasets to scalable, memory-efficient solutions for large-scale applications. Although, Postgres may not achieve the performance levels of specialised vector databases optimised for high-dimensional data searches and real-time processing and certain indices may result in higher memory consumption, for our case study it made perfect sense due to it being basically free on Azure for the amount of data in corpus we focused on. Domain selection When implementing a Retrieval-Augmented Generation (RAG) application, thoroughly analysing the given domain is critical for ensuring the application's success and effectiveness. In our case study, we had the flexibility to choose our domain, and we selected the legal domain, specifically focusing on the 54 titles of the US Code of Law. This decision was guided by several key considerations: Data Quality and Availability High-quality, comprehensive data is essential for accurate and contextually appropriate information retrieval. The legal domain, with its vast repositories of case law, statutes, and legal commentaries, offers rich, well-structured datasets often available in easily parseable formats like XML. Moreover, the legal domain's use of unambiguous and formal language ensures that every term is precise and every clause is clear, minimising the risk of misinterpretation. Model Versatility in Domain Knowledge A successful RAG application must have an underlying model that understands and incorporates domain-specific knowledge. Each domain has its own terminologies and concepts that the model must grasp to generate relevant content. The legal domain's extensive and well-documented data corpus ensures that general-purpose language models (LLMs) are well trained and acquainted with its specific terminology. This comprehensive coverage was a key factor in our selection of the US Code of Law, ensuring that the RAG application can produce reliable and authoritative outputs. For domains with less publicly available data, fine-tuning a general-purpose model is an option to ensure the RAG application meets the specific needs of that domain. Legal and Ethical Considerations Around Privacy Legal and ethical considerations are crucial, especially in sensitive or regulated domains. Hosting your LLM and embedding models in the EU can help ensure compliance with GDPR, as cloud providers in the EU are required to follow these regulations. However, to truly prevent AI companies from using data for training without consent, pseudonymisation is an effective approach. By using third-party SaaS pseudonymisation models, sensitive information like names and addresses can be extracted and replaced with unique IDs, keeping the data protected and compliant with regulations like GDPR, HIPAA, or CCPA. One might argue that this is merely choosing between who has access to your data, but these third-party services specialise in privacy solutions and are less likely to violate privacy terms. We did not have to worry about this due to our decision to focus on the US Code of Law, but in a production environment, you most certainly will have to, so be prepared and wait for one of our future articles where we will cover this topic in depth. Data ingestion After establishing the key considerations around the domain, the next step is to determine the scope of the data you want to use and how you intend to collect and manage it. Domains that experience frequent updates or rapid changes require a robust strategy for continuous data integration. For example, in the financial markets, keeping the application updated with the latest market data, regulatory changes, and economic indicators ensures that the generated content remains relevant and accurate. This ties into the importance of building a sound infrastructure for data ingestion, management, and versioning. The US Code of Law, our chosen domain, changes at a slower pace, making it an excellent candidate for our case study. This slower rate of change reduces the frequency and, with it, the cost of re-embedding data. Although versioning is crucial in the legal context to track when specific laws came into force, we decided to simplify our case study by not addressing versioning at this stage. Given our goal to move quickly and not spend excessive time on data collection and dataset maintenance, we chose to mock this part of the solution. In a production environment, having a robust data ingestion pipeline is vital. Although we explored various ETLT (Extract, Transform, Load, and Transform) solutions, we ultimately decided to assume the existence of a solution that would fill our blob storage with relevant documents. Despite the availability of many good ETLT solutions with free connectors for different data sources, these typically yield unstructured data that requires further refinement and poses challenges for parsing and chunking. The decision to mock the data ingestion process also stemmed from our desire to create a case study that is generic and easily reusable in a production environment. Data ingestion pipelines often end up being highly customised based on specific application needs and the nature of your data. By focusing on the core aspects of our RAG application rather than the intricacies of data ingestion, we aimed to save time and concentrate on elements that could be effectively reused in future implementations. Framework choice: Semantic Kernel vs. LangChain We initially experimented with LangChain, a mature and robust framework known for its extensive features and tools that facilitated the development of our RAG application. However, we soon found that Semantic Kernel, although newer, provided several compelling advantages that aligned more closely with our project needs. Our team is highly familiar with the Microsoft ecosystem, and Semantic Kernel's design is specifically tailored for .NET developers. This alignment significantly reduced the learning curve, accelerating our development process and allowing us to implement and test different retrieval strategies more efficiently. Additionally, the strategic benefits of Microsoft's partnership with OpenAI played a crucial role in our decision. The partnership provides seamless integration capabilities, such as creating an instance of ChatGPT on Azure with just a few clicks. This ease of deployment, along with the robust support infrastructure offered by Microsoft and OpenAI, made Semantic Kernel an attractive choice for our project, ultimately guiding our decision to adopt it over other frameworks. Chunk size and overlap considerations In our exploration of Retrieval-Augmented Generation (RAG) applications, we also investigated how chunk size and chunk overlap influence performance. Chunking is a critical process in preparing documents for vector search, ensuring that text inputs stay within token limits for embedding models. Proper chunk sizing is essential not only for maintaining the integrity of the content, but also for reducing noise. Larger chunks tend to encapsulate more context, potentially including irrelevant or extraneous information, which can introduce noise and reduce the precision of the search and retrieval processes. Conversely, smaller chunks can minimise noise by focusing more narrowly on specific content, but they may also risk losing valuable contextual information necessary for accurate understanding. Therefore, finding an optimal chunk size is crucial to balancing between preserving context and minimising noise, enhancing the overall performance of the RAG system. We used the TextChunker from Microsoft.SemanticKernel.Text to create document chunks. To evaluate performance, we experimented with various chunk sizes of 256, 512, 1024, 1536 and 2048 tokens, along with different overlap sizes. This testing aimed to find the optimal balance between chunk size and overlap to maintain context and enhance retrieval accuracy. Testing multiple strategies with varying chunk sizes and overlaps significantly increased the number of embeddings required, presenting a challenge in managing and processing the data efficiently. This sets the stage for the next section, where we address the challenges and solutions related to embedding. Embedding strategy costs and need for optimisation To embed a document using various strategies, we follow specific steps for each: Basic, Hierarchical, Hypothetical Questions, Sentence Window, and Auto-Merging. When combined with variable chunk sizes and overlaps, we end up with 125 different combinations, meaning a single document could be embedded 125 times. Costs can escalate quickly: for example, the US Code of Law contains approximately 64 million words, with an average words-to-token ratio of x1.365. Using OpenAI's text-embedding-3-large model at $0.00013 per 1,000 tokens, embedding this corpus once costs around $8.43. If we assume each of the 125 combinations costs the same as the cheapest one—a highly optimistic estimate—the total cost would be approximately $1,053.75. However, this is most definitely an underestimate, as some strategies include interacting with an LLM in their process, which can be even pricier. Now imagine working with high volume of high velocity data that needs to be re-embedded frequently. This emphasises the need for robust, fail-safe solutions to avoid redundant processes and highlights the importance of optimising the embedding workflow to manage costs effectively, while also being able to precisely estimate the cost of embedding a corpus of text. Reducing operations to optimise embedding Each strategy involves some of the four distinct operations: chunking (C), summarising (S), question framing (Q), and embedding (E). Let's break down each of the strategies and the operations they require to try and identify optimisation opportunities. This is an oversimplification that ignores multiple chunk sizes and overlaps, which would result in many more small trees; however, the concept can be demonstrated nonetheless. Let's now break these trees down into individual branches and select only distinct ones among them. Out of seven individual branches, we are left with just four. If we now join these back into a tree by going through levels of these branches one by one and joining them into a single node if the operation type matches, we will end up with this: Right off the bat, we can see that we managed to reduce the number of embed operations from 7 to 4, a 42.86% reduction, which is huge. Further on, even though the chunking process is not charged since it is something we do ourselves locally, it still results in a reduction in time and RAM expenditure. Here we reduced number of chunking operations from 7 to 2, a 71.43% reduction. Again, this is a simplification since we actually have 125 small trees like the ones shown in the first picture, but these percentages are still applicable and valid. One other thing worth mentioning, this is useful in the scope of our case study since we wanted to benchmark strategies with multiple different chunk sizes and overlaps, but it is not far fetched to imagine having a production solution like this which allows users to pick and choose different strategies and chunk sizes and overlaps for different types of documents. Or even multiple strategies for the same document for different types of queries. In scenarios like that, this approach is still very useful. Designing a solution for executing operations Now that we have established what operations need to be done in order to embed a document, let's explore ways to do it. We receive a user request for embedding a certain document in all possible combinations of strategies and chunk sizes and overlaps, what next? Well, we cannot process this request right away, this is a long-running process because embedding takes quite some time. Models that we use have a maximum throughput defined as tokens per minute, which acts as a bottleneck that slows down the process. For that reason, we will have to use some kind of background service. So, when a user sends a request, we create an operation tree, store it in the database, and return 200 OK to the user, letting him know his request will be processed. You can notice we used Mongo. The reason for that, as already stated, is the fact that we wanted to move fast, and storing objects like trees in mongo is trivial as opposed to relational database. Background service based solution design Let's try to figure out how this background service should look like. We have multiple different operations we need to be able to execute: chunking, question framing, summarising, and embedding. The question arises: how should we segment our code? Do we create a single background service type that executes all these operations itself, or do we create multiple background services, one for each operation type? Well, it depends on what we want to achieve. As we already mentioned we wanted to move fast in producing this case study but we also wanted to create a scalable solution that can easily become production ready application. Both of these options can scale horizontally, the first one offers simplicity since it reduces infrastructure needs for communication between services, but the second one offers more granularity in scaling. Even though models act as bottlenecks, meaning that you will definitely need quite few instances of each model before your background services cannot keep up with them and they require another instance, it is not an impossible scenario. For that reason, we chose the second approach because, as we said, we wanted this case study to be transferable to a production environment and we wanted to keep our options open. Also it separates the code into more clear logical units with clear responsibilities, making it more readable and maintainable. The overhead of handling multiple background services and their need for communication, for the time being, we solved by hosting the services within same service host. So now we know we have multiple background services, each with its own responsibility. We also introduced a queue for each background service to draw tasks from and we ended up with something like this: In this setup, chunking background service loads operation tree from the database, it executes the chunking operation and enqueues messages for summarisation, question framing, and embedding. Summarising and question framing services execute their part of the process, sending requests to the LLM and enqueuing results for the embedding service to consume. Finally, embedding background service embeds the chunks by sending them to the embedding model. But this solution has its problems. Mainly, we have no way of tracking how much progress has been made on a single task. It could be minutes or hours, but we have no way of knowing. Progress tracking To be able to track the progress of an embedding job optimised with an operations tree, we need to know how many steps each operation tree node has. The issue is you cannot know that until you chunk your document, since the number of chunks will determine how many discrete tasks like question framing or summarising you will have. So we repurposed chunking background service in a way that, besides chunking and enqueuing those chunks, it also calculates the number of steps for the rest of the tree nodes, and we gave it a new name to reflect its newly gained responsibilities - the preprocessing background service. Now we can have all other background services update the state of the nodes upon processing a message and thus report back the amount of progress that has been made. The only issue now is that we have multiple processes competing for the same resource in the database. We can lock the resource to ensure that the number of steps is incremented correctly, but we risk creating a bottleneck if the load gets too high. For that reason, we created a new status tracking background service along with its queue that dequeues multiple status messages sent by other background services and batch updates the database, not only eliminating the risk of contention but also reducing the number of roundtrips to the database. Note that newly created Preprocessing Background Service now updates tree nodes with number of steps. Price estimate Another problem we had to solve was predetermining the price of embedding a document a certain way. These models can get pricey and if you’re dealing with a domain with high volume and high velocity, you need a way to know the amount of money embedding something will cost you. The main issue we had to solve here was the fact that there are some unknowns that you’re dealing with. Models are usually priced by the number of input tokens in case of embedding models and both input and output tokens in case of LLMs. The issue is when you turn chunks into questions and summaries, you do not know how much output tokens this will generate, and thus you do not know the exact price. For that reason we turn to heuristics. The number of input tokens you will send out towards models is something you can calculate, but for the number of output tokens, you have to rely on estimates based on the average number of output tokens each type of operation generates for a certain number of input tokens. Make sure to note that the number of input tokens is larger than the number of tokens in the document due to optimisations we made with the operations tree, since now we are executing embedding for multiple strategies and chunk size combinations at once. This offered us two paths we can take. One was to precisely calculate the number of input tokens in the operation tree and, based on estimates, try to calculate the number of output tokens that will produce as well as the price. But this approach is problematic because to determine the exact number of input tokens you need to execute preprocessing and chunk the entire document you're trying to embed, and that takes time. If you’re dealing with a large enough document it will be tens of seconds and that amounts to a bad user experience. The other approach is to base the estimate on the number of tokens in the document only, information that can be made readily available, and the types of operations our strategy selection will result in. This gives a slightly less precise estimate, but executes momentarily. To be able to track previous token consumption we need a similar solution like we had with status tracking since number of output tokens is available only with model response, and for that reason, we added yet another background service and accompanying queue. And there we have it, this solution is what we needed before proceeding to the next step, which is actual embedding and testing the quality of our RAG implementation. To be fair, there is still room for improvement as this is not a production-ready solution, but it can easily be made so. These background services can be containerised and hosted individually, internal queues we use to exchange messages can be replaced with a message broker that will offer queue message persistence, and Postgres database can be replaced with a native vector database if needed, but the core of the RAG application is there: a robust, scalable document processing pipeline. Conclusion In this article, we tried to illustrate all the different choices you will have to make when embarking on a journey of building a RAG application. We highlighted the importance of selecting a proper strategy and setting up an evaluation framework so you can make informed decisions when trying to improve your solution. We also talked about the importance of data quality and availability, issues of privacy, and other issues you will encounter when tackling a certain domain. And finally, we proposed a solution design that will be robust, scalable, and easily portable to real world scenarios outside this case study. All this was to set the stage for our next article, where we will put this solution to the test along with multiple different retrieval strategies in an attempt to find out how they perform on our dataset of choice, that is US Code of Law. Stay tuned for more! Get blog post updates Sign up to our newsletter and never miss an update about relevant topics in the industry. Sign up Hi, our name is — PIXIONOur best — WORKGet in — TOUCHOpen — POSITIONSRead our — BLOG Get in — TOUCH[email protected] 2025 Pixion Cookie Policy Legal information

Source: https://www.aegona.com/software-development/top-10-use-cases-retrieval-augmented-generation-rag-p2
Top 10 Use cases of Retrieval-Augmented Generation (RAG) – P2 | Aegona Skip to main content +84 28 7109 2939 [email protected] QTSC9, Quang Trung Software City, District 12, Ho Chi Minh City, Viet Nam Main navigation Home About Us Our Services Careers Professional Graduate Blog Contact Blog Breadcrumb Home Software Development Top 10 Use cases of Retrieval-Augmented Generation (RAG) – P2 Top 10 Use cases of Retrieval-Augmented Generation (RAG) – P2 One of the standout technologies gaining attention is Retrieval-Augmented Generation (RAG), a method that integrates the power of large language models (LLMs) with the ability to retrieve information from various data sources. To evaluate the potential of RAG in real-world applications, this article provides in-depth insights into 5 use cases of RAG, with specific examples illustrating its potential and impact. I. Top 10 use cases of RAG (P2) In Part 1, we introduced five use cases of RAG: Finance, Legal, Healthcare, Agriculture, and Pharmaceuticals. Now, let’s explore five more use cases: 1. RAG Applications in the Energy Sector In today’s digital age, using artificial intelligence (AI) to enhance efficiency and accuracy in the energy industry is increasingly vital. One of the most advanced technologies drawing attention is Retrieval-Augmented Generation (RAG). According to the study Natural Language Interaction with a Household Electricity Knowledge-based Digital Twin, the combination of LLMs and RAG significantly improves the accuracy and reliability of responses related to energy data. When users inquire about electricity consumption, RAG not only relies on the natural language generation capabilities of LLMs but also retrieves information from a knowledge graph containing consumption data. This means the answers provided are not only easy to understand but also more precise. When comparing the performance of various models like ChatGPT, Gemini, and Llama, the study reveals that RAG delivers higher-quality answers, especially for complex or nuanced questions. Imagine needing detailed information on household electricity consumption; you wouldn’t just receive general data but deep, actionable insights. However, it is undeniable that the research has certain limitations, such as relying on a specific dataset and potential biases in the LLM's training data. Therefore, further research is needed to explore the sustainability of RAG and LLMs in the generated responses. We look forward to future advancements in this field! 2. RAG Applications in Science When it comes to science, one of the biggest challenges researchers face is the vast amount of information from large-scale experiments. For instance, in studies related to the Electron Ion Collider (EIC), millions of pages of documents and data can overwhelm newcomers. This is where Retrieval-Augmented Generation (RAG) becomes an ideal companion. Recent research has introduced the RAGS4EIC system, an intelligent AI assistant capable of summarizing information related to EIC. One standout benefit of RAGS4EIC is its user-friendliness. The system helps researchers easily explore information about EIC without wading through a sea of documents. Furthermore, the study shows that RAG can handle complex queries, especially those related to physics equations. The future of RAGS4EIC promises optimization for various LLMs and expanded databases with additional resources, providing many benefits such as summarizing scientific information and helping researchers easily access and understand complex knowledge. 3. RAG Applications in Education In today’s digital era, learning is no longer confined to books and lectures. An intriguing study has shown that using chatbots can enhance the learning experience for students. For example, one study revealed that the chatbot Prof. Leodar was developed using Retrieval-Augmented Generation to help students grasp complex concepts. Students noted that the chatbot provided clear explanations and made it easier for them to apply the knowledge. Even learning analytics showed that students used Prof. Leodar more during exams and outside of official study hours. What makes Prof. Leodar stand out is its ability to provide personalized support. Thanks to its RAG architecture, this chatbot can deliver accurate and relevant responses tailored to specific subjects. Compared to conventional chatbots like ChatGPT, Prof. Leodar was rated higher for the accuracy and relevance of its answers. Of course, this remains a controversial topic, as educators argue that relying on such technologies may lead students to become dependent and more passive in their thinking. 4. RAG Applications in the Construction Industry The construction industry is one of the key sectors of the global economy. Currently, it faces many challenges in optimizing productivity. To overcome these obstacles, artificial intelligence (AI) technology, particularly Retrieval-Augmented Generation (RAG), has been applied. According to a report, some of the key benefits of RAG in construction include: Improving information accuracy and quality: RAG allows systems to collect and analyze data from various sources, such as technical documents, research reports, or system software data. As a result, the summaries generated are clear and easy to understand, helping staff quickly find the necessary information without wasting time. Reducing risks in construction processes: When information is compiled with higher reliability, workers can make faster and more accurate decisions. This not only enhances work efficiency but also improves the quality of the final product. Supporting innovative product development: RAG helps engineers design more creative ideas by analyzing and synthesizing data from multiple sources. Instead of relying solely on personal experience, they can leverage information from previous construction projects, research documents, and other systems to find optimal solutions more quickly. 5. RAG Applications in Real Estate Today, the exploitation of geographical data has become more critical than ever. Retrieval-Augmented Generation (RAG) technology not only helps process information but also opens new opportunities for investors and consumers. An interesting study titled CHATMAP: Large Language Model Interaction with Cartographic Data highlights its potential for the real estate sector. Natural interaction with geographical data: This research developed a system that allows users to inquire about geographic locations using natural language. This means that even individuals without technical knowledge can easily access information from data. Generating detailed area insights: The system uses large language models (LLM) to create question-and-answer pairs based on source data. For example, if you want to know which areas have the most coffee shops, the system will quickly provide accurate information. Applications in real estate analysis: RAG technology helps real estate analysts identify key factors in pricing and project development, creating more opportunities for investors and customers. Diverse applications: The system’s applications go beyond real estate. It can also serve urban planning and tourism recommendations, helping users find locations of interest. >> Read more: Sales Technology Trends 2024: Integrating AI into CRM software How Much Is The Cost Of Mobile App Development In Vietnam? Although this research is just a beginning, it demonstrates the potential for applying RAG in real estate. Further efforts will focus on developing more comprehensive systems with a deeper understanding of the characteristics and culture of each area. II. Final Thought Aegona has now introduced you to the top five use cases of RAG actively transforming digitally with the support of GenAI and making these technologies "smarter" with RAG. If you haven’t seen Part 1, click here for a more comprehensive overview. AI Solutions provided by Aegona include: AI development based on OpenAI, Microsoft Azure AI, TensorFlow, H2O.ai, Vertex AI, Keras, Amazon SageMaker, etc. Outsourcing of AI-integrated system software Mobile app development with intelligent virtual assistants Website design integrated with smart chatbots If you are interested in our services, please contact Aegona at the following: AEGONA LTD - IT SERVICE & SOFTWARE DEVELOPMENT Email: contact@aegona.com Phone: Office: (+84) 28 7109 2939. Hotline: (+84) 91 451 8869 | (+84) 83 940 5469 WhatsApp: (+84) 91 451 8869 Website: www.aegona.com, www.aegona.vn Address: Orbital Building (QTSC Building 9), Quang Trung Software City, District 12, Ho Chi Minh city, Vietnam. Retrieval Augmented Generation AI Development Company In VietNam related post Software Development DeepSeek AI: The AI Assistant & Applications for Business Discover DeepSeek, the advanced AI assistant revolutionizing technology. Learn how DeepSeek can enhance productivity, automate tasks, and deliver intelligent solutions Software Development Programming Trends 2025: Insights and Predictions In the ever-evolving world of technology, programming trends play a critical role in shaping the future of innovation. As we bid farewell to 2024, it’s time to look back at the trends that defined the year and anticipate what’s ahead in 2025. Let’s explore the most popular programming trends of 2024 and the emerging trends that will dominate the tech landscape. Software Development Staff Augmentation vs Outsourcing: What Works Best for Your Business Strategy? Discover the differences between staff augmentation and project outsourcing. Learn which model best suits your business goals and IT needs. Software Development Top 5 Open-Source Document Management Systems [Update 2025] Looking for the best open-source and self-hosted document management software? Explore the top five open-source solutions that provide free document management. Software Development How 6G Transforms Mobile App Development? 6G empowers app developers and businesses to push boundaries like never before. Let’s explore how this next-generation network is transforming the app development landscape. Software Development Top 5 Shopify Development Companies And Agencies In Vietnam Discover the top 5 Shopify development companies in Vietnam offering custom app solutions, SEO-friendly designs, and multi-channel integrations to boost your online store's performance. Software Development Top 5 Odoo Development Companies in Viet Nam [Update 2024] In this article, we will explore the top 5 Odoo development companies in Vietnam 2024, highlighting their expertise and contributions to various industries. Life in Aegona AEGONA | Company Trip 2024 at Mandala Mui Ne, Phan Thiet Company Trip 2024 was an unforgettable experience for Aegona's team, filled with bonding, relaxation, and new memories. Software Development What’s New In Odoo 18: A Quick Overview of New Features With enhancements across sales, eCommerce, CRM, inventory, and more, Odoo 18 offers everything you need to streamline operations and drive growth. Software Development Top 5 Mobile App Development Companies in Vietnam Let’s take a look at the top 5 mobile app development companies in Vietnam and see what makes them stand out. Software Development Top 10 Use Cases of Retrieval-Augmented Generation (RAG) - P1 Let's explore five use cases of RAG is applied, such as finance, legal, healthcare, agriculture, and pharmaceuticals. Software Development Top 4 Third-Party eCommerce Modules on Odoo's App Store You Need to Know Sometimes you need specialized tools to handle the unique demands of your e-commerce operations. That’s where Odoo eCommerce modules come in handy! Software Development AI Development Company In VietNam | Schedule A Free Consult Now! Aegona is advancing its software development services, focusing on AI development using existing AI platforms like MyChatGPT. Read more! Software Development What is RAG? RAG Application Software Development Services What is RAG? How does this technology work, and how can Aegona leverage RAG to benefit businesses in Vietnam and globally? Let’s explore this in the article. Life in Aegona Smart Delivery Vietnam 2024 | RECAP New Trends in Logistics Smart Delivery Vietnam 2024 | RECAP New Trends in Logistics: cross-border e-commerce, reverse logistics, passive insulated shipping containers, and more! connect with us 100% CUSTOMER SUPPORT THERE'RE SEVERAL WAYS TO CONNECT WITH US You can reach our customer service at 84-28-71092939 or [email protected] For additional assistance, we offers the following support channels Contact Us contact us Full Name Company name Business phone Email Message AEGONA © 2023 | PRIVACY POLICY QTSC Building 9, Quang Trung Software City, Tan Chanh Hiep Ward, District 12, Ho Chi Minh City, Viet Nam +84914518869 [email protected] COMPANY About Us Why Choose Us Our Work Team Contact Us OUR SERVICES Web Application Development Mobile App Development Cloud-Based Development Custom Software Development OUR EXPERTISE Microsoft Technology UI/UX Design CMS and eCommerce Case Study Resources Blog News Professional Graduate (+84) 914 518 869 (+84) 28 7109 2939

Source: https://www.recursive.house/blog/rag-examples-the-future-of-ai-driven-information
RAG Examples: Shaping the Future of AI Information HomeAbout Services AI Consulting Services Case StudyBlogsCONTACT US Table of contents: toc-link: Style me, then remove me RAG Examples: The Future of AI-Driven Information ‍ Introduction Retrieval-Augmented Generation (RAG) is changing how we find and use information with AI. Recursive House is a company that helps make RAG implementations better. These examples help computers answer questions and suggest things you might like. RAG examples make it easier to find information quickly and accurately. They're useful for many things, like helping students learn, lawyers do research, and writers create stories. RAG examples also help businesses work faster by finding information quickly. In the future, RAG examples will make it even easier for people to use and understand lots of information. This is exciting because it will change how we learn and work with computers. ‍ Interactive RAG Diagram Interactive RAG Models RAG (Retrieval-Augmented Generation) combines search and writing to help AI give better answers. Here are some real-world RAG examples and RAG use cases: Customer Help Chatbots RAG helps chatbots answer questions better. For example, if you ask about a laptop that's getting too hot, the chatbot can find info about overheating and give you good advice. Shopping Recommendations RAG can help you find the right things to buy. If you ask about a camera for taking pictures of animals, it can look up reviews and tell you which ones are best. News Writing RAG helps write news stories faster. It can grab the latest info from different places and put it all together in a news article. Smart Tutors RAG makes learning tools smarter. An AI tutor can find the right info to help explain things to students in a way they understand. Doctor's Helper RAG can help doctors by finding important medical info. It can look up new treatments and help doctors make good choices for their patients. Smart Assistants RAG makes virtual assistants smarter. They can find up-to-date info about things like weather or events and tell you about them. Legal Research Tools RAG helps lawyers find important documents faster. It can search for laws and court cases and give short summaries to save time. These RAG LLM examples show how retrieval augmented generation is making AI solutions better in many areas. It helps machines find and use information more easily, which makes them more helpful to people. As more companies use RAG, it will keep making things better in customer service, shopping, news, schools, hospitals, and more. ‍ Adaptive RAG Models Adaptive RAG models make AI systems smarter by knowing when to look up information based on what you ask. This helps them give better answers. Here are some real-life examples of how these models are used: OpenAI's ChatGPT ChatGPT is a great example of a RAG model. It finds information and creates human-like responses to your questions. This helps it answer many different types of questions well. Medical Help Systems Doctors use RAG models to help diagnose patients. The system looks up medical studies and creates advice based on what it finds. This helps doctors make better decisions. Customer Support Chatbots Many companies use RAG models in their chatbots. These chatbots can find information and give detailed answers to customer questions. For example, if you ask how to fix a product, the chatbot can give you step-by-step instructions. Legal Research Tools Lawyers use RAG models to find important legal documents quickly. The system finds documents and creates short summaries of the main points. This makes lawyers' work faster and easier. Personal Learning Helpers Schools use RAG models to help students learn better. These systems can find explanations and study materials that fit each student's needs. If a student is having trouble with a topic, the system can explain it in a way they understand. Code Writing Tools Programmers use RAG models to help write code. The system finds useful code examples and creates new code to solve specific problems. This helps programmers work faster and better. News Summary Makers News companies use RAG models to create up-to-date news summaries. These systems find the latest news from different sources and create short summaries. This helps people stay informed about what's happening in the world. These RAG examples show how AI can be used in many different ways. They help make information easier to find and use, which makes work more efficient and helps people learn and understand things better. ‍ RAG in Healthcare ‍ RAG in Healthcare Retrieval-Augmented Generation (RAG) is making healthcare better by helping doctors make better decisions, supporting medical research, and giving patients personalized care. Here are some real-life examples of RAG in healthcare: Making Diagnoses More Accurate RAG helps doctors figure out what's wrong with patients faster and better. It looks at a patient's health history and compares it with medical guidelines. For example, if a patient has symptoms that could mean different things, RAG can help the doctor choose the right diagnosis by looking at all the patient's information. Helping with Medical Research RAG makes medical research easier by finding important studies and summarizing what they found. This helps researchers work faster. For instance, researchers use RAG to gather information from many studies about a specific health problem, making it easier to understand the big picture. Giving Personalized Treatment Advice RAG looks at things like a patient's health history, genes, and lifestyle to suggest treatments that are just right for them. For example, a patient with diabetes might get a special plan for medicines, food, and exercise that fits their needs perfectly. Making Patients Happier RAG helps doctors and patients talk better by making messages more personal. This makes patients feel better about their care. Patients might get special messages on their phones about their treatment or appointments, which are written just for them. Watching Patients from Far Away RAG is really good at helping doctors keep an eye on patients who aren't in the hospital. It can look at information from patients right away and tell doctors if something's wrong. For example, patients with long-term health problems might wear devices that send their health information to doctors. RAG can check this information and tell doctors if the patient needs help right away. In conclusion, RAG is making healthcare better by helping doctors make better choices, supporting research, and giving patients care that's just right for them. It's a powerful tool that's making patients healthier and hospitals work better. RAG examples, RAG LLM examples, and RAG use cases show how Retrieval Augmented Generation is being used in real life to solve problems and make things better. These AI solutions use information retrieval and generative models to give personalized recommendations and answer questions in smart ways. They help with things like making content, managing knowledge, and understanding information in context. This makes chatbots, educational tools, and research systems work better. RAG is improving how we use information, making things more accurate and efficient in many different areas. ‍ Legal and Financial Use Cases Retrieval-Augmented Generation (RAG) is a cool new technology that's helping lawyers and bankers do their jobs better. Here are some real-life examples of how RAG is being used. Legal Use Cases Document Analysis and Summarization Lawyers use RAG to quickly find important information in big legal documents and make short summaries. Example: A law firm uses RAG to look at old cases about a client. It finds important information and makes summaries to help lawyers get ready for new cases. This RAG example helps them use old information to plan better for court. eDiscovery Optimization RAG helps lawyers find and understand documents faster during legal investigations. Example: A company that makes medical stuff uses RAG to learn from old lawsuits about their products. It finds important papers and makes summaries, helping lawyers give better advice more quickly. Compliance Audits RAG helps companies follow the law by finding and checking important legal documents. Example: When a company is updating its rules to follow new laws, RAG finds old legal advice and lawyer opinions. This helps make sure the new rules follow the law, making the whole process easier. Financial Use Cases Portfolio Management RAG helps manage investments by giving up-to-date market information and personalized advice. Example: An investment company uses RAG to look at current market news. When big things happen, like changes in interest rates, RAG finds important information and tells managers how it might affect different investments. This RAG use case helps them make smart choices quickly. Fraud Detection and Prevention RAG helps find fraud by looking at money transfers in real-time. Example: A bank uses RAG to watch for weird patterns in money transfers between countries. It compares old records with new ones to find possible fraud faster than before, saving money. Credit Scoring and Risk Assessment RAG makes credit scores better by combining a company's own information with other financial data. Example: A new finance company uses RAG to look at a customer's spending history, credit reports, and big economic trends. This retrieval augmented generation example helps them decide if it's safe to lend money to more people. These RAG examples show how this AI solution is making work easier and better in law and finance. By using real-time information and creating new content, companies can work faster, follow rules better, and help their clients more. RAG is changing how these jobs are done, making them more efficient and effective. ‍ RAG in Education ‍ RAG in Education Retrieval-Augmented Generation (RAG) is transforming how students learn, making education more interactive and personalized. Here are some great examples of how RAG is being used in schools and learning environments: Smart Study Buddy A university developed a system that helps students understand books better. Students can ask questions about what they're reading, and the system finds answers from digital books. This helps students learn faster and grasp concepts more effectively. Math Helper Researchers have built a RAG system that assists with solving math problems. Using trusted math resources, it helps students tackle subjects like algebra and geometry. Feedback shows students prefer this system over traditional help because it’s more precise and reliable. Personal Learning Apps Some educational apps leverage RAG to create tailored learning experiences. These apps answer questions, adapt quizzes based on performance, and adjust to the student’s pace, making learning more engaging and effective. Smart Tutoring Systems Online tutoring platforms are integrating RAG to provide instant support. When students ask a question, the system retrieves relevant information and explains it thoroughly, acting like an always-available, knowledgeable tutor. School Helper Robots Virtual assistants powered by RAG are being used in schools to assist with tasks like finding information about classes, homework, or summarizing lessons. It’s like having a super-intelligent friend to support students throughout the school day. RAG is revolutionizing education by making it more interactive and customized for individual learners. It simplifies complex topics, provides precise answers, and turns studying into a more engaging process. Companies like Recursive House are advancing this technology further, combining RAG with other AI systems to create tools tailored to the needs of students and educators. By doing so, they’re helping schools deliver better learning experiences and making education more enjoyable for everyone. ‍ E-commerce and Customer Support with RAG Retrieval-Augmented Generation (RAG) is enhancing shopping experiences and customer support by making them more efficient and personalized. Here are some practical examples of how businesses are leveraging RAG: Better Product Suggestions RAG helps online stores recommend products by analyzing search queries and customer reviews. Example: Searching for "best running shoes under $100" prompts RAG to gather product information and user feedback, generating a curated list of suitable options. Smarter Customer Help Chatbots RAG-powered chatbots deliver more accurate and tailored responses to customer inquiries. Example: A telecom provider uses a RAG chatbot to assist with troubleshooting, such as resolving slow internet issues with step-by-step solutions. Answering Common Questions Faster RAG quickly retrieves up-to-date answers for frequently asked questions, streamlining customer service. Example: Asking an online store about its return policy triggers RAG to provide the latest details, reducing the need for manual assistance. Keeping Track of Orders RAG offers real-time order updates, keeping customers informed about their packages. Example: A clothing store uses RAG to respond to queries like "Where is my order?" by providing accurate tracking details and estimated delivery times. Personal Marketing Messages RAG tailors marketing communications based on customer preferences and purchase history. Example: An online grocery store sends personalized emails about organic products to customers who frequently buy them. Understanding Customer Feedback RAG analyzes customer reviews to extract actionable insights for improving services. Example: A hotel chain uses RAG to summarize feedback across platforms, identifying strengths and areas for improvement to enhance guest experiences. RAG is revolutionizing e-commerce and customer support by enabling smarter product recommendations, efficient issue resolution, and personalized communication. This technology improves customer satisfaction while helping businesses operate more effectively. ‍ RAG in Journalism Journalism and Media Applications with RAG Retrieval-Augmented Generation (RAG) is transforming journalism and the way we consume news by enhancing efficiency, accuracy, and personalization. Here are practical applications of RAG in the field: Automatic News Writing RAG assists news outlets in creating articles quickly by gathering and synthesizing information from various sources. Example: When major events occur, RAG compiles key details from multiple sources to produce timely news articles, ensuring rapid dissemination of information. Making Long Stories Short RAG condenses lengthy reports into concise summaries by identifying the most important details. Example: A news organization uses RAG to create digestible summaries of complex topics, helping readers understand critical information more easily. Better Fact-Checking RAG aids journalists in verifying the accuracy of claims by cross-referencing data with reliable sources. Example: When a public figure makes a statement, RAG quickly checks the facts to confirm its validity, improving the credibility of news stories. Personal News for Everyone RAG tailors news delivery based on individual interests, curating stories that align with readers' preferences. Example: A personalized news app uses RAG to recommend articles about topics a user enjoys, making the news more engaging and relevant. News You Can Talk To Interactive platforms powered by RAG enable readers to ask questions about articles and receive additional information. Example: A news website uses RAG to provide detailed answers when users inquire about specific aspects of a story, enhancing reader understanding. RAG is revolutionizing journalism by streamlining article creation, improving fact-checking, and delivering personalized content. These advancements ensure that news is faster, more accurate, and tailored to individual interests, benefiting both media companies and readers. ‍ Future Trends and Potential Improvements in RAG Systems Retrieval-Augmented Generation (RAG) continues to evolve, offering exciting possibilities and advancements. Here's what the future holds for RAG systems: Using Different Types of Information Future RAG systems will integrate diverse data sources like images, videos, and audio alongside text, enabling richer and more detailed responses. Example: A RAG-powered assistant could analyze a photo or video to answer questions about its content. Always Learning RAG systems will continuously update their knowledge in real time without requiring complete retraining, ensuring they remain current. Example: A news RAG system could immediately incorporate breaking stories into its knowledge base. Being Fair and Trustworthy Efforts are underway to eliminate biases in RAG systems, ensuring equitable and unbiased outputs for all users. Example: Developers are refining algorithms to avoid favoring specific demographics or viewpoints. Faster and More Interactive Enhanced speed and conversational capabilities will make RAG systems more engaging and user-friendly. Example: A customer service chatbot powered by RAG could hold natural, seamless dialogues with users. Better at Understanding Questions Improved contextual understanding will help RAG systems accurately interpret complex or ambiguous queries. Example: When asked, "What's the impact of climate change on agriculture?" a RAG system could provide a detailed and precise explanation. Checking Their Own Work Future RAG systems will self-verify their outputs, increasing reliability and reducing errors. Example: Before presenting an answer, the system could cross-check sources to ensure accuracy. New Ways to Measure How Well They Work Developers are designing advanced evaluation metrics to assess and improve RAG systems' performance. Example: Testing methods that measure contextual accuracy, response clarity, and relevance will help refine system effectiveness. These advancements will enhance RAG's versatility and application across various domains. Examples include smarter chatbots, personalized educational tools, and improved legal or business information retrieval systems. Future use cases might involve tailored recommendations for media or advanced support for tasks like homework and research. As AI technology progresses, RAG will revolutionize content creation, learning, and information management, making these processes faster, more efficient, and enjoyable for everyone. ‍ Conclusion AI is revolutionizing how we access answers and retrieve information, with Recursive House at the forefront of this transformative field. Through innovative technologies, Recursive House develops advanced systems that provide personalized recommendations and streamline content creation by efficiently analyzing vast amounts of data and deriving meaningful insights. Their expertise in Retrieval-Augmented Generation (RAG) enables the creation of intelligent solutions that deliver accurate, context-aware responses and make information more accessible. These RAG-powered systems are applied across various domains, from customer service chatbots to educational tools and legal research platforms, significantly enhancing performance and usability. Looking ahead, AI and RAG systems promise even greater advancements. They will empower businesses with smarter tools and simplify information discovery for individuals. Recursive House remains committed to driving innovation, ensuring we maximize the potential of these groundbreaking technologies. ‍ ‍ Share this post Recursive House Recursive House provides consulting and development services tocompanies looking to integrate AI technology deeply into their companyoperations. Using our expertise we teach and build tools for companies to outcompete in marketing, sales, and operations. Thank you! Your submission has been received! Oops! Something went wrong while submitting the form. Latest Posts RAG Architecture: Real-Time AI Access Retrieval-Augmented Generation (RAG) architecture has become a transformative force in AI, reshaping how businesses access and utilize real-time information Read More AI Use Cases: Compelling Reasons to Embrace AI AI is transforming various industries with practical applications. For instance, it enhances customer interactions by personalizing communication and aids decision-making through data analysis. Read More AI Development Companies Transforming Industries Globally AI software development companies are revolutionizing many industries. They help businesses improve efficiency by using advanced computer programs to streamline tasks and processes. Read More Trusted Clients Helping Clients Big and Small with Clarity & Results Drop us a line, coffee’s on us What's better than a good conversation and a cappaccino? Address Toronto, Ontario NameEmail Your BudgetHow did you find us?Massage Thank you! Your submission has been received! Oops! Something went wrong while submitting the form. Looking for More Information? Download our latest report on the AI market to gain valuable insights, understand emerging trends, and explore new opportunities. Stay ahead in this rapidly evolving industry with our comprehensive analysis. Thank you! Your submission has been received! Oops! Something went wrong while submitting the form. View all News Lets Chat About Your Future Unlock the power of AI with Recursive House’s tailored AI/ML and GenAI services. Our expert team follows a proven development process to deliver innovative, robust solutions that drive business transformation. Thank you! Your submission has been received! Oops! Something went wrong while submitting the form. HomeServicesCase StudyBlogContact

Source: https://buzzclan.com/data-engineering/retrieval-augmented-generation/
Retrieval-Augmented Generation: Ultimate Guide Skip to content Toggle Navigation Home About Services IT Infrastructure Business Intelligence Managed IT Services Data And Analytics Cyber Security Database As A Service Cloud Consulting Staffing Services Digital Transformation QA Services Data Engineering Workforce Management ServiceNow Mobile App Healthcare Data Services View All Services Services IT Infrastructure Services Cyber Security Services Database As A Service Cloud Consulting Staffing Services Digital Transformation All Services Company About Careers Privacy Policy Applicant Privacy Policy CCPA DIR-ITSAC Blogs Contact Us Retrieval-Augmented Generation: Revolutionizing AI with Dynamic Knowledge Integration Home Data Engineering Retrieval-Augmented Generation: Revolutionizing AI with Dynamic Knowledge Integration Ananya Arora Nov 25, 2024 Introduction In the rapidly evolving landscape of artificial intelligence and natural language processing, Retrieval-Augmented Generation (RAG) has emerged as a groundbreaking approach to enhancing the capabilities of large language models. As organizations and researchers seek to unlock the full potential of AI-driven language understanding and generation, RAG stands out as a powerful technique that bridges the gap between vast knowledge bases and dynamic, context-aware responses. This comprehensive exploration of Retrieval-Augmented Generation aims to provide readers with a deep understanding of RAG, its inner workings, and its transformative impact on natural language processing. From its fundamental principles to real-world applications, we will explore RAG’s intricacies and illuminate its role in shaping the future of AI-driven communication and problem-solving. Throughout this piece, we will navigate through several key areas: defining RAG and its historical context, unraveling the technical aspects of how it functions, examining its myriad benefits, exploring diverse applications across industries, providing implementation strategies, showcasing real-world case studies, and peering into future trends and ongoing research. By the end of this journey, readers will gain a comprehensive understanding of the Retrieval-Augmented Generation and its potential to revolutionize how we interact with and leverage artificial intelligence in our daily lives and professional endeavors. What is Retrieval-Augmented Generation (RAG)? Retrieval-augmented generation (RAG) is an innovative approach in natural language processing that combines the power of large language models with the ability to retrieve and incorporate external knowledge. At its core, RAG is a hybrid system that enhances the generation capabilities of AI models by allowing them to access and utilize relevant information from a vast corpus of data during the text generation process. The fundamental idea behind RAG is to augment the inherent knowledge of a pre-trained language model with dynamically retrieved information. This approach addresses one of the key limitations of traditional language models: their reliance solely on the knowledge embedded in their parameters during training. By incorporating a retrieval mechanism, RAG enables models to tap into up-to-date, context-specific information, leading to more accurate, relevant, and informative outputs. The concept of RAG can be traced back to the ongoing efforts in the AI community to create more robust and reliable language models. As researchers and developers grappled with the challenges of maintaining accuracy and relevance in rapidly changing information landscapes, the need for a system that could dynamically incorporate new knowledge became apparent. The evolution of RAG is closely tied to advancements in information retrieval systems, question-answering models, and large language models. Early work in this direction focused on improving question-answering systems by retrieving relevant passages from large document collections. As language models grew in size and capability, researchers began exploring ways to combine these models with retrieval mechanisms to enhance their performance on knowledge-intensive tasks. A significant milestone in the development of RAG came with the publication of the paper “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks” by Lewis et al. in 2020. This work introduced a unified framework for retrieval-augmented generation and demonstrated its effectiveness across various natural language processing tasks. Since then, RAG has gained considerable attention in the AI community, with numerous researchers and organizations building upon and refining the concept. The historical context of RAG is also intertwined with the broader trends in AI and NLP. As the limitations of purely generative models became more apparent, especially in tasks requiring up-to-date or specialized knowledge, the AI community began exploring hybrid approaches that could leverage both learned representations and external knowledge sources. RAG emerged as a promising solution to this challenge, offering a flexible and powerful framework for integrating retrieval and generation. In recent years, the rapid advancement of large language models like GPT-3, BERT, and their successors has further accelerated the development and adoption of RAG. These models’ impressive generative capabilities provided an ideal foundation for RAG systems. By combining the fluency and coherence of large language models with the ability to retrieve and incorporate relevant information, RAG opened up new possibilities for creating more intelligent and adaptive AI systems. As we delve deeper into RAG’s workings and applications, it’s important to recognize its place in the broader context of AI development. RAG represents a significant step towards creating more versatile, knowledgeable, and context-aware AI systems. It addresses some key challenges traditional language models face while paving the way for more advanced and capable AI assistants and tools. How Does Retrieval-Augmented Generation Work? Retrieval-augmented generation operates on a sophisticated mechanism that seamlessly integrates information retrieval with text generation. To understand how RAG works, it’s essential to break down its core components and processes and examine how they interact to produce enhanced outputs. At a high level, RAG consists of two primary components: a retriever and a generator. The retriever is responsible for identifying and extracting relevant information from a large corpus of data. At the same time, the generator uses this retrieved information along with its pre-trained knowledge to produce the final output. Let’s delve into each component and the overall process in more detail. The Retriever The retriever component is designed to efficiently search a vast database of documents or knowledge sources to find information relevant to the given input or query. This process typically involves several steps: Indexing: Before retrieval can occur, the knowledge base must be indexed. This involves creating a searchable representation of each document or information, often using techniques like dense vector embeddings or inverted indices. Query Processing: When a new input or query is received, the retriever processes it to create a search-friendly representation. This might involve generating embeddings for the query or extracting key terms. Similarity Matching: The processed query is compared against the indexed documents to find the most relevant matches. This can be done using various similarity metrics, such as cosine similarity for vector embeddings or BM25 for term-based retrieval. Ranking and Selection: The retrieved documents or passages are ranked based on relevance, and a subset is selected for generation. The Generator The generator component is typically a large language model pre-trained on vast text data. In the context of RAG, this model is adapted to work with the retrieved information. The generator’s role includes: Contextual Understanding: Processing the original input or query to understand the context and requirements of the task. Information Integration: Incorporating the retrieved information into its generation process. This often involves attending to both the input and the retrieved documents simultaneously. Text Generation: Producing coherent and relevant text that addresses the input while leveraging the retrieved information. The RAG Process The overall RAG process can be summarized in the following steps: Input Reception: The system receives an input, which could be a question, a prompt, or any text that requires a response. Retrieval: The input is passed to the retriever, which searches the knowledge base and returns relevant documents or passages. Augmentation: The retrieved information is combined with the original input to create an augmented context. Generation: The augmented context is fed into the generator, which produces the final output. Output Delivery: The generated text is returned as the response to the original input. RAG Architecture The architecture of a RAG system is designed to facilitate efficient interaction between the retriever and generator components. Key aspects of the architecture include: Encoder-Decoder Framework: Many RAG systems utilize an encoder-decoder architecture, where the encoder processes the input and retrieved documents, and the decoder generates the output. Attention Mechanisms: Sophisticated attention mechanisms allow the generator to focus on relevant input parts and retrieve information during generation. Parametric and Non-Parametric Knowledge: RAG combines the parametric knowledge embedded in the pre-trained generator with the non-parametric knowledge accessed through retrieval. End-to-end Training: Advanced RAG systems can be trained end-to-end, simultaneously optimizing the retriever and generator components. Caching and Efficiency Optimizations: To improve performance, RAG systems often incorporate caching mechanisms and other optimizations to reduce latency in retrieval and generation. The intricate interplay between retrieval and generation in RAG systems allows for a dynamic and adaptive approach to natural language processing tasks. By leveraging pre-trained knowledge and dynamically retrieved information, RAG can produce more accurate, relevant, and up-to-date responses than traditional language models. As we continue to explore RAG’s benefits and applications, it’s important to remember this underlying mechanism. RAG’s power and versatility come from its ability to retrieve and integrate relevant information on the fly, enabling it to tackle a wide range of knowledge-intensive tasks with improved accuracy and context awareness. Benefits of Retrieval-Augmented Generation Retrieval-augmented generation offers many advantages that address key limitations of traditional language models and open up new possibilities in natural language processing. Let’s explore the primary benefits of RAG in detail: Improved Accuracy One of the most significant benefits of RAG is its ability to dramatically improve generated content’s accuracy. This improvement stems from several factors: Up-to-date Information: RAG can respond based on the latest data by retrieving information from a continually updated knowledge base, reducing the risk of outdated or incorrect information. Fact Verification: The retrieval component allows the system to cross-reference generated content against source documents, enhancing factual accuracy. Reduced Hallucination: Traditional language models sometimes generate plausible-sounding but incorrect information, a phenomenon known as “hallucination.” RAG mitigates this by grounding responses in retrieved facts. Contextual Relevance: By incorporating context-specific information retrieved for each query, RAG can produce more accurate and relevant responses tailored to the user’s specific needs. Improved Specificity: RAG enables models to provide more detailed and specific information by drawing upon a vast knowledge base rather than relying solely on generalized knowledge. Enhanced Knowledge Integration RAG excels at integrating external knowledge into the generation process, offering several advantages: Expanded Knowledge Base: While traditional language models are limited to the knowledge embedded in their parameters during training, RAG can access a much larger and more diverse pool of information. Domain Adaptation: RAG systems can easily adapt to specialized domains by incorporating relevant domain-specific knowledge bases without requiring extensive retraining of the entire model. Multi-source Integration: RAG can seamlessly combine information from multiple sources, providing a more comprehensive and nuanced understanding of complex topics. Dynamic Learning: As the knowledge base is updated, RAG can immediately incorporate new information into its responses without retraining the entire model. Transparency and Explainability: By explicitly retrieving and using external information, RAG can provide clearer explanations for its outputs, enhancing trust and interpretability. Efficiency and Scalability RAG offers significant advantages in terms of efficiency and scalability: Reduced Model Size: By offloading some knowledge to external databases, RAG can achieve high performance with smaller, more efficient language models. Faster Updates: Updating the knowledge base is quicker and more straightforward than retraining large language models, allowing for more frequent and agile knowledge updates. Resource Optimization: RAG can optimize resource usage by retrieving only the necessary information for each query rather than storing all knowledge within the model parameters. Scalable Knowledge Expansion: The retrieval component allows for virtually unlimited expansion of the knowledge base without increasing the size of the core language model. Improved Inference Speed: In many cases, RAG can provide faster inference times compared to extremely large language models, especially for knowledge-intensive tasks. Versatility and Adaptability RAG systems demonstrate remarkable versatility across various applications: Task Flexibility: RAG can be applied to various NLP tasks, from question-answering and summarization to content generation and dialogue systems. Language and Domain Adaptability: By changing the retrieval corpus, RAG can easily adapt to different languages, cultures, and specialized domains. Personalization: RAG can incorporate user-specific or organization-specific knowledge bases, enabling highly personalized interactions. Hybrid Approaches: RAG can be combined with AI techniques, such as reinforcement learning or few-shot learning, to create even more powerful and adaptive systems. Enhanced Ethical and Legal Compliance RAG offers several benefits in terms of ethical and legal considerations: Source Attribution: By explicitly retrieving information, RAG can more easily provide source attributions for its outputs, addressing concerns about copyright and intellectual property. Bias Mitigation: The ability to retrieve information from diverse sources can help reduce biases present in pre-trained models. Content Moderation: RAG systems can be more easily constrained to retrieve from approved sources, helping to ensure the generation of appropriate and compliant content. Auditability: The retrieval process provides an additional layer of transparency, making auditing and verifying the system’s outputs easier. As we continue to explore RAG’s applications and implementation strategies, these benefits highlight why it has become such a promising approach in natural language processing. The combination of improved accuracy, enhanced knowledge integration, efficiency, versatility, and ethical considerations makes RAG a powerful tool for addressing many challenges in developing advanced AI systems for language understanding and generation. Applications of Retrieval-Augmented Generation Retrieval-augmented generation has found various applications across various domains, revolutionizing how we approach knowledge-intensive NLP tasks and enhancing the capabilities of large language models. Let’s explore some of the key applications of RAG: Knowledge-Intensive NLP Tasks RAG has proven particularly effective in tasks that require access to extensive and up-to-date knowledge: Question Answering: RAG excels in open-domain question answering, where it can retrieve relevant information from a large corpus to provide accurate and detailed answers. This is particularly useful in applications like customer support chatbots, educational tools, and research assistants. Fact-Checking and Verification: By retrieving and cross-referencing information from reliable sources, RAG can verify claims and detect misinformation, making it a valuable tool for journalists, researchers, and fact-checking organizations. Information Synthesis: RAG can gather and synthesize information from multiple sources to create comprehensive reports, literature reviews, or state-of-the-art summaries on specific topics. Named Entity Recognition and Linking: RAG can enhance named entity recognition by retrieving contextual information about entities, improving accuracy in tasks like entity linking and disambiguation. Text Summarization: RAG can produce more accurate and informative summaries of long documents or multiple related texts by retrieving key information and context. Large Language Models RAG has significantly enhanced the capabilities of large language models: Knowledge Augmentation: RAG allows large language models to augment their built-in knowledge with retrieved information, enabling them to provide more accurate and up-to-date responses. Specialized Domain Adaptation: By incorporating domain-specific knowledge bases, RAG enables large language models to quickly adapt to specialized fields like medicine, law, or engineering without extensive retraining. Multilingual and Cross-lingual Applications: RAG can help language models bridge language gaps by retrieving and translating relevant information, enabling more effective cross-lingual communication and understanding. Content Generation: In tasks like article writing, report generation, or creative writing, RAG can provide language models with relevant facts, statistics, and context to produce more informative and accurate content. Dialogue Systems: RAG enhances the ability of conversational AI to engage in more informed and context-aware dialogues, making them more useful in applications like virtual assistants, customer service, and educational chatbots. Corrective and Active Retrieval RAG has opened up new possibilities in information retrieval and correction: Dynamic Fact Correction: RAG systems can actively retrieve and correct outdated or incorrect information in real time, ensuring that generated content remains accurate and up-to-date. Contextual Information Retrieval: RAG enables more sophisticated information retrieval by understanding the context of queries and retrieving information that is not just keyword-matched but contextually relevant. Iterative Refinement: Some RAG implementations use active retrieval techniques to iteratively refine their search and generation process iteratively, leading to more precise and relevant outputs. Bias Detection and Mitigation: By actively retrieving from diverse sources, RAG can help identify and mitigate biases in generated content, promoting more balanced and fair outputs. Source Triangulation: RAG can retrieve information from multiple sources to cross-verify facts and provide a more comprehensive view of complex topics. Industry-Specific Applications RAG has found applications across various industries: Healthcare: In medical applications, RAG can assist in diagnosis by retrieving relevant case studies, research papers, and treatment guidelines. It can also help in drug discovery by synthesizing information from vast databases of chemical compounds and clinical trials. Legal: RAG enhances legal research and contract analysis by retrieving relevant case laws, statutes, and precedents. It can also assist in due diligence processes by quickly synthesizing information from large volumes of legal documents. Finance: In financial services, RAG can be used for market analysis, risk assessment, and fraud detection by retrieving and analyzing vast amounts of financial data and news. Education: RAG powers intelligent tutoring systems to retrieve and present relevant educational content based on a student’s queries or learning needs. It can also assist in curriculum development and research. E-commerce: RAG enhances product recommendation systems by retrieving and synthesizing product information, user reviews, and market trends to provide more accurate and personalized recommendations. Journalism and Media: RAG assists in fact-checking, source verification, and content creation by retrieving relevant information from diverse sources and synthesizing it into coherent narratives. Scientific Research: In research applications, RAG can help with literature reviews, hypothesis generation, and experimental design by retrieving and synthesizing information from vast scientific databases and publications. Manufacturing and Engineering: RAG can assist in technical documentation, troubleshooting, and design processes by retrieving relevant specifications, standards, and best practices from extensive technical libraries. Emerging Applications As RAG technology continues to evolve, new and innovative applications are emerging: Augmented Creativity: RAG enhances creative processes in advertising, content creation, and product design by providing relevant inspirations, trends, and contextual information. Personalized Learning: Advanced RAG systems are being developed to create highly personalized learning experiences by retrieving and adapting educational content based on individual learning styles, preferences, and progress. Virtual Assistants and Digital Twins: RAG is enhancing the capabilities of virtual assistants and digital twins, allowing them to provide more accurate, context-aware, and personalized responses by retrieving user-specific or organization-specific information. Automated Journalism: Some news organizations are experimenting with RAG-powered systems that can generate news articles by retrieving and synthesizing information from multiple sources, with human editors providing oversight and final editing. Policy Analysis and Decision Support: Governments and organizations are exploring using RAG in policy analysis and decision-making processes, leveraging its ability to retrieve and synthesize complex information from diverse sources. Implementing Retrieval-Augmented Generation Implementing Retrieval-Augmented Generation (RAG) requires careful planning and consideration of various factors. This section will provide strategies, introduce tools and libraries, and offer a step-by-step tutorial for implementing RAG in various systems. Implementation Strategies Define Your Use Case: Identify the NLP task or problem you want to address with RAG. This will guide your choices regarding model architecture, retrieval strategy, and knowledge base selection. Choose Your Knowledge Base: Select or create a knowledge base relevant to your use case. This could be a collection of documents, a structured database, or a combination of both. Consider factors like data quality, update frequency, and coverage. Select a Retrieval Method: Choose an appropriate retrieval method based on your use case and knowledge base. Options include: – Dense Retrival: Using dense vector embeddings for both queries and documents. – Sparse Retrieval: Traditional keyword-based methods like BM25 or TF-IDF. – Hybrid Approaches: Combining dense and sparse retrieval for better performance. Choose a Generation Model: Select a pre-trained language model that fits your size, performance, and specialization requirements. Popular choices include GPT models, T5, or BART. Design the RAG Architecture: Decide on the overall architecture of your RAG system. This includes how the retriever and generator components interact, how retrieved information will be incorporated into the generation process and any additional components like rerankers or fusion mechanisms. Implement Efficient Indexing: To ensure fast retrieval, implement efficient indexing techniques for large-scale applications. This may involve using specialized data structures or distributed indexing systems. Optimize for Latency and Resource Usage: Consider techniques like caching frequently retrieved information, batching queries, or quantization to reduce the model size and inference time. Implement Monitoring and Logging: Set up robust monitoring and logging systems to track the performance of your RAG implementation, including retrieval accuracy, generation quality, and system latency. Plan for Continuous Improvement: Develop a strategy for continuously updating your knowledge base and fine-tuning your models based on new data and user feedback. Tools and Libraries Several tools and libraries are available to facilitate the implementation of RAG systems: Hugging Face Transformers: Provides pre-trained models and tools for retrieval and generation tasks. It offers easy-to-use interfaces for implementing RAG with models like BART and T5. LangChain : It is a popular framework for developing applications with large language models, including RAG implementations. It provides abstractions for working with different retrieval and generation components. Haystack: It is an open-source framework specifically designed for building RAG systems. It offers modular components for document storage, retrieval, and generation. PyTorch and TensorFlow: These deep learning frameworks provide the foundation for implementing custom RAG architectures and training pipelines. FAISS: It (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors, useful for implementing dense retrieval systems. Elasticsearch: A popular search engine that can be used for implementing sparse retrieval in RAG systems. OpenAI API: Provides access to powerful language models that can be used as the generation component in RAG systems. Pinecone: A vector database service that efficiently stores and retrieves dense vector embeddings. Step-by-Step Tutorial Here’s a basic tutorial for implementing a simple RAG system using Python and some popular libraries: Set up the environment: ```python !pip install transformers pytorch faiss-cpu import torch from transformers import DPRQuestionEncoder, DPRContextEncoder, RAG, RagTokenizer, RagSequenceForGeneration ``` Prepare the knowledge base: ```python # For this example, we'll use a small in-memory knowledge base knowledge_base = [ "The capital of France is Paris.", "The Eiffel Tower is located in Paris.", "Paris is known as the City of Light." ] # Create FAISS index for efficient retrieval question_encoder = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base") ctx_encoder = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base") # Encode the knowledge base ctx_embeddings = ctx_encoder(knowledge_base, return_tensors="pt").pooler_output index = faiss.IndexFlatIP(ctx_embeddings.size(1)) index.add(ctx_embeddings.detach().numpy()) ``` Set up the RAG model: ```python model = RagSequenceForGeneration.from_pretrained("facebook/rag-sequence-nq") tokenizer = RagTokenizer.from_pretrained("facebook/rag-sequence-nq") # Replace the default retriever with our custom one model.set_retriever(index, question_encoder, ctx_encoder, knowledge_base) ``` Generate responses ```python def generate_response(question): input_ids = tokenizer(question, return_tensors="pt").input_ids output = model.generate(input_ids) return tokenizer.decode(output[0], skip_special_tokens=True) # Example usage question = "What is the capital of France?" response = generate_response(question) print(f"Question: {question}") print(f"Response: {response}") ``` This tutorial provides a basic implementation of RAG. In a real-world scenario, you must consider factors like scaling the knowledge base, handling updates, and fine-tuning the model for your specific use case. Remember that implementing RAG often requires iterative refinement and optimization. Start with a simple implementation and gradually enhance it based on performance metrics and user feedback. Case Studies and Real-World Examples Let’s explore some case studies and real-world examples from various industries and use cases to better understand the practical applications and impact of retrieval-augmented generation. Case Study 1: Enhancing Customer Support with RAG Company: (redacted) Inc. is a large technology company providing software solutions to businesses. Challenge: (redacted) Inc. struggled with the increasing volume and complexity of customer support queries. Based on a traditional language model, their existing chatbot system often provided generic or outdated responses, leading to customer frustration and increased workload for human support staff. Solution: The company implemented a RAG-based customer support system. They used their extensive knowledge base of product documentation, FAQs, and past support tickets as the retrieval corpus. The system was designed to understand customer queries, retrieve relevant information, and generate detailed, context-aware responses. Implementation: Knowledge Base Preparation: They indexed their entire support documentation, including product manuals, troubleshooting guides, and resolved ticket histories. Query Understanding: Implemented a fine-tuned BERT model to classify incoming queries into relevant categories. Retrieval System: A dense retrieval system based on sentence transformers was used to find the most relevant documents for each query. Generation Model: Employed a fine-tuned T5 model as the generator, which could incorporate retrieved information into its responses. Human-in-the-Loop: Implemented a confidence scoring system, routing complex queries to human agents when necessary. Results: 40% reduction in average response time for customer queries. 30% increase in first-contact resolution rate. 25% reduction in escalations to human agents. 90% positive feedback from customers on the accuracy and helpfulness of responses. Significant reduction in the workload of human support staff, allowing them to focus on more complex issues. Key Learnings: Regularly updating the knowledge base was crucial for maintaining accuracy. Implementing a feedback loop from human agents to improve the system’s performance over time was highly beneficial. Transparency in AI-generated responses, including source citations, increased customer trust. Case Study 2: RAG for Medical Research Assistance Organization: MedResearch Institute, a leading medical research facility. Challenge: MedResearch Institute researchers needed help keeping up with the rapidly growing medical literature. They needed a system that could assist in literature reviews, hypothesis generation, and identifying potential research directions. Solution: The institute developed a RAG-based research assistant system that could process natural language queries, retrieve relevant information from medical databases and journals, and generate comprehensive summaries and insights. Implementation: Data Integration: Integrated multiple medical databases, including PubMed, clinical trial registries, and internal research repositories. Specialized Embeddings: Developed domain-specific embeddings trained on medical literature to improve retrieval accuracy. Multi-step Retrieval: Implemented a multi-step retrieval process that identified relevant papers and extracted key information. Abstractive Summarization: A biomedical-specific T5 model fine-tuned on medical literature was used to generate summaries and insights. Citation and Fact-Checking: Implemented a system to provide citations for generated content and cross-verify facts across multiple sources. Results: 50% reduction in time spent on initial literature reviews for new research projects. 35% increase in relevant papers identified for each research query. 28% improvement in hypothesis generation, as measured by the number of novel research directions pursued. 95% accuracy in generated summaries, as verified by domain experts. Significant increase in cross-disciplinary insights due to the system’s ability to connect information from various medical subfields. Key Learnings: Domain-specific training and fine-tuning were crucial for high performance in specialized medical fields. Explaining and citing sources for generated content was essential for building trust among researchers. Continuous updates to the knowledge base and model were necessary to keep up with the rapidly evolving field of medical research. Examples and Best Practices Let’s understand the key best practices for RAG systems via examples. Legal Research and Contract Analysis Law firms use RAG systems to assist in legal research and contract analysis. These systems can quickly retrieve relevant case laws, statutes, and precedents, significantly reducing the time lawyers spend researching. Best practices include: Regularly updating the legal database to include the latest rulings and legislative changes. Implementing strict data security measures to protect sensitive legal information. Training the system to understand and interpret legal jargon and complex legal concepts. Personalized Education EdTech companies are leveraging RAG to create adaptive learning platforms. These systems can retrieve relevant educational content based on a student’s current knowledge level and learning style. Best practices include: Developing a diverse knowledge base that covers various learning styles and difficulty levels. Continuously implement a feedback mechanism to improve content recommendations based on student performance. Ensuring that retrieved content is age-appropriate and aligns with curriculum standards. Financial Analysis and Reporting Financial institutions are using RAG systems for market analysis and report generation. These systems can retrieve and synthesize information from various financial sources to produce comprehensive reports. Best practices include: Implementing real-time data integration to ensure the most up-to-date financial information is used. Developing robust fact-checking mechanisms to verify financial data and claims. Creating clear templates and guidelines for generated reports to ensure consistency and compliance with financial reporting standards. Content Creation and Journalism Media organizations are experimenting with RAG systems to assist in content creation and fact-checking. These systems can retrieve relevant information, verify claims, and even generate drafts of news articles. Best practices include: Implementing strong editorial oversight to ensure the quality and accuracy of AI-generated content. Developing clear guidelines for source attribution and transparency in AI-assisted journalism. Training the system to identify and mitigate potential biases in retrieved information and generated content. Technical Documentation and Knowledge Management Technology companies are using RAG systems to manage and retrieve technical documentation. These systems can assist developers and engineers in finding relevant information quickly. Best practices include: Implementing version control in the knowledge base to manage documentation for different software versions. Developing domain-specific retrieval models that understand technical jargon and concepts. Creating a user feedback system to improve retrieved information’s relevance and accuracy. These case studies and examples demonstrate the versatility and potential of the Retrieval-Augmented Generation across various industries. They highlight the importance of domain-specific adaptation, continuous learning, and human oversight in implementing successful RAG systems. As organizations continue exploring and refining RAG applications, we expect to see even more innovative use cases emerge, further transforming how we interact with and leverage information in our professional and personal lives. Future Trends and Research As the Retrieval-Augmented Generation evolves, several emerging trends and ongoing research directions shape its future. These developments promise to enhance RAG systems’ capabilities further and expand their applications across various domains. Emerging Trends Trends Description Multimodal RAG Current research focuses on extending RAG beyond text to include other modalities such as images, audio, and video. This would allow RAG systems to retrieve and generate content across different media types, opening up new possibilities in visual question answering, audio transcription, and video captioning. Conversational RAG There's a growing interest in developing RAG systems that can maintain context over multiple conversation turns. This would enable more natural and coherent interactions in applications like chatbots and virtual assistants. Federated RAG Researchers are exploring federated RAG systems to address privacy concerns and enable collaborative learning. These systems would allow multiple organizations to train and improve RAG models without sharing sensitive data. Quantum-Inspired RAG As quantum computing advances, quantum-inspired algorithms could enhance the retrieval component of RAG systems, potentially leading to faster and more accurate information retrieval. Explainable RAG There's an increasing focus on making RAG systems more transparent and interpretable. This includes developing methods to explain why certain information was retrieved and how it influenced the generated output. Adaptive RAG Future RAG systems may dynamically adapt their retrieval and generation strategies based on the task, user preferences, and available computational resources. Cross-Lingual and Multilingual RAG Researchers are working on RAG systems that can seamlessly operate across multiple languages, retrieving information in one language and generating responses in another. Ongoing Research Efficient Retrieval at Scale: As knowledge bases grow larger, efficient retrieval becomes increasingly challenging. Ongoing research is focused on developing more scalable retrieval methods, including: Hierarchical retrieval architectures Learned index structures Approximate nearest neighbor search techniques Dynamic Knowledge Incorporation: Researchers are exploring ways to make RAG systems more adaptable to new information. This includes: Continuous learning approaches that can update the retrieval and generation components in real-time Methods for quickly incorporating new knowledge without full retraining Retrieval Augmented Training: Beyond using retrieval during inference, ongoing work is being done on incorporating retrieval into the training process. This could lead to more knowledgeable and efficient language models. Improved Integration of Retrieved Information: Current research is focusing on better ways to integrate retrieved information into the generation process, including: More sophisticated attention mechanisms Methods for resolving conflicts between retrieved information and the model’s built-in knowledge Bias Mitigation in RAG Systems: Researchers are investigating techniques to identify and mitigate biases in RAG systems’ retrieval and generation components. RAG for Specialized Domains: There’s ongoing work on adapting RAG systems for highly specialized domains like scientific research, legal analysis, and medical diagnosis. This includes developing domain-specific retrieval and generation models. Long-form Generation with RAG: Researchers are exploring ways to use RAG to generate longer, more coherent text, such as full articles or reports. Meta-learning for RAG: There is interest in developing RAG systems that can quickly adapt to new tasks or domains with minimal fine-tuning using meta-learning techniques. Significant Papers: Several recent papers have made important contributions to the field of Retrieval-Augmented Generation: “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks” (Lewis et al., 2020): This seminal paper introduced the RAG framework, demonstrating its effectiveness across various NLP tasks. It laid the groundwork for much of the subsequent research in this area. “REALM: Retrieval-Augmented Language Model Pre-Training” (Guu et al., 2020): This paper proposed a method for incorporating retrieval into the pre-training process of language models, showing improvements in both efficiency and performance. “Dense Passage Retrieval for Open-Domain Question Answering” (Karpukhin et al., 2020): While not specifically about RAG, this paper introduced significant improvements in dense retrieval methods, which have been widely adopted in RAG systems. “Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering” (Izacard and Grave, 2021): This work proposed the Fusion-in-Decoder approach, which improved the integration of retrieved passages in the generation process. “Retrieval-Enhanced Machine Learning” (Metzler et al., 2021): This paper provided a broader perspective on retrieval-augmented machine learning, discussing applications beyond NLP and potential future directions. “Improving Language Models by Retrieving from Trillions of Tokens” (Borgeaud et al., 2022): This work from DeepMind demonstrated the potential of scaling up the retrieval corpus to trillions of tokens, showing significant improvements in language model performance. “RETRO: Improving Language Models by Retrieving from Trillions of Tokens” (Borgeaud et al., 2022): This paper introduced a new architecture for retrieval-enhanced language models that can efficiently scale to large retrieval databases. “Atlas: Few-shot Learning with Retrieval Augmented Language Models” (Izacard et al., 2022): This work explored using RAG for few-shot learning, showing how retrieval can enhance a model’s ability to adapt to new tasks with limited examples. These ongoing research directions and emerging trends highlight the dynamic nature of the field and the significant potential for further advancements in the Retrieval-Augmented Generation. As researchers continue pushing the boundaries of what’s possible with RAG, we expect to see more powerful, efficient, and versatile systems that can handle increasingly complex tasks across various domains. Conclusion Throughout this exploration, we’ve delved into RAG’s core concepts, examining its inner workings, benefits, and wide-ranging applications across various industries. We’ve seen how RAG addresses key limitations of conventional language models, such as the inability to access up-to-date information and the tendency to generate inaccurate or hallucinated content. By grounding generated responses in retrieved information, RAG systems can provide more reliable, factual, and contextually relevant outputs. The benefits of RAG are manifold. RAG offers advantages over traditional approaches, from improved accuracy and enhanced knowledge integration to increased efficiency and scalability. Its ability to dynamically incorporate external knowledge makes it particularly well-suited for knowledge-intensive tasks and specialized domains, where access to accurate and up-to-date information is crucial. We’ve explored diverse applications of RAG across industries, from enhancing customer support and assisting in medical research to revolutionizing legal research and personalized education. These case studies and examples demonstrate RAG’s versatility and transformative potential in solving real-world problems and improving decision-making processes. The implementation of RAG, while complex, is becoming increasingly accessible thanks to the growing ecosystem of tools, libraries, and frameworks. As we’ve discussed, successful implementation requires careful consideration of various factors, from choosing the right knowledge base and retrieval method to designing an effective integration between retrieval and generation components. Looking to the future, the field of RAG is rich with promising research directions and emerging trends. From multimodal and conversational RAG to quantum-inspired approaches and explainable systems, ongoing research continues to push the boundaries of what’s possible with this technology. As RAG systems become more sophisticated, we expect more innovative applications and use cases to emerge. However, as with any powerful technology, the development and deployment of RAG systems come with responsibilities. Issues of bias, privacy, and ethical use of information must be carefully considered and addressed. The challenge lies in harnessing the power of RAG while ensuring its responsible and beneficial use for society. FAQs What is the difference between RAG and traditional language models? RAG combines a retrieval system with a generative model, allowing it to access external knowledge during generation. Traditional language models rely solely on their pre-trained parameters, limiting their ability to incorporate new or specific information. This makes RAG more flexible and capable of providing up-to-date, contextually relevant responses. How does RAG handle information not in its knowledge base? RAG will rely more heavily on its generative model when faced with information outside its knowledge base. However, it may also retrieve related information to provide context. If the system is well-designed, it should indicate when it’s unsure or lacks specific information, prompting users to verify or provide additional details. Can RAG be used for tasks other than text generation? While primarily used for text-based tasks, RAG principles are being applied to other domains. Researchers are exploring multimodal RAG systems that can work with images, audio, and video. These systems could assist in tasks like answering visual questions, audio transcription, or generating visual content based on textual descriptions. How often does the knowledge base in a RAG system need to be updated? The frequency of updates depends on the specific application and the rate at which relevant information changes. For rapidly evolving fields like current events or scientific research, frequent updates (daily or weekly) may be necessary. For more stable domains, less frequent updates might suffice. Many systems are designed to allow continuous or real-time updates to the knowledge base. Is RAG more computationally expensive than traditional language models? Due to the additional retrieval step, RAG can be more computationally intensive. However, smaller generative models can offset this, as some knowledge is offloaded to the external database. Overall efficiency depends on factors like the knowledge base’s size, the retrieval mechanism’s complexity, and the specific task at hand. How does RAG handle potential biases in its knowledge base? Bias in RAG systems can stem from both the retrieval corpus and the generative model. Mitigating bias involves carefully curating the knowledge base, implementing diverse and representative data sources, and using techniques to detect and correct biases in retrieval and generation processes. Ongoing research focuses on developing more robust methods for bias detection and mitigation in RAG systems. Can RAG be used in multilingual or cross-lingual applications? Yes, RAG can be adapted for multilingual and cross-lingual applications. This involves using multilingual knowledge bases, retrieval systems, and generative models capable of understanding and generating text in multiple languages. Some advanced RAG systems can retrieve information in one language and generate responses in another, facilitating cross-lingual knowledge transfer. How does RAG ensure the privacy and security of sensitive information? Privacy and security in RAG systems depend on how the knowledge base is managed and accessed. Techniques like federated learning, differential privacy, and secure enclaves can be employed to protect sensitive data. Additionally, careful access controls and data anonymization can be implemented to ensure that only appropriate information is retrieved and used in responses. What are the main challenges in implementing a RAG system? Key challenges include efficiently scaling retrieval to large knowledge bases, effectively integrating retrieved information with the generation process, ensuring the accuracy and relevance of retrieved information, maintaining up-to-date knowledge bases, and addressing issues of bias and privacy. Additionally, domain-specific adaptations and handling of complex, multi-step reasoning tasks pose ongoing challenges for RAG implementation. Get In Touch Follow Us Ananya Arora Ananya Arora is a fearless explorer in the realm of data engineering, constantly pushing boundaries and seeking new horizons. Armed with her keyboard and a toolkit of cutting-edge technologies, Ananya fearlessly ventures into uncharted territory, eager to uncover insights hidden within the data. Despite the occasional mishap or data breach, Ananya remains undeterred in her pursuit of innovation, confident that her pioneering approach to data engineering will lead her to success, one breakthrough at a time. Table of Contents Share This Blog. Previous Next 2025-01-03T09:23:44+00:00By Ananya Arora|Categories: Data Engineering| BuzzClan is a leading IT solutions provider, specializing in cloud computing, data management, quality assurance, and digital transformation. Our highly skilled team delivers customized strategies that empower businesses to optimize operations, drive innovation, and achieve sustainable growth. Services Toggle Navigation Cloud Consulting Data Engineering Services Quality Assurance Solutions Digital Transformation Solutions Quick Links Toggle Navigation Career Contact Us Blogs Privacy Policy Applicant Privacy Policy USA Dallas, Texas 4264 Entre Ct, Suite Q Chantilly, VA 20151 Canada 7-7003 Steeles Avenue W., Toronto, ON M9W 0A2 1460 Chevrier Blvd, Suite 200 Winnipeg, MB R3T 1Y7 India Block C, 56/6, Sector 62, Noida, U.P. 201309 E 289, Sector 75, Mohali, Punjab 160055 Kenya P.O BOX 18388-00500, Nairobi © BuzzClan. All rights reserved. Sign Up For Our Newsletter Submit Thank you for subscribing to our Newsletter There was an error trying to send your message. Please try again later. Page load link Go to Top

Source: https://ieeexplore.ieee.org/document/10769240#:~:text=The%20chapters%20contain%20real-world%20case%20studies%20and%20sample,retrieval%2C%20effective%20prompt%20engineering%2C%20and%20quantitatively%20evaluating%20performance.
Unlocking Data with Generative AI and RAG: Enhance generative AI systems by integrating internal data with large language models using RAG | Packt Publishing books | IEEE Xplore IEEE Account Change Username/Password Update Address Purchase Details Payment Options Order History View Purchased Documents Profile Information Communications Preferences Profession and Education Technical Interests Need Help? US & Canada: +1 800 678 4333 Worldwide: +1 732 981 0060 Contact & Support About IEEE Xplore Contact Us Help Accessibility Terms of Use Nondiscrimination Policy Sitemap Privacy & Opting Out of Cookies A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. © Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.

Source: https://www.aporia.com/learn/introduction-to-rags-examples-from-the-real-world/
From Chatbots to Code: The Diverse Applications of RAG | Aporia Product Overview Security Reliability Observability Solutions Cases RAG hallucinations Prompt injection Company policy Toxicity Data leakage Capabilities Custom policy Multimodal Session explorer Cost tracking Dashboards New Multimodal Learn more > Docs Resources Technical Documentation Company policy Integrations GitHub Changelog Learn Aporia Blog GenAI Academy Classic ML Academy Ebooks Glossary Videos More Events Use Case AI Act Aporia Labs NewBlog Aporia Has Been Acquired by Coralogix Read more Pricing Company About Customers Contact us News Room Careers Security Log in Book a demo Start For Free Aporia has been acquired by Coralogix, instantly bringing AI security and reliability to thousands of enterprises | Read the announcement Home Learn RAG RAG Introduction to RAGs: Real-world applications and examples Alon Gubkin 8 min read Feb 25, 2024 In the world of natural language processing (NLP) and large language models (LLMs), Retrieval-Augmented Generation (RAG) stands as a transformative approach, seamlessly blending the strengths of retrieval and generation models. This innovative paradigm empowers machines to enhance content creation by combining pre-existing knowledge with creative generation. RAG refines information synthesis and leverages context and relevance, promoting richer and contextually aware outputs. Let’s explore what Retrieval Augmented Generation is and its core principles. In addition, we’ll unravel its practical applications, promising advancements, and its crucial role in enhancing language models’ capabilities for a diverse range of tasks. Retrieval-Augmented Generation – What is RAG? RAG, or Retrieval Augmented Generation, serves as a dual-pronged methodology, combining the efficiency of information retrieval with the creative ingenuity of text generation. At its core, RAG involves leveraging a pre-existing knowledge base, often obtained from diverse sources such as encyclopedias or databases, to augment the content generation process. RAG serves as an artificial intelligence framework aimed at improving the performance of language models, specifically addressing concerns related to “AI hallucinations” and ensuring the freshness of data. The unique architecture of RAG combines sequence-to-sequence (seq2seq) models with components from Dense Passage Retrieval (DPR). This combination enables the model to generate contextually relevant responses and grounds them for inaccurate information retrieved from external knowledge sources. Here’s how RAG works: In 2020, Meta unveiled the RAG framework to broaden the capabilities of Language Models (LLMs) beyond their initial training data. RAG empowers LLMs to tap into specialized knowledge, allowing for more precise responses—a concept akin to an open-book exam. In this scenario, the model goes beyond relying solely on memorized facts and instead accesses real-world information to answer questions. This inventive methodology signifies a shift from traditional closed-book approaches, introducing a paradigm shift that greatly improves AI models’ accuracy and contextual comprehension. The model’s ability to access external knowledge ensures a more dynamic and informed response, exemplifying a significant stride in the evolution of language models. Image source: yourgpt.ai Retrieval: RAG initiates with a dedicated retriever model that extracts relevant information from a knowledge base. This retriever efficiently matches queries, obtaining a subset of data from sources such as textual corpora or databases. Generation: Utilizing the retrieved information, this model transforms raw data into human-like text, ensuring a balanced fusion of factual accuracy and expressive language. Training: The model is trained to increase its accuracy and reliability to ensure that it can produce more accurate and dependable results. A prominent example is IBM leveraging RAG to anchor customer-care chatbots in reliable and verified content. RAG enables AI systems to transcend scripted interactions, delivering users a personalized experience that dynamically adjusts to changing requirements. Retrieval-augmented generation for knowledge-intensive NLP tasks For knowledge-intensive natural language processing (NLP) tasks, Retrieval-Augmented Generation (RAG) emerges as a powerful solution. This innovative approach transcends traditional language models by seamlessly integrating the strengths of retrieval and generation mechanisms. Its application introduces a new linguistic proficiency and contextual awareness era, particularly suited for domains that demand a rich understanding of intricate information landscapes. RAG transcends traditional language models by seamlessly integrating retrieved information with generative capabilities, ensuring responses are contextually relevant and grounded in accurate and up-to-date knowledge. Imagine a customer inquiring about the latest features of a software product. Through its retrieval phase, RAG instantly fetches the most recent information from dynamic sources like release notes, forums, or official documentation. Active Retrieval Augmented Generation Active Retrieval Augmented Generation explores how RAG can actively retrieve and integrate up-to-date information during interactions, ensuring the language model adapts to the latest data. This proactive approach enhances the model’s responsiveness in dynamic environments, making it particularly valuable for applications demanding real-time, accurate information. For instance, in a news summarization task, RAG can actively retrieve and incorporate the latest developments, delivering timely and accurate summaries reflective of the most recent information. Retrieval Augmented Generation vs. fine-tuning RAG’s strength lies in its ability to seamlessly blend pre-existing knowledge with creative generation, offering a more balanced and nuanced approach. In contrast, fine-tuning often focuses on refining a model’s performance on specific tasks through iterative adjustments. While both approaches have merits, RAG’s unique combination of retrieval and generation proves advantageous in scenarios requiring a sophisticated understanding of context, making it a preferred strategy for knowledge-intensive NLP tasks. Retrieval Augmented language model Retrieval Augmented Language Models (RALLM) represent a significant evolution in natural language processing, encapsulating the essence of retrieval augmentation. These LLM retrieval augmented generation models seamlessly integrate contextual information retrieval with the language generation process, amplifying their capacity to produce coherent and informed text. In-context Retrieval-Augmented language models In the specialized domain of In-Context Retrieval-Augmented Language Models, emphasis is placed on enhancing contextual awareness. By actively retrieving and incorporating information within the context of ongoing interactions, these models excel in maintaining relevance and accuracy, contributing to more sophisticated language understanding. RAG chatbot RAG Chatbot RAG Chatbot transforms traditional chatbot interactions by integrating LLM Retrieval Augmented Generation. Unlike scripted counterparts, it dynamically adapts to user queries, utilizing a retriever model for information retrieval and a language model for contextually rich responses. This ensures a personalized and responsive experience, surpassing the limitations of predefined scripts. For instance, in customer support, the chatbot actively retrieves updated information from knowledge bases, ensuring real-time, accurate assistance and personalized interactions, enhancing user satisfaction and problem resolution. However, even RAG Chatbots aren’t fully solving hallucinations. They might generate incorrect or nonsensical information in different scenarios where the complexity exceeds the model’s training, or the input is ambiguous, lacks context, or contains contradictions. Retrieval Augmented Generation paper The Retrieval Augmented Generation Paper dissects RAG’s theoretical foundations and practical applications. It navigates through key papers, unraveling the complexities of blending retrieval and generation models. Image source: analyticsvidhya.com For example, a research paper discussing recent advances in Retrieval-Augmented Text Generation has demonstrated its prowess in diverse applications. In this context, innovative implementations showcase how RAG significantly enhances content creation, producing text that seamlessly blends information retrieval with creative generation. OpenAI Retrieval Augmented Generation OpenAI Retrieval Augmented Generation scrutinizes OpenAI’s role in advancing language models by seamlessly integrating retrieval and generation processes. Understanding OpenAI’s approach sheds light on the cutting-edge advancements in this field. Through its retrieval phase, RAG taps into external knowledge sources, like Dense Passage Retrieval (DPR) or cosine similarity, ensuring that responses are grounded in accurate and up-to-date information. A tangible example is OpenAI’s development of ChatGPT with RAG features, where information retrieval enhances the model’s responses, creating a more informed and contextually aware conversational agent. Retrieval Augmented Generation architecture Examining the intricate design elements, Retrieval Augmented Generation Architecture dissects the structural framework that underpins the seamless collaboration between retrieval and generation models. A real-world example is the architecture adopted by Google’s LaMDA (Language Model for Dialogue Applications), where retrieval mechanisms enhance dialogue context, allowing for more coherent and contextually relevant conversations. Integral to the success of RAG architectures is the integration with vector databases. These databases act as repositories of encoded information, storing semantically rich representations of textual data. Source: analyticsvidhya.com Vector databases provide a structured and efficient means of organizing and retrieving information. RAG architectures leverage these DBs to augment the retrieval process, enabling the model to access and comprehend various contextual information. The vectors serve as a bridge between the retrieval and generation components, enhancing the overall efficiency and effectiveness of the language model. For example, incorporating specific data into general-purpose models, such as IBM Watson.ai’s Granite, using a vector DB, improves understanding and boosts efficiency across several AI applications. Retrieval Augmented code generation and summarization For code-related tasks, retrieval augmented code generation and summarization use the power of RAG to enhance precision. This specialized application ensures the generation of accurate and relevant code snippets and summaries by leveraging both retrieval and generation processes, catering to the specific requirements of developers and programmers. For instance, GitHub Copilot, powered by RAG principles, actively retrieves relevant code snippets during development. This ensures developers receive accurate and contextually appropriate suggestions, accelerating the coding process and minimizing errors in software development. Final words Retrieval-augmented generation (RAG) stands at the forefront of revolutionizing natural language processing, seamlessly integrating retrieval and generation for enhanced language models. Its applications span from in-context conversational agents to dynamic code generation and summarization tasks, showcasing adaptability across diverse domains. RAG’s active retrieval mechanisms ensure real-time adaptation to evolving information, addressing challenges like hallucinations and advancing the reliability of AI interactions. As we explore language models, the knowledge retrieval and creative generation embodied in RAG promises a future where machines comprehend and adeptly contribute to human-like conversations, setting the stage for a new era of sophisticated and context-aware artificial intelligence. Optimize RAG Chatbots with Aporia’s AI Guardrails for Accuracy and Reliability Enhance your RAG chatbot’s performance and reliability with Aporia’s AI Guardrails. Tackle hallucinations and ensure accuracy in real-time interactions. Discover more here: Mitigate hallucinations Prevent prompt injections Moderate chats Explore sessions Track costs Want to see how Aporia works? Book a short guided demo with one of our experts. Rate this article Submit Rating Average rating 5 / 5. Vote count: 1 No votes so far! Be the first to rate this post. Alon Gubkin Alon is the CTO of Aporia. Related topics : Compliance GenAI For Practitioners GenAI Leadership Generative AI by Industry Intro to GenAI LLM LLM Observability RAG On this page Retrieval-Augmented Generation – What is RAG? Retrieval Augmented language model RAG chatbot Retrieval Augmented Generation paper OpenAI Retrieval Augmented Generation Retrieval Augmented Generation architecture Retrieval Augmented code generation and summarization Final words Building an AI agent? Consider AI Guardrails to get to production faster Learn more Table of Contents Retrieval-Augmented Generation – What is RAG? Retrieval Augmented language model RAG chatbot Retrieval Augmented Generation paper OpenAI Retrieval Augmented Generation Retrieval Augmented Generation architecture Retrieval Augmented code generation and summarization Final words Related Articles RAG Step by Step: Building a RAG Chatbot with Minor Hallucinations In the rapidly evolving landscape of artificial intelligence, Retrieval Augmented Generation (RAG) has emerged as a groundbreaking technique that enhances... Deval Shah Read Now 12 min read RAG RAG in Production: Deployment Strategies and Practical Considerations As organizations rush to implement Retrieval-Augmented Generation (RAG) systems, many struggle at the production stage, their prototypes breaking under real-world... Deval Shah Read Now 23 min read RAG Vector Databases: Powering the Future of Generative AI Have you ever wondered how ChatGPT can engage in such fluid conversations or how Midjourney creates stunning Nimages from text... Deval Shah Read Now 13 min read RAG What is a Reasoning Engine? TL/DR What is a Reasoning Engine? Imagine a digital brain that can sift through vast amounts of information, apply logical... Deval Shah Read Now 17 min read RAG Ultimate Guide to Prompt Engineering + Examples Have you ever wondered how to get the most relevant responses from LLM-based chatbots like ChatGPT and Claude? Enter prompt... Deval Shah Read Now 27 min read RAG Evolution of RAG in Generative AI Generative AI has become a major focus in artificial intelligence research, especially after the release of OpenAI’s GPT-3, which showcased... Deval Shah Read Now 15 min read RAG RAGs are Not the Solution for AI Hallucinations The use of large language models (LLMs) in various applications has raised concerns about the potential for hallucinations, where the... Amir Gorodetzky Read Now 5 min read Try AI Guardrails for free Start for free book a demo Get a Demo Product Aporia’s Platform AI Security AI Reliability AI Observability Docs Solutions Prompt Injection Prompt Leakage SQL Enforcement Company Policy Data Leakage PII Leakage Toxicity Allowed Topics Off-Topic Discussion RAG Hallucinations Competition Discussion Custom Policy Capabilities Custom policy Multimodal Session Explorer Cost Tracking Dashboards Pricing Resources Documentation Datasets Company Policy Integrations GitHub Learn Aporia Blog GenAI Academy Observability Academy Ebooks Videos More Events Developer Forum AI Act Aporia Labs Company About us Customers Press Careers Contact Us Acquisition of Aporia: FAQs for Customers © Aporia 2025 Privacy Policy Terms of Use User Agreement Certified Follow Us

Source: https://medium.com/@ashishmule/building-rag-systems-step-by-step-insights-from-real-world-projects-best-practices-expert-guide-1f77f707d44a
Building and Improving RAG Systems: Step-by-Step Insights from Real-World Projects, Best Practices & Expert Guide to Success | by Ashish Mule | Dec, 2024 | Medium Open in app Sign up Sign in Write Sign up Sign in Building and Improving RAG Systems: Step-by-Step Insights from Real-World Projects, Best Practices & Expert Guide to Success Ashish Mule · Follow 5 min read · Dec 28, 2024 -- Listen Share In the world of AI, retrieval-augmented generation (RAG) is an advanced technology used to improve the output of language models (LLMs) by combining them with external knowledge sources. RAG models allow systems to retrieve relevant information from large datasets and generate more accurate, contextually appropriate answers. However, as with all cutting-edge technologies, there are nuances and challenges when implementing RAG solutions in real-world scenarios. In this blog, I’ll share detailed insights from my experience working on RAG projects for clients across various industries. These lessons reflect common challenges faced while working with OCR, chunking strategies, embedding models, LLMs, and prompt engineering. Understanding these nuances will help you make better decisions when implementing RAG-based solutions in your own projects. 1. OCR Pipelines Are Not Perfect: Incorporating Data Checks and Structured Formats One of the most common issues we face in RAG projects involves optical character recognition (OCR). OCR pipelines, while powerful, are far from perfect. They often struggle with recognizing distorted text, non-standard fonts, or handwriting. As a result, if you’re working with data that’s extracted through OCR, it’s important to recognize that the resulting text may be incomplete or inaccurate. Solution: Clients who require high levels of accuracy should consider incorporating a validation layer for OCR results. This could involve automatic or manual checks to verify the data post-extraction. Alternatively, if OCR validation is not feasible, it’s advisable to source the data in a structured format that is easier to parse and extract, independent of OCR systems. Formats such as CSV, JSON, or even well-structured PDFs with consistent fonts can make a world of difference in ensuring that the data extracted is both accurate and usable. 2. Chunking Strategy Limitations in Real-Life Scenarios Chunking is a technique often used in information retrieval, where the data is split into manageable “chunks” for easier processing. However, in real-life scenarios, chunking strategies often fall short, especially when dealing with abbreviations and their expanded forms. For instance, an agent tasked with answering a question may retrieve a chunk that contains the full form of an abbreviation but does not provide the correct context. If the user queries about a specific abbreviation like “AI” and the chunk retrieved contains “Artificial Intelligence,” the information might not be directly applicable to the user’s question. Solution: To address this, an agentic RAG model becomes essential. Unlike basic RAG models, agentic RAG models are designed to use more advanced reasoning to understand the context and ensure that the retrieved chunks contain relevant and actionable answers. By incorporating advanced retrieval and reasoning algorithms, you can better handle cases where chunking alone won’t suffice. 3. Open Source Embedding Models and LLMs Have Limitations When implementing RAG-based solutions, many developers initially turn to open-source embedding models (like Mixtrals, LLaMa, and BGE-embeddings). While these models can provide some value, they often fall short when compared to more powerful models like those offered by OpenAI, such as GPT-4. Open-source models, despite their versatility and accessibility, often lack the fine-tuning and scale required for real-world business applications. They can sometimes produce less accurate or coherent results, especially when dealing with nuanced or complex queries. Solution: For better performance and more reliable outputs, it’s best to use LLMs and embedding models provided by established players like OpenAI. These models have been extensively trained on diverse datasets, are continuously improved, and benefit from fine-tuning that addresses many common issues found in other models. With OpenAI’s models, you’ll achieve far greater results, particularly when dealing with complex data and the need for more precise or contextually relevant information. 4. LLMs and Output Sequence Lengths: The Challenge of Constraints LLMs have a powerful ability to generate text, but they also face significant constraints when it comes to handling output sequence lengths. A common request I’ve encountered is asking LLMs to summarise long paragraphs into a fixed-length output, such as “a 10-word summary.” However, LLMs may struggle with such rigid instructions, especially when tasked with summarising highly detailed or dense information. Solution: To work around these limitations, it’s more effective to re-frame your prompt. Rather than demanding an exact word count, try breaking down the request into manageable tasks. For example, asking the LLM to produce “a 10-word summary, with each word as a bullet point” leads to more accurate and concise responses. This method allows the LLM to better focus on the essence of the content and produce a meaningful summary that adheres more closely to the user’s requirements. 5. Negative Prompting Issues: Avoiding Unintended Behaviour Negative prompting, where you instruct the model to generate something it should avoid, can often lead to unintended results, especially when working with large prompts. For example, if you include a section like: Valid output: Invalid output: where the “Invalid output” section is a negative example, the LLM may interpret it as a positive cue. This can lead to the generation of responses that mimic the “invalid output” format instead of avoiding it, especially when the context is lengthy. Solution: To ensure that negative prompts are understood correctly, keep the context brief and clear. Instead of using negative examples, focus on positive reinforcement by providing clear, concise, and contextually relevant examples of the desired output. Avoid complex structures in negative prompting to prevent misinterpretation by the model. 6. Detailed Responses Before Summarising: Maximising Summarization Quality When tasked with summarising content, it’s tempting to ask the model to generate a summary directly. However, this approach can sometimes lead to vague or incomplete summaries. I’ve found that it’s more effective to first prompt the LLM to provide a detailed response to a question or request. Then, you can summarise the detailed response for a more accurate and nuanced summary. Solution: For example, if you’re summarising a research paper, first ask the LLM to provide a comprehensive breakdown of the key points. Once you have a detailed response, you can manually or automatically condense it into a shorter, more concise summary. This two-step approach ensures that the summary remains rich in context and covers all important aspects without missing key information. Conclusion While RAG technologies have immense potential, they are not without their challenges. From OCR inaccuracies to the limitations of open-source models and the complexities of prompt engineering, there are many pitfalls to avoid. However, with the right strategies — such as using validation layers for OCR, employing agentic RAG models for chunking, and opting for high-performance models like those from OpenAI — you can significantly enhance the accuracy and efficiency of your RAG-based systems. By understanding these challenges and solutions, you can make more informed decisions and achieve better results in your AI-powered projects. Whether you’re working on customer service applications, document processing systems, or intelligent search engines, these insights will guide you to success in the complex world of retrieval-augmented generation. If you enjoyed this article, don’t forget to give it a 👏! Your support means a lot, and helps me continue sharing more valuable content. I’m glad you liked the post! If you found these insights helpful, feel free to connect with me on LinkedIn to stay updated with my latest content and projects. You can find me on LinkedIn here. Looking forward to connecting and sharing more knowledge with you! Rag Pipeline Artificial Intelligence NLP Technology Data Science -- -- Follow Written by Ashish Mule 0 Followers ·3 Following Senior Data Scientist Follow No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams

Source: https://www.chitika.com/rag-for-code-generation/
RAG for Code Generation: Automate Coding with AI & LLMs Home Resources GraphRAG Podcasts About Us Contact Sign in Subscribe RAG Applications How To Use RAG for Code Generation Learn how to use Retrieval-Augmented Generation (RAG) for code generation by integrating structured knowledge retrieval with AI models. Improve code accuracy, automate development tasks, enhance debugging, and generate context-aware code snippets efficiently. Arooj 03 Feb 2025 • 17 min read The very tools designed to make coding easier—frameworks, libraries, and templates—are now the bottleneck. Developers spend hours sifting through documentation, piecing together boilerplate code, and debugging repetitive patterns. It’s ironic, isn’t it? The tools meant to save time often end up consuming it. Retrieval-Augmented Generation (RAG) flips this script entirely. By combining the precision of information retrieval with the creativity of generative AI, RAG doesn’t just automate code generation—it tailors it to your project’s unique context. This isn’t about generic snippets; it’s about creating code that feels like it was written by someone who knows your system inside out. But here’s the real question: can RAG truly replace the manual grind of coding without sacrificing quality or control? And if so, what does that mean for the future of software development? Let’s explore. Image source: reliasoftware.com What is Retrieval-Augmented Generation (RAG)? RAG is a powerful AI approach that goes beyond pre-trained knowledge by actively retrieving real-time data from external sources. It’s like combining a researcher and a writer—one gathers facts, the other crafts responses—ensuring accuracy and context. RAG shines where static models struggle, like legal tech (pulling case precedents for arguments) or e-commerce (fetching product specs for recommendations). However, its effectiveness depends on the quality of retrieval, making optimization crucial. When done right, RAG bridges the gap between static AI and dynamic, domain-specific intelligence. The Evolution of Code Generation Code generation has come a long way, but the real breakthrough lies in how RAG integrates contextual awareness. Unlike traditional models that spit out generic snippets, RAG tailors its output by retrieving project-specific data—like internal libraries or team coding standards. This isn’t just automation; it’s collaboration between AI and developer intent. Take debugging, for instance. Instead of manually sifting through error logs, RAG pulls relevant fixes from your issue tracker or even Stack Overflow. It doesn’t just save time—it reduces cognitive load, letting developers focus on solving problems rather than finding them. RAG also democratizes coding. By turning natural language prompts into functional code, it empowers non-developers to prototype ideas. The result? Faster innovation cycles and fewer bottlenecks. Moving forward, the challenge will be balancing this accessibility with maintaining code quality and security—a frontier worth exploring. Image source: youtube.com Benefits of Using RAG in Code Generation RAG helps reduce technical debt by ensuring code consistency across projects. By retrieving reusable components and design patterns, it minimizes redundant or poorly structured code, making systems more scalable and maintainable. In large teams, RAG aligns outputs with internal coding standards by referencing style guides or past commits, reducing code review efforts and improving collaboration—like having a senior developer available 24/7. Beyond developers, RAG also empowers cross-functional teams. A product manager, for instance, can generate prototype code that follows best practices, speeding up development without compromising quality. To maximize its benefits, organizations need well-curated knowledge bases. Understanding the Core Concepts Retrieval-Augmented Generation (RAG) marries two forces: retrieval and generation. Imagine a chef who not only remembers recipes but also picks the freshest ingredients to create a dish just for you. This blend ensures outputs are relevant and rich in context. However, RAG’s success hinges on a strong knowledge base—a poor one can lead to misguided results, much like using expired ingredients. When paired with trusted sources like GitHub repositories or internal docs, RAG becomes a powerful tool for producing clean, functional code. The Mechanics of Retrieval Systems The retriever in RAG is like a skilled librarian who finds the exact page you need rather than just any book. Using techniques like vector embeddings, it maps queries and documents into a shared space for precise matching. In environments with domain-specific data, such as proprietary APIs, this method can save significant debugging time. Regular updates and a feedback loop are key to keeping the retriever sharp, ensuring it anticipates user needs accurately. Language Models in Code Generation Language models in code generation need solid context to excel. By fine-tuning these models on domain-specific data, they can grasp the nuances of internal coding standards or niche frameworks. For example, a fintech firm fine-tuned its model on proprietary APIs, leading to code suggestions that met internal security protocols while reducing manual reviews. Enhancing outputs further, detailed prompts act like a well-drawn map, guiding the model with clarity. Integrating Retrieval with Generation A retrieval-first workflow grounds the generative process in real-time, domain-specific data, reducing hallucinations and aligning outputs with current, relevant info. Take a healthcare chatbot that fetches up-to-date clinical guidelines before offering advice—this builds user trust and ensures accuracy. The success of this hybrid approach depends on well-tuned vector embeddings and ongoing fine-tuning, creating a system that evolves with changing data while maintaining top performance. Setting Up a RAG System for Code Generation Think of setting up a RAG system like building a custom toolbox—it’s all about having the right tools for the job. Start with a well-curated knowledge base. This isn’t just about dumping documentation into a database; it’s about structuring it for fast, accurate retrieval. For example, a fintech company might index API docs, compliance rules, and past code snippets to ensure the system generates secure, functional code. Next, focus on the retrieval mechanism. Tools like FAISS or Elasticsearch can help, but the real game-changer is embedding quality. Poor embeddings? Your system retrieves junk. Fine-tune embeddings using domain-specific data to ensure precision. Here’s the twist: don’t overlook feedback loops. Developers can flag irrelevant suggestions, which the system uses to refine future outputs. It’s like teaching a junior developer—iterative and rewarding. Finally, integrate lightweight prompt engineering. Clear, structured prompts guide the generative model, ensuring outputs align with your coding standards. Image source: together.ai 1. Required Tools and Libraries When it comes to tools, vector databases like FAISS or Pinecone are non-negotiable. Why? They make retrieval lightning-fast by organizing your data into embeddings that the system can search efficiently. But the quality of these embeddings depends heavily on your choice of embedding models. OpenAI’s Ada or Cohere’s embeddings are great starting points, but fine-tuning them on your domain-specific data can take your RAG system from “meh” to “wow.” For libraries, LangChain is a standout. It simplifies the integration of retrieval and generation workflows, letting you focus on building rather than debugging. Pair it with Hugging Face Transformers to leverage pre-trained models for tasks like code generation or documentation. Pro tip: Don’t ignore monitoring tools. Libraries like Prometheus or Grafana can track retrieval accuracy and latency, helping you spot bottlenecks early. Think of them as your system’s health dashboard—essential for scaling effectively. 2. Preparing the Knowledge Base Your knowledge base isn’t just a repository—it’s the backbone of your RAG system. Start by curating high-quality, domain-specific data. This means pulling from internal documentation, codebases, and even issue trackers. But don’t stop there. Include external sources like GitHub repositories or Stack Overflow threads to add depth and diversity. Now, let’s talk structure. Use embedding models to convert your data into searchable vectors. But here’s a twist: prioritize metadata tagging. For example, tagging code snippets with programming languages, frameworks, or even project phases can drastically improve retrieval accuracy. Think of it as giving your system a map instead of a maze. One overlooked factor? Data freshness. Outdated knowledge leads to irrelevant results. Automate updates with tools like DVC (Data Version Control) to keep your base current. Trust me, a well-maintained knowledge base doesn’t just support your RAG system—it supercharges it. 3. Configuring the Retrieval Component Your retrieval component isn’t just about speed—it’s about precision. Start by using vector embeddings tailored to your domain. For example, if you’re working with Python code, opt for embeddings fine-tuned on programming languages. This ensures your system retrieves relevant snippets, not random noise. But here’s where it gets interesting: multi-vector indexing. Instead of relying on a single embedding, use multiple embeddings for different data types—like code, documentation, and error logs. This layered approach boosts accuracy by matching queries to the right context. Think of it as having a specialist for every task. Now, let’s address a hidden factor: query optimization. Use techniques like query expansion to include synonyms or related terms. For instance, a search for “bug” could also retrieve results tagged with “error” or “issue.” The result? A retrieval system that feels intuitive and delivers spot-on results every time. 4. Connecting Retrieval to the Generation Model The magic happens when retrieval and generation work in sync. The key is embedding alignment—ensuring the vector space used for retrieval matches the one the generation model understands. Without this, your generative model might misinterpret retrieved data, leading to irrelevant or incoherent outputs. Now, let’s talk context packaging. Instead of dumping raw retrieved data into the model, structure it. For example, prepend retrieved code snippets with metadata like file paths or function names. This gives the model a richer context to generate more accurate and actionable outputs. Here’s a lesser-known trick: dynamic weighting. Not all retrieved data is equally relevant. Use scoring mechanisms to prioritize high-confidence results. For instance, in debugging, prioritize logs with recent timestamps. This approach not only improves output quality but also reduces noise, making your RAG system feel smarter and more intuitive. Implementing Code Generation with RAG Using RAG for code generation is like having a supercharged assistant that knows your codebase inside and out. Instead of starting from scratch, it pulls in relevant code snippets from repositories or internal knowledge bases, grounding its outputs in real-world examples. For instance, if you're building an API, RAG can fetch boilerplate code for authentication, adjust it to your framework, and even suggest parameter tweaks—cutting down on bugs and aligning with best practices. It doesn’t replace your creativity but handles the repetitive work, letting you focus on unique challenges. Image source: arunapattam.medium.com Creating Effective Prompts Crafting detailed prompts is key. A vague request like “generate a login function” may produce generic code, but adding specifics—framework details, authentication methods, or edge cases—turns it into a precise blueprint. Think of it as giving clear instructions: the more detail you provide (like “write a Python function using json.loads to parse a JSON string and handle invalid inputs with exceptions”), the better the output. Iterative refinement also helps: start broad, review, and tweak until the code fits perfectly. Managing Context and Dependencies RAG excels when you structure retrieved data to reflect your project’s flow. When generating a function that interacts with a database, include details like schema, connection methods, and constraints to minimize guesswork. A handy trick is dependency mapping—retrieving related modules or libraries beforehand ensures seamless integration. Don’t forget to specify library or framework versions in your prompts to avoid subtle, hard-to-debug issues. Generating Modular Code Break down code generation into smaller, self-contained units rather than one large module. For example, instead of asking for a complete authentication module, prompt RAG to generate specific components like token validation or password hashing. Using incremental prompts—starting with high-level requirements and then zooming in on details—helps maintain logical consistency and makes debugging easier. Testing and Validation Integrate testing right into the generated code. Prompt RAG to produce both a function and its corresponding unit tests to catch errors early. Pair your RAG outputs with static analyzers or linters (like Pylint for Python) to ensure the code meets your standards and handles edge cases. Testing across multiple library versions further ensures compatibility in fast-changing environments. In short, treat validation as a continuous loop: test, refine, and iterate for robust, production-ready code. Advanced Usage and Optimization Imagine RAG as a smart GPS that recalculates your route based on traffic. With dynamic retrieval prioritization, you can assign weights to data—favoring recent commits or highly-rated snippets—to ensure your generated code stays current. For example, pulling from active GitHub repositories minimizes outdated solutions. Plus, by integrating domain-specific elements (like medical ontologies for healthcare apps), RAG can deliver code that not only works but also complies with industry regulations. Image source: trantorinc.com 1. Customizing the Retrieval Strategy Not all retrieval strategies are equal. For projects with complex, layered dependencies—like microservices—recursive retrieval digs deep, pulling every related component (from endpoints to authentication methods). In dynamic settings, such as debugging a live e-commerce platform during a flash sale, adaptive retrieval prioritizes recent logs and commits, ensuring fixes are based on the latest context. Fine-tuning your queries with domain-specific keywords further sharpens the system’s ability to fetch the right data. 2. Fine-Tuning Models for Specific Languages Fine-tuning isn’t just about more data—it’s about understanding language nuances. Whether it’s Python’s flexible syntax or Java’s strict type system, training with language-specific idioms (like Python’s list comprehensions or Java’s try-with-resources) ensures that the generated code feels natural. Domain-adaptive pretraining on real-world projects, such as Django for Python or Spring Boot for Java, reduces syntax errors and makes suggestions more contextually relevant. Don’t forget to analyze and adjust for common error patterns, like off-by-one mistakes, to create a system that writes code you’d actually use. 3. Scaling RAG Systems for Large Projects As projects grow, so do data sources. Federated retrieval—querying multiple databases in parallel—helps avoid bottlenecks and speeds up response times. Pre-ranking data sources using metadata or historical query patterns can significantly cut down on unnecessary retrievals, as demonstrated by a DevOps team that reduced latency by 40%. Leveraging auto-scaling cloud services also ensures that your system adapts smoothly to spikes in demand, proving that smart resource management is key to scaling successfully. 4. Integrating Feedback Loops for Improvement Real-time feedback is a game-changer. Embedding user input (like thumbs-up/down ratings) directly into the retrieval and generation cycle lets the system adjust dynamically, favoring more relevant data over time. Automatic feedback mechanisms—such as classifiers that flag low-confidence outputs—can trigger corrective actions without human intervention. Moreover, contextual feedback (tied to specific use cases like debugging logs versus API documentation) ensures that the system evolves in meaningful ways. Ultimately, these feedback loops help build a RAG system that learns smarter, not just harder. Case Studies and Real-World Applications Let’s talk about how RAG is making waves in the real world. Take GitHub Copilot, for example. By integrating RAG, it retrieves context-specific code snippets from vast repositories, helping developers write efficient code faster. One team reported a 40% reduction in development time for repetitive tasks like boilerplate generation. That’s not just faster—it’s smarter. Now, contrast that with healthcare. A hospital network used RAG to enhance its knowledge base, retrieving clinical guidelines tailored to specific cases. The result? Doctors spent 30% less time searching for information, allowing them to focus on patient care. It’s like having a medical librarian on speed dial. RAG isn’t just about speed—it’s about precision. In e-commerce, a platform improved product search by combining retrieval with generative descriptions, boosting conversion rates by 15%. Whether it’s debugging code or saving lives, RAG proves that context is king. Image source: astera.com Automating Boilerplate Code Generation Boilerplate code is the broccoli of software development—necessary but tedious. RAG flips the script by pulling reusable patterns from internal repositories and external libraries, then tailoring them to your project’s context. For instance, a fintech startup used RAG to auto-generate API integration templates, cutting onboarding time for new developers by 50%. That’s not just efficiency; it’s onboarding with a turbo boost. Here’s why it works: RAG doesn’t just retrieve code—it understands why it’s relevant. By analyzing metadata like function usage and dependencies, it ensures the generated code aligns with your architecture. Think of it as a sous-chef who preps exactly what you need, no more, no less. Enhancing Developer Productivity Let’s talk about meetings—the productivity killer. RAG can eliminate the need for endless clarification sessions by acting as a real-time knowledge hub. For example, a SaaS company integrated RAG into their IDE, enabling developers to instantly retrieve project-specific guidelines, past decisions, and even code review notes. The result? Fewer interruptions, more deep work. Here’s the magic: RAG thrives on context. By combining retrieval with generative capabilities, it delivers not just answers but actionable insights tailored to the task at hand. This is especially powerful during debugging, where RAG can surface relevant error logs and past fixes, slashing resolution times. RAG in Collaborative Coding Environments Team coding can suffer from miscommunication, but RAG helps keep everyone on the same page. For example, a fintech startup used RAG to centralize code reviews, allowing developers to instantly access annotated feedback and historical decisions. This streamlined process reduced redundant discussions and reinforced coding standards. RAG works best with a structured knowledge base—tagged code snippets with metadata like author notes and dependencies create a shared context for all. This is especially beneficial for remote teams, where asynchronous collaboration can lead to gaps in understanding. For even better results, integrate RAG with version control systems. This way, each pull request comes with context-aware suggestions, making reviews faster and more insightful. The end result is a collaborative coding environment that feels seamless rather than chaotic. Challenges and Considerations Let’s talk about the elephant in the room: data quality. RAG systems are only as good as the knowledge base they pull from. Imagine feeding outdated or poorly tagged code snippets into the system—it’s like asking a GPS to navigate with a 10-year-old map. A real-world example? A retail company’s RAG tool suggested deprecated APIs, leading to hours of rework. The fix? Regularly auditing and updating the knowledge base. Another curveball? Balancing retrieval precision with speed. Developers want answers fast, but overly broad retrieval can flood them with irrelevant results. Think of it like searching for a needle in a haystack—except the haystack keeps growing. Techniques like multi-vector indexing can help, but they require careful tuning. Finally, privacy concerns can’t be ignored. Using proprietary or sensitive data in RAG systems demands airtight compliance with regulations like GDPR. Without it, you’re risking more than just bad code—you’re risking trust. Image source: code-rag-bench.github.io Ensuring Code Quality and Reliability Context is king when it comes to code quality. RAG systems thrive on well-structured, domain-specific knowledge bases, but here’s a twist—context layering can make or break reliability. For example, a fintech company used RAG to generate compliance-critical code. By embedding regulatory guidelines directly into the retrieval layer, they avoided costly errors. Without this, the system might’ve generated code that passed tests but failed audits. Another overlooked factor? Dependency mapping. RAG can retrieve snippets that work in isolation but break when integrated. Think of it like assembling IKEA furniture without checking if all the screws fit. A better approach? Use dependency-aware prompts that account for versioning and compatibility. Finally, don’t underestimate automated testing integration. Embedding unit tests into generated code isn’t just a nice-to-have—it’s essential. It’s like having a safety net that catches issues before they spiral into production nightmares. Addressing Security Concerns Let’s talk about data leakage—a sneaky risk in RAG systems. when RAG retrieves sensitive data, it’s not just about encryption. The real game-changer is query anonymization. By stripping identifiable metadata from queries, you prevent exposing proprietary information during retrieval. For instance, a healthcare provider used this approach to safeguard patient data while still leveraging RAG for real-time diagnostics. Another underappreciated tactic? Access throttling. Think of it as a speed bump for malicious actors. By limiting query rates and implementing role-based access controls, you reduce the risk of unauthorized data scraping. This is especially critical in multi-tenant environments where one breach could cascade. Finally, don’t sleep on red teaming. Simulating attacks on your RAG pipeline uncovers vulnerabilities you didn’t know existed. It’s like stress-testing a bridge before letting traffic flow. The takeaway? Security isn’t static—it’s a living, breathing process. Ethical Implications of Automated Code Generation Let’s zero in on algorithmic bias—a silent disruptor in automated code generation. bias often sneaks in through training data. If your RAG system pulls from repositories with skewed patterns (e.g., underrepresentation of certain programming paradigms), it can perpetuate inequities. For example, a fintech company found their model favoring legacy systems over modern, inclusive frameworks, stalling innovation. One effective fix? Diverse data curation. By sourcing from a wide range of repositories and auditing for inclusivity, you can reduce bias at the root. Another game-changer is bias-aware prompts. These guide the system to prioritize fairness, like ensuring gender-neutral variable names or avoiding culturally loaded terms. But here’s the twist: bias isn’t just technical—it’s cultural. Engaging diverse stakeholders during development can surface blind spots you’d never catch alone. The result? Systems that don’t just generate code but foster equity in tech. FAQ 1. What is Retrieval-Augmented Generation (RAG) and how does it enhance code generation? Retrieval-Augmented Generation (RAG) is a cutting-edge AI technique that combines the strengths of retrieval-based methods and generative models to produce contextually accurate and information-rich outputs. In the context of code generation, RAG enhances the process by retrieving relevant, real-time data from external knowledge bases, such as code repositories, documentation, or issue trackers, and integrating this information into the generated code. This approach ensures that the outputs are not only aligned with the latest standards and practices but also tailored to specific project requirements. By reducing reliance on static training data, RAG minimizes errors, accelerates development workflows, and empowers developers to create robust, high-quality code with greater efficiency. 2. How can RAG be integrated into existing software development workflows? RAG can be seamlessly integrated into existing software development workflows by embedding it within commonly used tools and processes. Start by incorporating RAG-enabled plugins into Integrated Development Environments (IDEs) to provide context-aware code suggestions and completions. Enhance your debugging process by using RAG to retrieve relevant error resolutions from internal issue trackers or external forums. For documentation, RAG can automate updates by analyzing code changes and generating synchronized documentation. Additionally, integrating RAG with version control systems allows for contextually enriched code reviews, ensuring alignment with project standards. By tailoring RAG to your team’s specific needs and maintaining an up-to-date knowledge base, it becomes a powerful assistant that complements and enhances your existing development practices. 3. What are the best practices for preparing a knowledge base for RAG in code generation? Preparing a knowledge base for RAG in code generation requires a structured and meticulous approach to ensure relevance and accuracy. Begin by curating high-quality, domain-specific data, including code snippets, documentation, and best practices, while eliminating outdated or redundant information. Organize the data with metadata tagging to enhance retrieval precision and ensure that the knowledge base is easily searchable. Automate regular updates to keep the information current and aligned with evolving project requirements. Incorporate feedback loops from developers to refine the knowledge base continuously, addressing gaps or inaccuracies. Finally, prioritize security and compliance by anonymizing sensitive data and adhering to regulatory standards, ensuring the knowledge base is both robust and trustworthy. 4. How does RAG ensure the accuracy and reliability of generated code? RAG ensures the accuracy and reliability of generated code by grounding its outputs in real-time, contextually relevant data retrieved from trusted external sources. By leveraging a well-maintained knowledge base, RAG minimizes the risk of hallucinations and outdated information, producing code that aligns with current standards and practices. The retrieval component dynamically fetches precise snippets or documentation tailored to the task, while the generative model integrates this data to create coherent and functional outputs. Additionally, incorporating automated testing and validation mechanisms during the generation process further enhances reliability, ensuring that the generated code meets quality benchmarks and integrates seamlessly into existing systems. 5. What tools and frameworks are recommended for implementing RAG in code generation projects? Implementing RAG in code generation projects benefits greatly from leveraging specialized tools and frameworks. Vector databases like FAISS and Pinecone are essential for efficient data retrieval, enabling quick access to relevant information. Libraries such as LangChain and Hugging Face Transformers provide robust support for integrating retrieval and generation components, streamlining the development process. LlamaIndex is another valuable tool, aiding in data processing and model training to ensure seamless integration of machine learning components. For monitoring and maintaining system health, tools like Prometheus and Grafana are highly recommended. By combining these resources, developers can build scalable, efficient, and reliable RAG systems tailored to their specific project needs. Conclusion RAG isn’t just a tool—it’s a bridge between static knowledge and dynamic problem-solving in code generation. Think of it as a well-trained sous-chef in a bustling kitchen, fetching the exact ingredients you need, exactly when you need them. By integrating RAG into workflows, developers can focus on creativity while the system handles the grunt work of retrieval and synthesis. Take GitHub Copilot, for example. It’s not just speeding up coding; it’s reducing bugs by referencing real-world examples from vast repositories. Similarly, in a healthcare project, RAG cut development time by 30% by pulling domain-specific code snippets directly into prototypes. These aren’t just efficiency gains—they’re paradigm shifts. But RAG thrives on quality inputs. A poorly curated knowledge base is like a cluttered pantry—chaos in, chaos out. Invest in your data, and RAG will return the favor tenfold. Featured Building a Retrieval-Augmented Generation System with Deep Seek R1 This guide explores advanced strategies for optimizing DeepSeek R1 in RAG systems, including dynamic embedding scaling, multi-modal data integration, adaptive indexing, query re-ranking, caching, parallelization, and domain-specific fine-tuning. 04 Feb 2025 17 min read Featured Is RAG Dead? How DeepSeek R1 is Redefining Custom RAG Chatbots With the rise of DeepSeek R1, RAG chatbots are evolving beyond their limits. This post explores how advanced retrieval techniques, dynamic embeddings, and real-time adaptation are redefining chatbot intelligence, making them more accurate, scalable, and context-aware. 04 Feb 2025 13 min read DeepSeek's Viral Surge: RAG Implications for Enterprise AI Deployments DeepSeek's rapid adoption is making waves in enterprise AI, redefining Retrieval-Augmented Generation (RAG). This analysis explores its implications for AI deployments, efficiency, and how businesses can leverage its capabilities for smarter, scalable solutions. 03 Feb 2025 2 min read Chitika: Explore Retrieval Augmented Generation Trends © 2025 Terms of Service Privacy Policy Powered by Ghost Social Media Handles

Source: https://aboniasojasingarayar.substack.com/p/chapter-4-real-world-applications
Chapter 6 - Real-World Applications of RAG and LLMs Subscribe Sign in Share this post Abonia Sojasingarayar Chapter 6 - Real-World Applications of RAG and LLMs Copy link Facebook Email Notes More Chapter 6 - Real-World Applications of RAG and LLMs Hands-On real time RAG and LLM application Abonia Sojasingarayar Nov 07, 2024 67 Share this post Abonia Sojasingarayar Chapter 6 - Real-World Applications of RAG and LLMs Copy link Facebook Email Notes More 2 8 Share In the rapidly evolving landscape of artificial intelligence, the integration of Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) has opened up new horizons for real-world applications. This chapter delves into the practical use of RAG and LLMs across a wide array of domains, showcasing how these technologies are transforming industries and solving complex problems. By exploring case studies in various domains such as conversational AI, biomedical document understanding, and legal search, we aim to provide a comprehensive overview of how RAG and LLMs are being applied in real-world settings. Additionally, we will examine how these models are utilized to solve real-world problems, including open-domain question answering, long-form text generation, and multi-step reasoning. This exploration will not only highlight the potential of RAG and LLMs but also offer insights into the challenges and future directions of these technologies. The chapter is structured to provide a detailed look at the applications of RAG and LLMs. It begins with a deep dive into case studies across different domains, providing you with a practical understanding of how these models are being deployed. This is followed by a section on solving real-world problems with RAG, offering examples of open-domain question answering, long-form text generation, and multi-step reasoning. To ensure a broad perspective, we also explore the landscape of vector-capable solutions, including approximate nearest neighbor libraries, vector databases, and cloud offerings. Finally, we delve into specific industries to see how RAG and LLMs are being utilized, providing a comprehensive understanding of their applications and impact. Thanks for reading Abonia Sojasingarayar! Subscribe for free to receive new posts and support my work. Subscribe In this chapter, we will cover the following topics : Case studies in various domains: Solving real-world problems with RAG Landscape of vector-capable solutions How RAG and LLMs are used in specific industries Find source code used in this chapter LLM Engineering Book Case studies in various domains In our earlier chapter, we explored the functionality of the RAG pipeline. If you haven't already, we highly suggest revisiting the preceding chapters before delving into this one, as it primarily focuses on the demonstration of implementation details. Below, we provide a simple illustration of how RAG operates in a question-answer format. NaiveRAG (Naive Retrieval Augmented Generation) This is the most basic form of RAG. It involves three steps: Retriever: Searches a large corpus of text (like Wikipedia) for passages relevant to a given prompt or question. Generator: Uses the retrieved passages along with the prompt to generate a response using an LLM. (Optional) Reranker: In some cases, a reranking step might be included where the retrieved passages are scored and re-ordered based on their relevance to the prompt. NaiveRAG has limitations. The retrieved passages might not always be the most relevant, and the LLM might struggle to integrate them effectively. AdvancedRAG This builds upon NaiveRAG by addressing its limitations. It can involve various techniques like: Improved Retrieval Strategies: Optimizing how passages are searched and retrieved for better relevance. Fine-tuning the Retriever: Adapting the retrieval process based on specific tasks or domains. Advanced Prompt Engineering: Creating more specific prompts for the LLM to leverage the retrieved information effectively. ModularRAG This is the most flexible approach. It breaks down the RAG process into independent modules: Search Module: Responsible for retrieving relevant passages. Memory Module: Manages the retrieved information. Fusion Module: Combines the retrieved information with the prompt. Routing Module: Decides which information to use based on the task. Predict Module: Generates the final output using the LLM. Task Adapter Module: Adapts the entire process for specific tasks. ModularRAG allows for more customization and control over each step in the reasoning process. Both NaiveRAG and AdvancedRAG can be seen as special cases of ModularRAG with a fixed set of modules. Framework Supporting LLM powered Application Frameworks supporting RAG (Retrieval-Augmented Generation) and LLM (Large Language Models) application development include LangChain, LlamaIndex, Haystack, TinyLLM, Griptape,Embedchain and more. LangChain is an open-source framework designed to simplify the development of applications powered by large language models. It provides a comprehensive toolkit for building more complex and interactive LLM applications, going beyond basic search and retrieval. LangChain's components include chains, which allow the chaining of components together, facilitating the use of PromptTemplates and LLMChains for interactive applications. LlamaIndex is a plug-and-play solution for search-centric applications, focusing on providing quick access to specific information within large datasets. It is more of a specialized framework compared to LangChain, which offers a broader range of applications requiring deeper customization. Haystack emerges as a comprehensive NLP framework, empowering developers to craft applications infused with cutting-edge NLP models and LLMs. With a diverse range of capabilities spanning question answering, answer generation, and semantic document search, Haystack heralds a new era of NLP-driven application development. Core concepts such as Pipelines and Nodes structure and process data, while Agents, powered by LLMs, navigate complex queries. Specialized tools augment agent capabilities, exemplified by calculators or WebRetrievers, while DocumentStores provide compatibility with various database technologies. Delve into the vast potential of NLP frameworks with Haystack's robust features and functionalities. Conversational AI Creating a conversational AI chatbot tailored to specific data needs involves several steps, from processing PDF documents to integrating a large language model (LLM) for generating responses. In this section we will walk you through the process, using HuggingFace Embeddings, FAISS for vector storage, and Ollama mistral model. The langchain library is instrumental in managing conversation chains, indexing data, and crafting prompt templates. By the end of this tutorial, you'll have built a RAG system with a conversational UI, capable of detecting hallucinations in the LLM's responses. Before we start we can install Ollma in our local machine for inference as follow: 1.Download Ollama For linux users-To install Ollma on Linux, you can use the following command in your terminal. This command downloads and executes the installation script directly from the Ollma official site: curl -fsSL https://ollama.com/install.sh | sh For Windows users- Windows users can download Ollma by visiting the official Ollma website and following the link to the Windows download page:https://ollama.com/download/windows For mac users-https://ollama.com/download/mac 2. Pulling the Model After installing Ollma, you can pull the model of your choice using the following command: ollama pull mistral This command downloads the specified model (in this case, "mistral") to your local machine, making it available for inference. First, install necessary packages !pip install pypdf langchain langchain-community tiktoken llama-cpp-python panel streamlit Then, you need to process PDF documents to extract text and metadata. This step involves reading each page of the PDFs, extracting the text, and organizing it into a structured format for indexing. import PyPDF2

def prepare_docs(pdf_docs):

    docs = []

    metadata = []

    content = []

    for pdf in pdf_docs:

        pdf_reader = PyPDF2.PdfReader(pdf)

        for index, page in enumerate(pdf_reader.pages):

            doc_page = {'title': pdf + " page " + str(index + 1),

                        'content': page.extract_text()}

            docs.append(doc_page)

    for doc in docs:

        content.append(doc["content"])

        metadata.append({"title": doc["title"]})

    print("Content and metadata are extracted from the documents")

    return content, metadata Next, chunk the extracted content into smaller segments for easier processing and retrieval. This step uses the `RecursiveCharacterTextSplitter` from langchain to split the content based on a specified chunk size and overlap. from langchain.splitter import RecursiveCharacterTextSplitter

def get_text_chunks(content, metadata):

    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(

        chunk_size=512,

        chunk_overlap=256,

    )

    split_docs = text_splitter.create_documents(content, metadatas=metadata)

    print(f"Documents are split into {len(split_docs)} passages")

    return split_docs

Index the chunked documents into a FAISS-based vector database for efficient similarity search. This step uses HuggingFace Embeddings to generate embeddings for the documents, which are then stored in a FAISS database.

from langchain.vectorstore import HuggingFaceEmbeddings, FAISS

def ingest_into_vectordb(split_docs):

    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})

    db = FAISS.from_documents(split_docs, embeddings)

    DB_FAISS_PATH = 'vectorstore/db_faiss'

    db.save_local(DB_FAISS_PATH)

    return db Configure a conversational chain for the Ollama model, integrating it with the vector database for information retrieval. This setup enhances the conversational experience by combining language generation with memory and retrieval functionalities. To use another model replace the model name in Ollama(model="llama"). from langchain.memory import ConversationBufferMemory

from langchain.retrievalchain import ConversationalRetrievalChain

def get_conversation_chain(vectordb):

    llm = Ollama(model="mistral")

    retriever = vectordb.as_retriever()

    memory = ConversationBufferMemory(

        memory_key='chat_history', return_messages=True, output_key='answer'

    )

    conversation_chain = ConversationalRetrievalChain.from_llm(

        llm=llm,

        retriever=retriever,

        memory=memory,

        return_source_documents=True

    )

    print("Conversational Chain created for the LLM using the vector store")

    return conversation_chain Run below cell to prepare,chunk and vectorize the data.In below example I tested with my CV.Do not hesitate to use any pdf that you wants to work with. pdf_docs=["./data/CV.pdf"]

content, metadata = prepare_docs(pdf_docs)

split_docs = get_text_chunks(content, metadata)

vectordb=ingest_into_vectordb(split_docs)

Now , ask your Question.We created a conversational chain and now ready to chat with your own data. 

### Question 1

user_question = "who is Abonia Sojasingarayar?"

response=conversation_chain({"question": user_question})

print("Q: ",user_question)

print("A: ",response['answer']) Output: Q:  who is Abonia Sojasingarayar?
A:   Abonia Sojasingarayar is a Machine Learning Scientist, Data Scientist, NLP Engineer, Computer Vision Engineer, AI Analyst, and Technical Writer. They have education from the Université Pondicherry in India, IA School in Boulogne-Billancourt, France, and Institut F2I in Paris, France. Abonia has certifications from IBM and deeplearning.IA, and they are proficient in various tools and techniques related to their field such as Python, TensorFlow, GCP professional data engineer Badges, Watson Assistant, and RPA (Robotic Process Automation) among others. They have worked on projects involving API integration, machine learning pipeline development, and research engineering.

### Question 2 

user_question = "where did she graduated?"

response=conversation_chain({"question": user_question})

print("Q: ",user_question)

print("A: ",response['answer'])

print("\nConversation Chain: \n",response)

Output:

Q:  where did she graduated?

A:   Abonia Sojasingarayar graduated from the Université Pondicherry in India with a licence en technologie informatique et Ingénierie degree. Conversation Chain: {'question': 'where did she graduated?', 'chat_history': [HumanMessage(content='who is Abonia Sojasingarayar?'), AIMessage(content=' Abonia Sojasingarayar is a Machine Learning Scientist, Data Scientist, NLP Engineer, Computer Vision Engineer, AI Analyst, and Technical Writer. They have education from the Université Pondicherry in India…'), HumanMessage(content='where did she graduated?'), AIMessage(content=' Abonia Sojasingarayar graduated from the Université Pondicherry in India with a licence en technologie informatique et Ingénierie degree.')], 'answer': ' Abonia Sojasingarayar graduated from the Université Pondicherry in India with a licence en technologie informatique et Ingénierie degree.', 'source_documents': [Document(page_content='Abonia Sojasingarayar \n \n \n \nMachine Learning Scientist | Data Scientist | NLP Engineer | Computer Vision Engineer | AI \nAnalyst | Technical Writer \n ……", metadata={'title': './data/CV.pdf page 2'})]} So, if we observe, when I query again without explicitly specifying the name, the model is now capable of recalling previous interactions, thanks to the integration of a memory buffer. This enhancement allows the model to retain information from past conversations, enabling it to provide more contextually relevant responses in subsequent interactions. Finally, build a user interface for interacting with the chatbot. This UI allows users to ask questions related to their documents, with the application processing these questions, retrieving relevant information, and generating responses. import streamlit as st

def handle_userinput(user_question):

    response = st.session_state.conversation({'question': user_question})

    st.session_state.chat_history = response['chat_history']

    for i, message in enumerate(st.session_state.chat_history):

        if i % 2 == 0:

            st.write(user_template.replace("{{MSG}}", message.content), unsafe_allow_html=True)

        else:

            st.write(bot_template.replace("{{MSG}}", message.content), unsafe_allow_html=True)

def main():

    st.set_page_config(page_title="Chat with your PDFs", page_icon=":books:")

    st.header("Chat with multiple PDFs :books:")

    user_question = st.text_input("Ask a question about your documents:")

    if user_question:

        handle_userinput(user_question)

    with st.sidebar:

       st.subheader("Your documents")

        pdf_docs = st.file_uploader("Upload your PDFs here and click on 'Process'", accept_multiple_files=True)

        if st.button("Process"):

            content, metadata = prepare_docs(pdf_docs)

            split_docs = get_text_chunks(content, metadata)

            vectorstore = ingest_into_vectordb(split_docs)

            st.session_state.conversation = get_conversation_chain(vectorstore) Now, with our RAG conversational chatbot, we have the flexibility to upload any PDF document and immediately initiate a conversation. Biomedical document understanding Biomedical document understanding through Large Language Models (LLMs) involves leveraging advanced AI technologies to process and comprehend vast amounts of medical text, such as research papers, clinical studies, and patient records. This capability is crucial for healthcare providers to stay updated with the latest medical information and make informed decisions. LLMs, specifically designed for healthcare applications, can analyze complex medical texts, extract meaningful information, and generate insights for healthcare professionals. They are distinguished by the databases they were trained on, with clinical LLMs focusing on medical literature for diagnostic support and biomedical LLMs facilitating fast and accessible biomedical text mining.LLMs in healthcare have numerous use cases, including processing extensive databases of medical literature and patient data, learning from historical cases, and providing insights for accurate and timely diagnosis. For example, a LLM can analyze a patient’s symptoms, medical history, and clinical findings to generate a personalized treatment plan, incorporating the latest research findings and treatment guidelines. This enhances patient care and outcomes by enabling healthcare professionals to make more informed decisions. Pretrained models like BioBERT, ClinicalBERT, BlueBERT, and BioGPT have shown significant advancements in applying AI in the medical field. BioBERT, trained on large-scale biomedical corpora, excels in understanding complex medical texts and terminology, making it effective for tasks like disease prediction and drug-drug interaction analysis. ClinicalBERT, adapted from BioBERT, is fine-tuned on clinical notes for more accurate patient data analysis and decision support. BlueBERT offers a balanced understanding of both biomedical and clinical texts, making it versatile for various applications. BioGPT, a generative pretrained transformer model, is useful for generating coherent medical text. Med-PaLM, a large-scale generalist biomedical AI system, stands out as a multimodal generative model designed to handle various types of biomedical data, including clinical language, medical imaging, and genomics. It leverages advances in language and multimodal foundation models, allowing for rapid adaptation to different tasks and settings. Med-PaLM achieves remarkable performance on a wide range of tasks within the MultiMedBench benchmark, often surpassing state-of-the-art specialist models. This model demonstrates promising potential for downstream data-scarce biomedical applications and has the ability to process inputs with multiple images during inference, effectively handling complex medical scenarios. Legal Summarizer A Legal Summarizer that simplifies complex legal documents, making them easier to understand for various audiences, including lawyers and non-lawyers. It extracts and condenses the key points, legal terms, and essential information from legal documents, such as court cases, legal briefs, and legislation, into concise summaries. This is particularly useful for: Lawyers: They can quickly grasp the core arguments, legal issues, and outcomes without sifting through lengthy documents. It helps in preparing for legal cases, advising clients, and researching legal topics more efficiently. Non-lawyers: For individuals who need to understand legal documents for personal, business, or educational reasons but lack expertise in legal terminology. It provides a clear, accessible summary of legal documents, enabling them to make informed decisions or understand legal implications more effectively. Legal Education and Research: Students and researchers can use Legal Summarizers to grasp complex legal concepts and cases without the need to read through entire volumes of legal texts. This aids in studying law, conducting legal research, and preparing for exams. Legal Assistants and Support Staff: They can use summaries to provide clients or colleagues with essential information from legal documents, making it easier to convey complex legal issues in a straightforward manner. To perform document summarization using LLMs with the LangChain library, you have three main options: Stuff, Map-Reduce, and Refine. In the coming section we will see the hands on guide for each method. Option 1: Stuff This method involves stuffing all your documents into a single prompt and passing it to an LLM. Import necessary modules and define the prompt template. Create an LLM chain with the defined prompt. Define a StuffDocumentsChain that takes the LLM chain and combines all documents into a single prompt. Run the summarization. from langchain.chains.combine_documents.stuff import StuffDocumentsChain

from langchain.chains.llm import LLMChain

from langchain.prompts import PromptTemplate

loader = PyPDFLoader("./data/Raptor-Agreement.pdf")

# Define prompt

prompt_template = """Write a concise summary of the following:

"{text}"

CONCISE SUMMARY:"""

prompt = PromptTemplate.from_template(prompt_template)

# Define LLM chain

llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo-16k")

llm_chain = LLMChain(llm=llm, prompt=prompt)

# Define StuffDocumentsChain

stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name="text")

docs = loader.load()

print(stuff_chain.run(docs)) Due to the limitations of summarizing lengthy content, we may not be able to provide the full summary results here. However, you're welcome to clone the GitHub repository associated with this project. Within the repository, you'll find notebooks corresponding to each chapter. By running these notebooks, you'll be able to explore the complete results and gain a deeper understanding of the concepts discussed. Feel free to experiment. Option 2: Map-Reduce This method involves summarizing each document individually (map) and then combining these summaries into a final summary (reduce). -Define the map and reduce prompts. -Create an LLM chain for mapping each document to an individual summary. -Use a ReduceDocumentsChain to combine the summaries. -Optionally, use a MapReduceDocumentsChain to automate the process. from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain

from langchain_text_splitters import CharacterTextSplitter

llm = ChatOpenAI(temperature=0)

# Map

map_template = """The following is a set of documents

{docs}

Based on this list of docs, please identify the main themes 

Helpful Answer:"""

map_prompt = PromptTemplate.from_template(map_template)

map_chain = LLMChain(llm=llm, prompt=map_prompt)

# Reduce

reduce_template = """The following is set of summaries:

{docs}

Take these and distill it into a final, consolidated summary of the main themes. 

Helpful Answer:"""

reduce_prompt = PromptTemplate.from_template(reduce_template)

# Run chain

reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)

combine_documents_chain = StuffDocumentsChain(

    llm_chain=reduce_chain, document_variable_name="docs"

)

reduce_documents_chain = ReduceDocumentsChain(

    combine_documents_chain=combine_documents_chain,

    collapse_documents_chain=combine_documents_chain,

    token_max=4000,

)

map_reduce_chain = MapReduceDocumentsChain(

    llm_chain=map_chain,

    reduce_documents_chain=reduce_documents_chain,

    document_variable_name="docs",

    return_intermediate_steps=False,

)

text_splitter = CharacterTextSplitter.from_tiktoken_encoder(

    chunk_size=1000, chunk_overlap=0

)

split_docs = text_splitter.split_documents(docs)

print(map_reduce_chain.run(split_docs)) Here is the output: This document outlines the terms and conditions of an Agreement between Raptor Technologies, LLC (Raptor) and a Subscriber organization for access to Raptor's Subscription Services. The key themes include: 1. License and Terms: Raptor grants a limited, non-exclusive license to the Subscriber to use its Subscription Services subject to certain terms and conditions. The Subscriber is responsible for providing their own Internet access and equipment to use the Subscription Services. 2. Confidentiality: The Subscriber agrees to keep confidential any information related to the Subscription Services and Equipment provided by Raptor, except as expressly permitted. 3. Data Collection and Distribution: The Subscriber is prohibited from disclosing or making public individual's personally identifying information obtained through the Subscription Services except as required in the ordinary course of business or by applicable law. 4. Fees and Term: The Agreement has an initial term of one year, during which the Subscriber must pay the Annual Software Access Fee for each Campus that will utilize the Subscription Services. Upon termination, all amounts due to Raptor remain payable and all licenses granted under the Agreement terminate at the end of the pre-paid annual term. 5. Termination: The Subscriber may terminate the Agreement with 60 days' written notice prior to the end of the then-current term. Sections 1, 2, 3, 6, and 7 survive termination. 6. Disclaimers: Raptor does not guarantee or warrant any information made available within the Subscription Services, including determinations of an individual's registered sex offender status or custom alert status. The Subscriber is responsible for ensuring compliance with applicable laws and regulations related to data collection and distribution. 7. Other Provisions: The Agreement includes provisions related to acts beyond Raptor's control, lack of creation of partnership or agency relationship, and non-assignment by the Subscriber without consent. Contact information for written notices and effective date are also included. Option 3: Refine This method involves iteratively refining a summary based on new context. -Define the prompt template for refining. -Load the summarize chain with the refine chain type. -Run the summarization with the input documents. from langchain import load_summarize_chain, PromptTemplate

prompt_template = """Write a concise summary of the following:

"{text}"

CONCISE SUMMARY:"""

prompt = PromptTemplate.from_template(prompt_template)

chain = load_summarize_chain(llm, chain_type="refine")

chain.run(split_docs) Output: " This document outlines the terms of a subscription agreement between Subscriber (district/school or organization) and Raptor Technologies LLC (Raptor) for access to Raptor's Subscription Services. The agreement grants Subscriber a limited, non-exclusive license to use the services in accordance with the agreement and applicable laws. Confidential information provided by Raptor must be kept confidential and not disclosed to third parties without prior written consent. Individual's personally identifying information obtained through the services must not be disclosed except as required by law or in the ordinary course of business. Subscriber is responsible for providing its own Internet access and equipment to use the services, and fees are payable annually in advance. The agreement has an initial term of one year, with automatic renewal unless written notice of non-renewal is given.\n\nRaptor disclaims all responsibility for determinations of an individual’s registered sex offender status or custom alert status based on the information conveyed in connection with the Subscription Services. Subscriber is solely responsible for such determinations and understands that information provided by Raptor is not intended to substitute for the determinations made by Subscriber and its employees and contractors.\n\nThe agreement may be amended only pursuant to a written agreement between the Parties. All terms and conditions of this Agreement shall be binding upon, inure to the benefit of, and be enforceable by, the Parties and their respective successors and permitted assigns. Raptor will not be in default of this Agreement for any performance failure caused by occurrences beyond Raptor’s reasonable control (including, but not limited to, acts of God). This Agreement does not create any right enforceable by any person not a party. Nothing in this Agreement shall create the relationship of partners or principal-agent between the parties. Subscriber may not assign this Agreement without the prior written consent of Raptor. The waiver or failure of Raptor to exercise in any respect any right provided for under this Agreement shall not be deemed a waiver of any further right under this Agreement." Each of these methods has its use cases depending on the specific requirements of your document summarization task. The Stuff method is simpler but may not capture all nuances. Map-Reduce provides a more detailed approach but requires more setup. Refine offers a way to iteratively improve a summary based on additional context. Solving real-world problems with RAG Retrieval-Augmented Generation (RAG) approach addresses the challenge of generating factually accurate and coherent long-form text for open-domain question answering (QA), long-form text generation, and multi-step reasoning tasks. By retrieving relevant documents and incorporating their information into the model's responses, RAG improves the relevance, factual correctness, and attribution of the generated text, making it particularly useful for domains requiring external knowledge such as science, medicine, and technical support. 1.Open-domain QA Let's dive directly to the demo.LangChain provides multiple built-in document loaders, that work with PDF files, JSON files, or a Python file in your file directory. We can use LangChain’s PyPDFLoader to import your PDF seamlessly. Here we will be using the data directly from the website to ask questionsHere we will be using medium articles. Install necessary packages !pip install nest_asyncio langchain_community langchain playwright html2text sentence-transformers faiss-cpu

from langchain_community.document_transformers import Html2TextTransformer

from langchain.text_splitter import CharacterTextSplitter

from langchain_community.embeddings import HuggingFaceEmbeddings

from langchain_community.vectorstores import FAISS

import nest_asyncio

nest_asyncio.apply()

# Articles to index

articles = ["https://medium.com/@abonia/bertscore-explained-in-5-minutes-0b98553bfb71",

            "https://medium.com/@abonia/document-based-llm-powered-chatbot-bb316009de93/",]

# Scrapes the blogs above

loader = AsyncChromiumLoader(articles)

docs = loader.load()

 When our document is long, it’s necessary to split up our document text into chunks. There are various ways to split your text. Let’s just use the simplest method CharacterTextSplitter to split based on characters and measure chunk length by the number of characters. # Converts HTML to plain text 

html2text = Html2TextTransformer()

docs_transformed = html2text.transform_documents(docs)

# Chunk text

text_splitter = CharacterTextSplitter(chunk_size=100, 

                                      chunk_overlap=0)

chunked_documents = text_splitter.split_documents(docs_transformed)

# Load chunked documents into the FAISS index

db = FAISS.from_documents(chunked_documents, 

                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))

retriever = db.as_retriever() The text chunks are then translated into numerical vectors through embeddings, allowing us to work with text data like semantic search in a computationally efficient manner. We can choose an embedding model provider like OpenAI, HuggingFaceEmbedding, Jina etc for this task.We then need to store our embedding vectors in a vector store, which allows us to search and retrieve the relevant vectors at query time. We can expose the vector store in a retriever interface. To retrieve text, we can choose a search type like “similarity” to use similarity search in the retriever object where it selects text chunk vectors that are most similar to the question vector. k=2 lets us find the top 2 most relevant text chunk vectors. A RetrievalQA chain chains a large language model with our retriever interface. You can also define the chain type as one of the four options: “stuff,” “map reduce,” “refine,” “map_rerank.” The default chain_type=”stuff” incorporates ALL text from the documents into the prompt. The “map_reduce” type breaks texts into groups, poses the question to the LLM for each batch separately, and derives the ultimate answer based on the replies from each batch. The “refine” type partitions texts into batches, presents the first batch to the LLM, and then submits the answer along with the second batch to the LLM. It progressively refines the answer by processing through all the batches. The “map-rerank” type divides texts into batches, submits each one to the LLM, returns a score indicating how comprehensively it answers the question, and determines the final answer based on the highest-scoring replies from each batch. from langchain import PromptTemplate

from langchain_community.llms import Ollama

from langchain.chains import RetrievalQA

prompt_template = """

### [INST] Instruction: Answer the question based on the medium article knowledge. Here is context to help:

{context}

### QUESTION:

{question} [/INST]

 """

# Create prompt from prompt template

prompt = PromptTemplate(

    input_variables=["context", "question"],

    template=prompt_template,

)

llm = Ollama(model="mistral")

qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True, chain_type_kwargs={"prompt": prompt_template},)

answer = qa.invoke("What is cosine similarity?")
 Output {'query': 'What is cosine similarity?', 'result': ' Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space. It is computed as the cosine of the angle between them, which indicates how similar they are in direction. The result ranges from -1 to 1, with 1 indicating perfect similarity and 0 indicating orthogonal (perpendicular) vectors.', 'source_documents': [Document(page_content='The formula for cosine similarity is:\n\n> similarity(A, B) = (A . B) / (||A|| ||B||)', metadata={'source': 'https://medium.com/@abonia/document-based-llm-powered-chatbot-bb316009de93/'}), Document(page_content='Cosine similarity — This method measures the cosine of the angle between two\nvectors, which indicates how similar they are in direction. Cosine similarity\nranges from -1 to 1, with 1 indicating perfect similarity.', metadata={'source': 'https://medium.com/@abonia/document-based-llm-powered-chatbot-bb316009de93/'}), Document(page_content='Cosine Similarity and Cosine Distance — credit', metadata={'source': 'https://medium.com/@abonia/document-based-llm-powered-chatbot-bb316009de93/'}), Document(page_content='Where A and B are the two vectors being compared, . is the dot product of the\nvectors, and || || represents the Euclidean norm (magnitude) of the vectors.', metadata={'source': 'https://medium.com/@abonia/document-based-llm-powered-chatbot-bb316009de93/'})]} Sample code to build RAG Chain llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)

rag_chain = ( 

 {"context": retriever, "question": RunnablePassthrough()}

    | llm_chain

)

result = rag_chain.invoke("What is cosine similarity??")

print(result['text']) 2.Long-form text generation Multi-step reasoning and Augmentation To implement multi-step reasoning in a Retrieval-Augmented Generation (RAG) application, we can follow a multi-stage retrieval process that combines different retrieval methods for improved overall quality. Augmentation involves the process of effectively integrating context from retrieved passages with the current generation task. Before discussing more on the augmentation process, augmentation stages, and augmentation data, here is a taxonomy of RAG's core components: Retrieval augmentation can be applied in many different stages such as pre-training, fine-tuning, and inference. Augmentation Stages: RETRO is an example of a system that leverages retrieval augmentation for large-scale pre-training from scratch; it uses an additional encoder built on top of external knowledge. Fine-tuning can also be combined with RAG to help develop and improve the effectiveness of RAG systems. At the inference stage, many techniques are applied to effectively incorporate retrieved content to meet specific task demands and further refine the RAG process. Augmentation Source: A RAG model's effectiveness is heavily impacted by the choice of augmentation data source. Data can be categorized into unstructured, structured, and LLM-generated data. Augmentation Process: For many problems (e.g., multi-step reasoning), a single retrieval isn't enough so a few methods have been proposed: Iterative retrieval enables the model to perform multiple retrieval cycles to enhance the depth and relevance of information. Notable approaches that leverage this method include RETRO and GAR-meets-RAG. Recursive retrieval recursively iterates on the output of one retrieval step as the input to another retrieval step; this enables delving deeper into relevant information for complex and multi-step queries (e.g., academic research and legal case analysis). Notable approaches that leverage this method include IRCoT and Tree of Clarifications. Adaptive retrieval tailors the retrieval process to specific demands by determining optimal moments and content for retrieval. Notable approaches that leverage this method include FLARE and Self-RAG. The figure below depicts a detailed representation of RAG research with different augmentation aspects, including the augmentation stages, source, and process. Landscape of vector-capable solutions In the rapidly evolving landscape of technology, the advent of vector-capable solutions has marked a significant shift in how we process, store, and retrieve information. These solutions, rooted in the realm of Generative AI and Large Language Models (LLMs), have transformed the way we interact with data, offering unprecedented capabilities in areas such as computer vision, recommendation systems, and natural language processing tasks. This introduction to the landscape of vector-capable solutions aims to explore the essence of vector databases, their role in augmented generation (RAG), and their potential to revolutionize the future of data management and AI applications. Vector databases, at the heart of this landscape, are purpose-built to efficiently manage high-dimensional data represented as vectors. These vectors serve as mathematical representations of various data types, such as text, images, and videos, capturing their features and relationships in a way that traditional databases struggle to achieve. By leveraging vector embeddings, vector databases enable sophisticated search and retrieval mechanisms, turning raw data into a format that AI models can comprehend and utilize effectively. This capability is particularly crucial in the era of RAG, where the integration of vector databases plays a pivotal role in enriching LLMs with additional data and context, enhancing their performance and capabilities. The landscape of vector-capable solutions is not just limited to vector databases. It encompasses a range of tools and technologies designed to support the efficient creation, storage, and querying of vector embeddings. This includes open-source models like Google's 'text2vec' and 'BERT', as well as proprietary models developed by leading AI research institutions. These models are instrumental in generating vector embeddings, which are then stored in vector databases for subsequent retrieval and analysis.Moreover, the landscape is continually evolving, with new solutions emerging to address the growing demands of applications that require high-performance similarity search, real-time querying, and scalable deployment. Services like Pinecone, designed for scalable, high-performance similarity search, and SingleStore, known for its high performance and scalability, represent just a fraction of the innovative offerings in this space. These solutions not only cater to the specific needs of vector-intensive applications but also integrate seamlessly with broader data management and AI infrastructures, setting new standards for efficiency, scalability, and performance. Approximate nearest neighbor libraries These libraries play a crucial role in optimizing the search for nearest neighbors in high-dimensional spaces, where traditional methods often fall short due to computational constraints. The importance of ANN libraries cannot be overstated, as they enable efficient similarity searches in scenarios ranging from recommendation systems to image and document search, natural language processing, and fraud detection. Annoy Annoy, developed by Spotify, stands out as a notable ANN library. It is available in both C++ and Python, optimized for memory usage and facilitating the loading and saving of large datasets to disk. Annoy is designed to create large read-only file-based data structures that can be memory-mapped into memory, allowing multiple processes to share the same data efficiently. This makes it particularly suitable for applications that require high performance and scalability, such as Spotify's personalization and recommendation systems. ANN Library The ANN Library, created by David M. Mount and Sunil Arya, is another significant contribution to the ANN domain. Written in C++, this library supports both exact and approximate nearest neighbor searching in high dimensions. It implements various data structures based on kd-trees and box-decomposition trees and employs different search strategies. The library is designed to handle datasets ranging in size from thousands to hundreds of thousands points and dimensions up to 20. It allows users to specify a maximum approximation error bound, enabling a trade-off between accuracy and running time. FLANN and NMSLIB In addition to Annoy and the ANN Library, Python users have access to FLANN and NMSLIB. FLANN is a versatile library that implements a variety of ANN algorithms, including ball trees, KD trees, and LSH. NMSLIB, on the other hand, offers implementations of different ANN algorithms, including HNSW, catering to a broad range of applications. Faiss and HNSW Faiss, developed by Facebook AI, is a library designed to provide efficient similarity search and clustering of dense vectors. It supports a wide range of index types, including those based on hierarchical navigable small world (HNSW) graphs, which are particularly effective for high-dimensional data. The HNSW algorithm is known for its speed and memory efficiency, making it an excellent choice for applications requiring real-time vector search capabilities. Vector databases The landscape of vector databases is vast and growing, with several notable options available for different use cases. Some of the top vector databases in 2023 and 2024 include: Pinecone: A fully managed cloud service that simplifies the deployment and scaling of vector search systems. Milvus: An open-source vector database that supports various data types and integrates with machine learning models for automatic vectorization. Chroma: A vector database designed for high-performance similarity search and analytics. Weaviate: An open-source, graph-based vector database that offers both cloud and self-hosted deployment options. Deep Lake: A vector database focused on deep learning applications. Qdrant: A vector database that emphasizes high-performance similarity search and analytics. Elasticsearch: A widely used search and analytics engine that also supports vector search capabilities. Vespa: A real-time big data processing and serving engine that includes vector search capabilities. Vald: An open-source vector database designed for high-speed similarity search. ScaNN: A library developed by Google Research for efficient vector similarity search. Pgvector: An extension for PostgreSQL that adds support for vector data types and functions. Faiss: A library developed by Facebook AI Research for efficient similarity search and clustering of dense vectors. ClickHouse: An open-source column-oriented database management system that supports vector search. OpenSearch: A community-driven, open-source search and analytics suite that includes vector search capabilities. Apache Cassandra: A highly scalable, distributed NoSQL database that can be extended to support vector data types Vector Store in Langchain pip install chromadb We use OpenAIEmbeddings so we have to get the OpenAI API Key. import os

import getpass

os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')

from langchain_community.document_loaders import TextLoader

from langchain_openai import OpenAIEmbeddings

from langchain_text_splitters import CharacterTextSplitter

from langchain_community.vectorstores import Chroma

# Load the document, split it into chunks, embed each chunk and load it into the vector store.

raw_documents = TextLoader('press_conference.txt').load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)

documents = text_splitter.split_documents(raw_documents)

db = Chroma.from_documents(documents, OpenAIEmbeddings())

Similarity search

query = "What did the president say about Ketanji Brown Jackson"

docs = db.similarity_search(query)

print(docs[0].page_content)
 Output: Today, I urge the Senate to prioritize critical legislation for the American people. Let's move forward with passing the Climate Action Plan, investing in renewable energy, and protecting our planet for future generations. I also want to take a moment to recognize the dedication and service of our frontline healthcare workers. From doctors to nurses to medical staff, your tireless efforts during these challenging times have not gone unnoticed. Thank you for your unwavering commitment to keeping our communities safe and healthy It is also possible to do a search for documents similar to a given embedding vector using similarity_search_by_vector which accepts an embedding vector as a parameter instead of a string. embedding_vector = OpenAIEmbeddings().embed_query(query)

docs = db.similarity_search_by_vector(embedding_vector)

print(docs[0].page_content) The query is the same, and so the result is also the same. Today, I urge the Senate to prioritize critical legislation for the American people. Let's move forward with passing the Climate Action Plan, investing in renewable energy, and protecting our planet for future generations. I also want to take a moment to recognize the dedication and service of our frontline healthcare workers. From doctors to nurses to medical staff, your tireless efforts during these challenging times have not gone unnoticed. Thank you for your unwavering commitment to keeping our communities safe and healthy Asynchronous operations Vector stores are usually run as a separate service that requires some IO operations, and therefore they might be called asynchronously. That gives performance benefits as you don't waste time waiting for responses from external services. That might also be important if you work with an asynchronous framework, such as FastAPI. LangChain supports async operation on vector stores. All the methods might be called using their async counterparts, with the prefix a, meaning async. Qdrant is a vector store, which supports all the async operations, thus it will be used in this walkthrough. pip install qdrant-client 
from langchain_community.vectorstores import Qdrant

Create a vector store asynchronously

db = await Qdrant.afrom_documents(documents, embeddings, "http://localhost:6333")

Similarity search

query = "What statements did the president make regarding the Supreme Court nominee during the recent press conference?"

docs = await db.asimilarity_search(query)

print(docs[0].page_content) Output: Today, I urge the Senate to prioritize critical legislation for the American people. Let's move forward with passing the Climate Action Plan, investing in renewable energy, and protecting our planet for future generations. I also want to take a moment to recognize the dedication and service of our frontline healthcare workers. From doctors to nurses to medical staff, your tireless efforts during these challenging times have not gone unnoticed. Thank you for your unwavering commitment to keeping our communities safe and healthy Similarity search by vector embedding_vector = embeddings.embed_query(query)

docs = await db.asimilarity_search_by_vector(embedding_vector) Cloud offerings Azure Cloud Offerings for Vector Databases Azure Cosmos DB, Azure Cognitive Search, Azure SQL, Azure Cache for Redis (Enterprise), and Azure Data Explorer (ADX) provide a robust suite of services for vector database requirements. These services cater to various applications, from MongoDB and PostgreSQL compatible services to AI-oriented applications with Azure AI Search. Azure also offers the option to host popular "vector native" databases like Pinecone, Qdrant, FAISS, Milvus, and Elastic Search on Azure, providing a flexible and scalable infrastructure for managing vector embeddings and enhancing AI capabilities through vector search and retrieval-augmented generation (RAG). AWS Cloud Offerings for Vector Databases Amazon Web Services (AWS) offers a comprehensive suite of services for vector databases, including Amazon Aurora PostgreSQL-Compatible Edition, Amazon RDS for PostgreSQL, Amazon Neptune ML, Vector Search for Amazon MemoryDB for Redis, Amazon DocumentDB (with MongoDB compatibility), and Amazon OpenSearch Service. These services support the storage, indexing, and searching of high-dimensional vector data, making them ideal for machine learning applications and complex graph analysis. Google Cloud Platform (GCP) Offerings for Vector Databases Google Cloud Platform has integrated LangChain with all of its database offerings, including CloudSQL, Spanner, Firestore, Bigtable, and Memorystore for Redis, to enhance generative AI applications. These integrations support vector search and retrieval-augmented generation, enabling the development of applications that leverage Large Language Models (LLMs) with enterprise data. How RAG and LLMs are used in specific industries Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) are increasingly being used across various industries to enhance productivity, accuracy, and efficiency in tasks that require up-to-date, domain-specific knowledge. Here's how they are applied in specific industries: Healthcare In healthcare, RAG can be used to develop AI systems that provide accurate, up-to-date medical information to patients and healthcare providers. For example, RAG can be integrated into patient management systems to offer personalized medical advice based on the latest research and patient records, ensuring that healthcare providers have access to the most relevant information at their fingertips. Legal For the legal industry, RAG can significantly enhance the efficiency of legal research and case preparation. By integrating RAG with legal databases, AI systems can provide accurate citations, legal precedents, and case summaries, aiding lawyers in preparing arguments and ensuring the legality and accuracy of their cases. This not only improves the quality of legal services but also enhances auditability and transparency in legal proceedings. Finance and Banking In finance and banking, RAG can be used to develop AI-powered financial advisors that offer personalized financial advice based on real-time market data and customer financial information. This can include providing insights on investment opportunities, financial planning, and risk management, helping clients make informed financial decisions. Education In the education sector, RAG can be utilized to create AI-powered tutoring systems that offer personalized learning experiences. These systems can provide students with accurate, up-to-date information on various subjects, helping them stay ahead in their studies. RAG can also be used to develop AI-powered grading systems that provide detailed feedback on assignments, enhancing the learning experience for students. E-commerce For e-commerce businesses, RAG can be integrated into customer service chatbots to provide customers with accurate, real-time product information, shipping updates, and personalized recommendations. This can significantly improve customer satisfaction and increase sales by providing personalized shopping experiences. Manufacturing In manufacturing, RAG can be used to develop AI systems that monitor equipment and processes in real-time, providing operators with accurate, up-to-date information to ensure optimal performance and efficiency. This can help in predictive maintenance, reducing downtime and improving product quality. Environmental Monitoring For environmental monitoring, RAG can be integrated into AI systems that analyze satellite data and sensor readings to provide real-time information on environmental conditions. This can help in monitoring pollution levels, wildlife populations, and weather patterns, aiding in environmental conservation efforts. Summary Retrieval-Augmented Generation (RAG) systems, leveraging Large Language Models (LLMs), revolutionize various industries by enhancing AI capabilities beyond static training data. RAG facilitates real-time data integration, reducing costs and enhancing security by keeping sensitive data outside the model and allowing for real-time access restrictions. It offers greater explainability, reduces the likelihood of generating false information (hallucination), and overcomes context size limitations by dynamically retrieving relevant documents. This technology is instrumental in compliance checks, B2B sales, customer feedback analysis, product recommendations, financial consultation, insurance claims processing, financial reporting, and enhanced portfolio management, ensuring accuracy, efficiency, and security in these critical business processes. RAG's ability to dynamically pull relevant information from comprehensive databases ensures up-to-date, accurate, and personalized responses, making it a transformative tool in the evolving landscape of AI and business automation. Quiz questions Here is a quiz to assess understanding of this chapter : 1. Which of the following is NOT a common application of Conversational AI? - A. Personalized customer service - B. Voice-activated smart home devices - C. Social media bots - D. Real-time translation services - **Correct Answer: C. Social media bots 2. What is a significant challenge in Biomedical Document Understanding? - A. Lack of standardization in document formats - B. Difficulty in understanding complex medical terminologies - C. High computational cost - D. All of the above 3. In the context of Legal Search, what does AI primarily help with? - A. Streamlining the legal research process - B. Analyzing case law - C. Predicting legal outcomes - D. Writing legal documents 4. Which of the following is a benefit of using RAG (Retrieval-Augmented Generation) for Long-form text generation? - A. Improved accuracy in text generation - B. Reduced need for extensive training data - C. Increased speed in generating long texts - D. All of the above 5. Which of the following is NOT a type of Approximate Nearest Neighbor library? - A. Faiss - B. Annoy - C. Euclidean - D. PCA 6. Which industry does RAG (Retrieval-Augmented Generation) most commonly benefit? - A. Healthcare - B. Finance - C. Manufacturing - D. Retail 7. What is a key advantage of Conversational AI in customer service? - A. It allows for personalized interactions - B. It reduces the need for human agents - C. It can handle multiple queries simultaneously - D. It improves the speed of customer service 8. Which of the following is a primary goal of Biomedical Document Understanding? - A. To replace human medical professionals - B. To understand complex medical terminologies and documents - C. To automate medical procedures - D. To develop new drugs 9. Which of the following is a key benefit of using RAG for Long-form text generation? - A. It can only generate short texts. - B. It can generate long, coherent texts based on a given prompt. - C. It is faster than traditional methods. - D. It requires no training data. 10. Which of the following is a major use case for vector databases? - A. Storing and retrieving large datasets - B. Real-time data analysis - C. Performing similarity searches - D. All of the above Correct Answers: C. Social media bots D. All of the above A. Streamlining the legal research process D. All of the above C. Euclidean A. Healthcare A. It allows for personalized interactions B. To understand complex medical terminologies and documents B. It can generate long, coherent texts based on a given prompt. C. Performing similarity searches Connect with Me If you have any inquiries, feel free to reach out via message or email. Website/Newletter Connect with me on Linkedin Find me on Github Visit my technical channel on Youtube Thanks for reading Abonia Sojasingarayar! Subscribe for free to receive new posts and support my work. Subscribe 67 Share this post Abonia Sojasingarayar Chapter 6 - Real-World Applications of RAG and LLMs Copy link Facebook Email Notes More 2 8 Share Discussion about this post Comments Restacks Vivek Somvanshi Nov 7 Liked by Abonia Sojasingarayar Very helpful 👌 Expand full comment Reply Share 1 reply by Abonia Sojasingarayar 1 more comment... Top Latest Discussions No posts Ready for more? Subscribe © 2025 Abonia Sojasingarayar Privacy ∙ Terms ∙ Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts

Source: https://www.zenml.io/llmops-database/optimizing-rag-systems-lessons-from-production
AWS GenAIIC: Optimizing RAG Systems: Lessons from Production - ZenML LLMOps Database Product DATA SCience Iterate at warp speed Accelerate your ML workflow seamlessly Auto-track everything Automatic logging and versioning Shared ML building blocks Boost team productivity with reusable components Infrastructure Backend flexibility, zero lock-in One framework for all your MLOps and LLMOps needs Limitless scaling Effortlessly deploy across clouds Streamline cloud expenses Gain clarity on resource usage and costs Organization ZenML Pro Our managed control plane for MLOps ZenML vs Other Tools Compare ZenML to other ML tools Integrations 50+ integrations to ease your workflow Solutions GENAI & LLMS Finetuning LLMs Customize large language models for specific tasks Productionalizing a RAG application Deploy and scale RAG systems LLMOps Database A curated knowledge base of real-world implementations mlops Building Enterprise MLOps Platform architecture and best practices Abstract cloud compute Simplify management of cloud-based ML resources Track metrics and metadata Monitor and analyze ML model performance and data Success Stories Adeo Leroy Merlin Retail Brevo Email Marketing Developers Documentation Docs Comprehensive guides to use ZenML Deploying ZenML Understanding ZenML system architecture Tutorials Comprehensive guides to use ZenML GUIDES Quickstart Quickly get your hands dirty Showcase Projects of ML use cases built with ZenML Starter Guide Get started with the basics COMMUNITY Slack Join our Slack Community Changelog Discover what’s new on ZenML Roadmap Join us on our MLOps journey PricingBlog Sign In Start Free LLMOps Database Tech AWS GenAIIC Company AWS GenAIIC Title Optimizing RAG Systems: Lessons from Production Industry Tech Link https://aws.amazon.com/blogs/machine-learning/from-rag-to-fabric-lessons-learned-from-building-real-world-rags-at-genaiic-part-1?tag=soumet-20 Year 2024 Summary (short) AWS GenAIIC shares comprehensive lessons learned from implementing Retrieval-Augmented Generation (RAG) systems across multiple industries. The case study covers key challenges in RAG implementation and provides detailed solutions for improving retrieval accuracy, managing context, and ensuring response reliability. Solutions include hybrid search techniques, metadata filtering, query rewriting, and advanced prompting strategies to reduce hallucinations. Tags amazon_aws chunking elasticsearch embeddings error_handling monitoring prompt_engineering rag reliability scalability semantic_search serverless structured_output unstructured_data # RAG Implementation Lessons from AWS GenAIIC ## Overview and Context AWS GenAIIC (AWS Generative AI Innovation Center) has accumulated significant experience implementing RAG (Retrieval-Augmented Generation) solutions across various industries. This case study details their practical insights and best practices for building production-ready RAG systems. ## Key Technical Components ### RAG Architecture Fundamentals - Basic RAG workflow consists of three main components: - Vector stores are central to implementation ### Implementation Stack - OpenSearch Serverless for vector search - Amazon Bedrock for: - Custom chunking and ingestion pipelines - Query transformation and rewriting components ## Technical Challenges and Solutions ### Retrieval Optimization - Hybrid Search Implementation - Metadata Enhancement - Query Rewriting ### Chunking Strategies - Section-based Chunking - Small-to-Large Retrieval ### Advanced Production Features - Custom Embedding Training - Evaluation and Monitoring ### Response Quality Control - Hallucination Prevention - System Integration ## Production Considerations ### Performance Optimization - Balancing chunk sizes for retrieval accuracy - Managing embedding model context windows - Optimizing query rewriting latency - Implementing efficient quote verification ### Scalability Aspects - Vector store performance at scale - Managing large document collections - Handling concurrent queries - Resource allocation for embedding generation ### Reliability Mechanisms - Quote verification systems - Source tracking implementation - User feedback integration - Error handling and fallback strategies ## Results and Impact - Improved retrieval accuracy through hybrid search - Better response quality with enhanced context management - Reduced hallucinations through systematic verification - More efficient query processing through rewriting - Enhanced user trust through source verification ## Lessons Learned - Retrieval quality is crucial for RAG success - Multiple optimization techniques often needed - Balance between precision and context is key - Source verification critical for production systems - Evaluation needs both automated and human components Start your new ML Project today with ZenML Pro Join 1,000s of members already deploying models with ZenML. Learn more Try Free Simplify MLOps Product Features ZenML Pro New OSS vs Managed Integrations Pricing Resources Newsletter New Blog Docs Roadmap Slack Company Careers About Us Our Values Join Us ZenML vs Orchestrators Apache Airflow Dagster Databricks Flyte Kedro Kubeflow Prefect ZenML vs Exp Trackers MLflow Weights & Biases Neptune AI CometML ZenML vs e2e Platforms AWS Sagemaker ClearML Metaflow Valohai GCP Vertex AI Azure ML ClearML GenAI & LLMs LLMOps Database Finetuning LLMs Creating a code copilot Cheap GPU compute MLOps Platform Mix and match tools Create alerting Plugin custom stack components Leveraging Hyperscalers Train on Spot VMs Deploying Sagemaker Endpoints Managing GCP Vertex AI Training on Kubernetes Local to Sagemaker Pipelines © 2024 ZenML. All rights reserved. ImprintPrivacy PolicyTerms of Service | ZenML Cloud Status

Source: https://www.chatbees.ai/blog/rag-use-cases
Top 10 RAG Use Cases and 17 Essential Tools for Implementation ChatBees Home Use Cases PricingBlogDocumentationContact Us Sign InTry Free Open main menu Home Use Cases PricingBlogDocumentationContact Us Sign InTry Free Go to Home Home Posts Top 10 RAG Use Cases and 17 Essential Tools for Implementation Top 10 RAG Use Cases and 17 Essential Tools for Implementation Gain insights into how to effectively utilize RAG analysis with these RAG use cases and recommended tools for seamless implementation. May 14, 2024 • Table of Contents What is Retrieval-Augmented Generation (RAG)? The Power of Generative AI in Text Response Challenges of Outdated Information in AI Responses Optimizing AI Responses with Retrieval-Augmented Generation (RAG) The Rise of Retrieval-Augmented Generation in AI Development 8 Benefits of Retrieval-Augmented Generation 1. Access to updated information 2. Factual grounding 3. Note 4. Contextual relevance 5. Factual consistency 6. Utilizes vector databases 7. Improved response accuracy 8. Multi-modal capabilities Related Reading How Does Retrieval-Augmented Generation Work? Different Approaches to RAG Architectures Related Reading 10 Practical RAG Use Cases 1. Transforming Customer Service with RAG-enhanced Chatbots 2. Elevating Content Creation and Journalism with RAG 3. Enhancing Healthcare Decision-making with RAG 4. RAG in Education and Research 5. Legal Research and Compliance Analysis with RAG 6. Personalizing E-commerce Experiences using RAG 7. Empowering Financial Analysis with RAG 8. RAG for Personalized Recommendations 9. Leveraging RAG for Text Completion 10. Translation Tasks with RAG 17 Leading RAG Tools / Software Providers in ‘24 1. ChatBees Serverless RAG Use cases Onboarding Sales enablement Customer support Product & Engineering 2. Azure machine learning 3. ChatGPT Retrieval Plugin 4. HuggingFace Transformer plugin 5. IBM Watsonx.ai 6. Meta AI 7. FARM 8. Haystack 9. REALM 10. LangChain 11. Phoenix 12. Milvus 13. MongoDB 14. ColBERT 15. NeMo Guardrails 16. LlamaIndex 17. Verba Challenges and Best Practices of Implementing RAG Systems 1. Integration complexity 2. Scalability Optimizing Computational Load for Enhanced Performance Leveraging Vector Databases for Improved Scalability Use ChatBees’ Serverless LLM to 10x Internal Operations Serverless RAG for Seamless Integration in Workflow Onboarding - Quick Access to Onboarding Materials and Resources Sales Enablement - Easy Access to Product Information and Customer Data Customer Support - Prompt and Accurate Responses Product & Engineering - Quick Access to Project Data and Resources Related Reading Do not index Do not index Unlock the boundless potential of Retrieval Augmented Generation with these innovative RAG use cases. Discover how this groundbreaking technology is revolutionizing various industries, driving efficiency, and transforming processes beyond recognition. From enhancing search engine performance to elevating content creation, RAG use cases promise a future full of possibilities and game-changing solutions. Explore real-world examples and see how RAG utilization can maximize your productivity and outcomes. Embrace the power of RAG use cases and witness the evolution of technology! What is Retrieval-Augmented Generation (RAG)? RAG Use Cases RAG models can find relevant information from databases, knowledge bases, or web sources and incorporate it into their generated text. RAG aims to overcome limitations of closed-book language models that only rely on their training data: The Power of Generative AI in Text Response Generative artificial intelligence (AI) excels at creating text responses based on large language models (LLMs) where the AI is trained on a massive number of data points. The good news is that the generated text is often easy to read and provides detailed responses that are broadly applicable to the questions asked of the software, often called prompts. Challenges of Outdated Information in AI Responses The bad news is that the information used to generate the response is limited to the information used to train the AI, often a generalized LLM. The LLM’s data may be weeks, months, or years out of date and in a corporate AI chatbot may not include specific information about the organization’s products or services. That can lead to incorrect responses that erode confidence in the technology among customers and employees. Optimizing AI Responses with Retrieval-Augmented Generation (RAG) That’s where retrieval-augmented generation (RAG) comes in. RAG provides a way to optimize the output of an LLM with targeted information without modifying the underlying model itself; that targeted information can be more up-to-date than the LLM as well as specific to a particular organization and industry. That means the generative AI system can provide more contextually appropriate answers to prompts as well as base those answers on extremely current data. The Rise of Retrieval-Augmented Generation in AI Development RAG first came to the attention of generative AI developers after the publication of “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” a 2020 paper published by Patrick Lewis and a team at Facebook AI Research. The RAG concept has been embraced by many academic and industry researchers, who see it as a way to significantly improve the value of generative AI systems. 8 Benefits of Retrieval-Augmented Generation RAG Use Cases 1. Access to updated information RAG allows LLMs to access the most up-to-date information from databases. This eliminates the issue of LLMs being outdated or unable to incorporate new knowledge. 2. Factual grounding The knowledge base used in RAG serves as a source of factual information. This is enterprise data for example or other corpora that support a specific domain. domains are important for RAG, the more tightly bound the RAG corpora is to a specific domain the more efficient it will be. When the LLM generates a response, it can retrieve relevant facts, details, and context from the knowledge base. By incorporating this retrieved information into the generation process, the LLM is guided to produce responses that are grounded in factual knowledge. 3. Note it also assists in preventing hallucinations being sent to the end user. The LLM will still generate solutions from time to time where its training is incomplete but the RAG technique helps improve the user experience. 4. Contextual relevance The retrieval mechanism in RAG ensures that the retrieved information is relevant to the input query or context. By providing the LLM with contextually relevant information, RAG helps the model generate responses that are more coherent and aligned with the given context. This contextual grounding helps to reduce the generation of irrelevant or off-topic responses. 5. Factual consistency RAG encourages the LLM to generate responses that are consistent with the retrieved factual information. By conditioning the generation process on the retrieved knowledge, RAG helps to minimize contradictions and inconsistencies in the generated text. This promotes factual consistency and reduces the likelihood of generating false or misleading information. 6. Utilizes vector databases RAGs leverage vector databases to efficiently retrieve relevant documents. Vector databases store documents as vectors in a high-dimensional space, allowing for fast and accurate retrieval based on semantic similarity. 7. Improved response accuracy RAGs complement LLMs by providing them with contextually relevant information. LLMs can then use this information to generate more coherent, informative, and accurate responses, even multi-modal ones. 8. Multi-modal capabilities RAG models can be extended to work with multiple modalities, such as text and images. This allows them to generate contextually relevant text to textual and visual content, opening up possibilities for applications in image captioning, content summarization, and more. Related Reading How Does Rag Work Rag Llm Rag Pipeline Rag Rating Rag Workflow How Does Retrieval-Augmented Generation Work? RAG Use Cases Retrieval and generation modules are the core components that drive the efficacy of RAG systems. The retriever’s purpose is to retrieve relevant information based on the user's input. It sources external data, creating a knowledge library that underpins the system’s responses. The generator then takes this information and the user’s query to augment the LLM’s responses. Together, these modules enhance the system’s response capabilities, creating a more detailed and accurate output. Different Approaches to RAG Architectures There are different architectures for RAG systems, including retrieve-and-rank, retrieve-and-refine, and retrieve-and-rewrite. The retrieve-and-rank model focuses on pulling relevant information and then ranking it based on importance. Retrieve-and-refine refines the retrieved information based on the context of the query. Retrieve-and-rewrite rephrases the retrieved content to better align with the user’s input. Each architecture has its nuances and unique benefits, providing users with a tailored experience depending on the RAG system in use. Related Reading What Is RAG LLM RAG LLM Meaning Rag Model Llm Rag Use Case Llm Tech Stack Rag Fine Tuning Rag Nlp Rag Api Rag Stack Rag Systems Rag Evaluation Rag Service Rag Software RAG Architecture LLM LLM Rag Meaning RAG LLM Example 10 Practical RAG Use Cases RAG Use Cases 1. Transforming Customer Service with RAG-enhanced Chatbots RAG is revolutionizing chatbots by integrating real-time information to enhance the accuracy and relevance of responses. This innovation helps chatbots provide more valuable and efficient customer interactions. 2. Elevating Content Creation and Journalism with RAG RAG assists in producing articles that are rich in context and facts. By integrating the latest data and references, RAG ensures content is well-written, factually accurate, and up-to-date. 3. Enhancing Healthcare Decision-making with RAG RAG aids in decision-making by providing accurate medical information. It supports doctors and researchers by quickly accessing the latest research findings and clinical data relevant to a patient’s case or medical condition. 4. RAG in Education and Research RAG strengthens learning tools and research by providing the most current information on various topics. This enhancement streamlines discovery processes for students and researchers. 5. Legal Research and Compliance Analysis with RAG RAG helps retrieve legal precedents, regulations, and case studies to aid in legal decision-making and ensure compliance. Legal professionals can benefit from RAG's ability to stay up-to-date with the latest legal information. 6. Personalizing E-commerce Experiences using RAG RAG assists in personalizing customer experiences by analyzing customer data and market trends. This customization enables offering personalized product recommendations and enhancing customer engagement. 7. Empowering Financial Analysis with RAG RAG enhances forecasting and analysis by integrating the latest market data, financial reports, and economic indicators. This leads to more informed and timely investment decisions. 8. RAG for Personalized Recommendations RAG systems analyze customer data to generate personalized product recommendations based on past purchases and reviews. This improves the overall user experience and boosts revenue for organizations. 9. Leveraging RAG for Text Completion RAG models complete partial texts in a contextually relevant and consistent way. This feature is useful for tasks like email drafting and code completion. 10. Translation Tasks with RAG While not the primary use case, RAG models can be used for translation tasks. The document retrieval component retrieves relevant translations from a corpus, and the LLM generates translations consistent with these examples. 17 Leading RAG Tools / Software Providers in ‘24 RAG Use Cases 1. ChatBees ChatBees optimizes RAG for internal operations like customer support, employee support, etc., with the most accurate response and easily integrating into their workflows in a low-code, no-code manner. ChatBees' agentic framework automatically chooses the best strategy to improve the quality of responses for these use cases. This improves predictability/accuracy enabling these operations teams to handle higher volume of queries. More features of our service: Serverless RAG Simple, Secure and Performant APIs to connect your data sources (PDFs/CSVs, Websites, GDrive, Notion, Confluence) Search/chat/summarize with the knowledge base immediately No DevOps is required to deploy and maintain the service Use cases Onboarding Quickly access onboarding materials and resources be it for customers, or internal employees like support, sales, or research team. Sales enablement Easily find product information and customer data Customer support Respond to customer inquiries promptly and accurately Product & Engineering Quick access to project data, bug reports, discussions, and resources, fostering efficient collaboration. Try our Serverless LLM Platform today to 10x your internal operations. Get started for free, no credit card required — sign in with Google and get started on your journey with us today! 2. Azure machine learning Azure Machine Learning allows you to incorporate RAG in your AI using the Azure AI Studio or using code with Azure Machine Learning pipelines. 3. ChatGPT Retrieval Plugin OpenAI offers a retrieval plugin to combine ChatGPT with a retrieval-based system to enhance its responses. You can set up a database of documents and use retrieval algorithms to find relevant information to include in ChatGPT’s responses. 4. HuggingFace Transformer plugin HuggingFace provides a transformer to generate RAG models. 5. IBM Watsonx.ai The model can deploy RAG pattern to generate factually accurate output. 6. Meta AI Meta AI Research (Former Facebook Research) directly combines retrieval and generation within a single framework. It’s designed for tasks that require both retrieving information from a large corpus and generating coherent responses. 7. FARM An internal framework from Deepset to build transformer-based NLP pipelines including RAG. 8. Haystack End-to-end RAG framework for document search provided by Deepset 9. REALM Retrieval Augmented Language Model (REALM) training is a Google toolkit for open-domain question answering with RAG. 10. LangChain LangChain is an open-source framework that enables chaining of steps, including prompts and external APIs, to help LLMs answer questions more accurately and promptly. It simplifies the development of context-aware, reasoning-enabled applications powered by language models 11. Phoenix Created by Arize AI, it focuses on AI observability and evaluation, offering tools like LLM Traces for understanding and troubleshooting LLM applications, and LLM Evals for assessing applications’ relevance and toxicity. It provides embedding analysis, enabling users to explore data clusters and performance, and supports RAG analysis to improve retrieval-augmented generation pipelines. It facilitates structured data analysis for A/B testing and drift analysis. Phoenix promotes a notebook-first approach, suitable for both experimentation and production environments, emphasizing easy deployment for continuous observability. 12. Milvus Vector database optimized for similarity search workloads, like passage retrieval. Backed by Zilliz. 13. MongoDB MongoDB is a powerful, open-source, NoSQL database designed for scalability and performance. It uses a document-oriented approach, supporting data structures similar to JSON. This flexibility allows for more dynamic and fluid data representation, making MongoDB popular for web applications, real-time analytics, and managing large volumes of data. MongoDB supports rich queries, full index support, replication, and sharding, offering robust features for high availability and horizontal scaling 14. ColBERT State-of-the-art neural retrieval model for extracting highly relevant passages. From Microsoft. 15. NeMo Guardrails Created by NVIDIA, this model offers an open-source toolkit to add programmable guardrails to conversational systems based on large language models, ensuring safer and more controlled interactions. These guardrails allow developers to define how the model behaves on specific topics, prevent discussions on unwanted subjects, and ensure compliance with conversation design best practices. 16. LlamaIndex LlamaIndex is an advanced toolkit for building RAG applications, enabling developers to enhance LLMs with the ability to query and retrieve information from various data sources. This toolkit facilitates the creation of sophisticated models that can access, understand, and synthesize information from databases, document collections, and other structured data. It supports complex query operations and integrates seamlessly with other AI components, offering a flexible and powerful solution for developing knowledge-enriched applications 17. Verba Verba is an open-source RAG chatbot powered by Weaviate. It simplifies exploring datasets and extracting insights through an end-to-end, user-friendly interface. Supporting local deployments or integration with LLM providers like OpenAI, Cohere, and HuggingFace, Verba stands out for its easy setup and versatility in handling various data types. Its core features include seamless data import, advanced query resolution, and accelerated queries through semantic caching, making it an ideal choice for creating sophisticated RAG applications. Challenges and Best Practices of Implementing RAG Systems RAG Use Cases While RAG applications allow us to bridge the gap between information retrieval and natural language processing, their implementation poses a few unique challenges. In this section, we will look into the complexities faced when building RAG applications and discuss how they can be mitigated. 1. Integration complexity It can be difficult to integrate a retrieval system with an LLM. This complexity increases when there are multiple sources of external data in varying formats. Data that is fed into an RAG system must be consistent, and the embeddings generated need to be uniform across all data sources. To overcome this challenge, separate modules can be designed to handle different data sources independently. The data within each module can then be preprocessed for uniformity, and a standardized model can be used to ensure that the embeddings have a consistent format. 2. Scalability As the amount of data increases, it gets more challenging to maintain the efficiency of the RAG system. Many complex operations need to be performed - such as generating embeddings, comparing the meaning between different pieces of text, and retrieving data in real-time. Optimizing Computational Load for Enhanced Performance These tasks are computationally intensive and can slow down the system as the size of the source data increases. To address this challenge, you can distribute computational load across different servers and invest in robust hardware infrastructure. To improve response time, it might also be beneficial to cache queries that are frequently asked. Leveraging Vector Databases for Improved Scalability The implementation of vector databases can also mitigate the scalability challenge in RAG systems. These databases allow you to handle embeddings easily, and can quickly retrieve vectors that are most closely aligned with each query. Use ChatBees’ Serverless LLM to 10x Internal Operations ChatBees optimizes RAG for internal operations like customer support, employee support, etc., with the most accurate response and easily integrating into their workflows in a low-code, no-code manner. ChatBees' agentic framework automatically chooses the best strategy to improve the quality of responses for these use cases. This improves predictability/accuracy enabling these operations teams to handle higher volume of queries. Serverless RAG for Seamless Integration in Workflow Serverless RAG offers simple, secure, and performant APIs to connect data sources such as PDFs, CSVs, websites, GDrive, Notion, and Confluence. This allows for immediate search, chat, and summarization with the knowledge base without the need for DevOps to deploy or maintain the service. Onboarding - Quick Access to Onboarding Materials and Resources ChatBees helps in quickly accessing onboarding materials and resources for customers or internal employees like support, sales, and research teams. With RAG technology, ChatBees streamlines the onboarding process, making it efficient and time-saving. Sales Enablement - Easy Access to Product Information and Customer Data In sales enablement, ChatBees enables easy access to product information and customer data, making it simpler for sales teams to find the information they need on the go. This boosts efficiency and productivity in the sales process. Customer Support - Prompt and Accurate Responses For customer support, ChatBees allows agents to respond to customer inquiries promptly and accurately. RAG technology ensures that the responses are accurate and improve customer satisfaction rates. Product & Engineering - Quick Access to Project Data and Resources In product and engineering use cases, ChatBees offers quick access to project data, bug reports, discussions, and resources. This fosters efficient collaboration among teams, leading to improved productivity and project outcomes. Related Reading Rag Platform Rag Apps Langserve Rag Llama Rag Scale Openai Rag Langchain Alternatives Bedrock Knowledge Base Credal Ai Langchain Rag Databricks Rag Nuclia Aws Rag Azure Rag Related posts How Does RAG Work in Transforming AI Text Generation? May 4, 2024 How Does RAG Work in Transforming AI Text Generation? RAG is a cutting-edge technique in AI text generation that combines the power of retrieval-based and generative models. Learn how does RAG work. Why Retrieval Augmented Generation Is a Game Changer May 3, 2024 Why Retrieval Augmented Generation Is a Game Changer Learn about the exciting new field of retrieval augmented generation and how it is reshaping the way we think about content creation. What Is a RAG LLM Model & the 14 Best Platforms for Implementation May 5, 2024 What Is a RAG LLM Model & the 14 Best Platforms for Implementation Wondering what a RAG LLM model is and how it can benefit your organization? Implement this model and take your business to the next level. In-Depth Step-By-Step Guide for Building a RAG Pipeline May 6, 2024 In-Depth Step-By-Step Guide for Building a RAG Pipeline Building a RAG pipeline doesn't have to be complicated. Let this guide simplify the process and help you achieve your pipeline goals. Step-By-Step Process of Building an Efficient RAG Workflow May 8, 2024 Step-By-Step Process of Building an Efficient RAG Workflow Learn how to build an effective RAG workflow with this step-by-step guide, designed to improve communication and productivity within your team. A Comprehensive Guide to RAG NLP and Its Growing Applications May 9, 2024 A Comprehensive Guide to RAG NLP and Its Growing Applications RAG NLP is the future of natural language processing. This guide will provide you with a deep dive into its capabilities and potential applications. Ultimate Guide to RAG Evaluation Metrics, Strategies & Automation May 10, 2024 Ultimate Guide to RAG Evaluation Metrics, Strategies & Automation Looking to streamline your evaluation process? Dive into this ultimate guide on RAG evaluation metrics, strategies, and automation for insights. Understanding RAG Systems & 10 Optimization Techniques May 11, 2024 Understanding RAG Systems & 10 Optimization Techniques Learn how RAG Systems work and uncover 10 effective optimization strategies to elevate your performance. Dive into this valuable resource today! Step-By-Step Guide to Build a DIY RAG Stack & Top 10 Considerations May 12, 2024 Step-By-Step Guide to Build a DIY RAG Stack & Top 10 Considerations Take control of your data visualization with a DIY RAG stack. This guide walks you through the process and highlights key considerations. How to Optimize Your LLM App With a RAG API Solution May 13, 2024 How to Optimize Your LLM App With a RAG API Solution Improve your LLM app's capabilities with a Retrieval Augmented Generation API solution. Integrate this advanced technology to boost efficiency. Key RAG Fine Tuning Strategies for Improved Performance May 15, 2024 Key RAG Fine Tuning Strategies for Improved Performance Looking to improve your RAG fine tuning skills? This guide breaks down key strategies that will help you achieve better performance and results. How to Deploy a Made-For-You RAG Service in Minutes May 16, 2024 How to Deploy a Made-For-You RAG Service in Minutes Create your own RAG service in minutes with our easy-to-follow guide. Start monitoring, managing, and reporting with minimal effort required. Key Components and Emerging Trends of the New LLM Tech Stack May 17, 2024 Key Components and Emerging Trends of the New LLM Tech Stack Looking to understand the latest advancements in the LLM Tech Stack? This guide breaks down the key components and emerging trends you need to know. 17 Best RAG Software Platforms for Rapid Deployment of GenAI Apps May 18, 2024 17 Best RAG Software Platforms for Rapid Deployment of GenAI Apps Rapidly deploy your GenAI apps with these top 17 RAG software platforms. Explore your options and choose the right tool for your project today! What Is Retrieval-Augmented Generation & Top 8 RAG Use Case Examples May 19, 2024 What Is Retrieval-Augmented Generation & Top 8 RAG Use Case Examples Learn RAG use cases and how this technology is transforming the way content is generated. Explore top examples that can benefit your business. Top 16 RAG Platform Options for Hassle-Free GenAI Solutions May 20, 2024 Top 16 RAG Platform Options for Hassle-Free GenAI Solutions Simplify your GenAI solutions with one of these top 16 RAG platforms. Find the perfect fit for your business and streamline your processes. Complete Step-by-Step Guide to Create a RAG Llama System May 22, 2024 Complete Step-by-Step Guide to Create a RAG Llama System Take control of your projects with RAG Llama. Follow and learn how to implement and utilize this powerful project management tool effectively. How to Use LangServe to Build Rest APIs for Langchain Applications May 24, 2024 How to Use LangServe to Build Rest APIs for Langchain Applications Looking to simplify the creation of Rest APIs for your Langchain projects? Learn how LangServe can help streamline the process and efficiency. 12 Strategies for Achieving Effective RAG Scale Systems May 23, 2024 12 Strategies for Achieving Effective RAG Scale Systems From setting clear criteria to regular evaluations, these strategies will ensure that your RAG scale system is serving its purpose efficiently. The Ultimate Guide to OpenAI RAG (Performance, Costs, & More) May 25, 2024 The Ultimate Guide to OpenAI RAG (Performance, Costs, & More) Interested in learning about OpenAI RAG? This guide covers all aspects of this tool, from its performance capabilities to its associated costs. 15 Best Langchain Alternatives For AI Development May 26, 2024 15 Best Langchain Alternatives For AI Development Explore a variety of Langchain alternatives that offer features and capabilities for AI development. Find the perfect solution for your projects. How To Get Started With LangChain RAG In Python May 27, 2024 How To Get Started With LangChain RAG In Python Learn how to use LangChain RAG in Python with this beginner-friendly tutorial. Get up and running quickly with straightforward instructions. 22 Best Nuclia Alternatives for Frictionless RAG-as-a-Service May 28, 2024 22 Best Nuclia Alternatives for Frictionless RAG-as-a-Service Ready to ditch Nuclia? Discover the best alternatives for RAG-as-a-Service in this list of top-performing platforms that will meet all your needs. Complete AWS Bedrock Knowledge Base Setup May 29, 2024 Complete AWS Bedrock Knowledge Base Setup Set up your AWS Bedrock Knowledge Base with this guide. Learn how to effectively organize and manage your knowledge base for maximum efficiency. Complete Guide for Deploying Production-Quality Databricks RAG Apps May 30, 2024 Complete Guide for Deploying Production-Quality Databricks RAG Apps Deploying production-quality Databricks RAG apps can be challenging, but this guide will walk you through the process step by step. Top 11 Credal AI Alternatives for Secure RAG Deployment May 31, 2024 Top 11 Credal AI Alternatives for Secure RAG Deployment Explore the best alternatives to Credal AI for seamless RAG deployment. Find the perfect solution for your business needs with our list. A Step-By-Step Guide for Serverless AWS RAG Applications Jun 1, 2024 A Step-By-Step Guide for Serverless AWS RAG Applications Learn how to develop serverless AWS RAG applications with this detailed guide. Follow along and build your own applications in no time! Complete Guide for Designing and Deploying an AWS RAG Solution Jun 2, 2024 Complete Guide for Designing and Deploying an AWS RAG Solution Designing and deploying an AWS RAG solution can be complex, but with this guide, you'll have all the information you need to succeed. In-Depth Look at the RAG Architecture LLM Framework Jun 3, 2024 In-Depth Look at the RAG Architecture LLM Framework Explore the RAG Architecture LLM Framework in detail. Learn how this framework can revolutionize the way you approach architectural design. Complete RAG Model LLM Operations Guide Jun 5, 2024 Complete RAG Model LLM Operations Guide Get a comprehensive understanding of the RAG Model LLM with this operations guide. Learn to effectively implement this model in your organization. Decoding RAG LLM Meaning & Process Overview for Apps Jun 4, 2024 Decoding RAG LLM Meaning & Process Overview for Apps Curious about RAG LLM meaning and how it applies to apps? Let's know the process and give insights into this important aspect of app development. What Is RAG LLM & 5 Essential Business Use Cases Jun 6, 2024 What Is RAG LLM & 5 Essential Business Use Cases Learn what is RAG LLM and its practical applications in business with this informative guide. Get ready to leverage this powerful tool for success! LLM RAG Meaning & Its Implications for Gen AI Apps Jun 7, 2024 LLM RAG Meaning & Its Implications for Gen AI Apps Are you curious about the LLM RAG meaning and how it impacts Generation AI apps? This post breaks down the meaning and implications for you. What Are Some RAG LLM Examples? Jun 9, 2024 What Are Some RAG LLM Examples? Learn from RAG LLM examples to enhance your project management skills. Find out how this can help you identify and learn from project successes. ChatBees Twitter YouTube © 2024 ChatBees. All Rights Reserved.



